

=== Processing ../JMLR 2024/Accelerated Gradient Tracking over Time-varying Graphs for Decentralized Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Accelerated Gradient Tracking over Time-varying Graphs for Decentralized Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 5/21; Revised 12/23; Published 9/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAccelerated Gradient Tracking over Time-varying Graphs for
                Decentralized Optimization[0m

Box rectangle:  [32m(88.4, 151.9) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHuan Li
                lihuanss@nankai.edu.cn
                Institute of Robotics and Automatic Information Systems
                College of Artificial Intelligence
                Nankai University
                Tianjin 300071, China[0m

Box rectangle:  [32m(89.4, 217.0) -> (522.0, 271.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhouchen Lin B
                zlin@pku.edu.cn
                State Key Lab of General AI, School of Intelligence Science and Technology, Peking University
                Institute for Artificial Intelligence, Peking University, Beijing 100871, China
                Pazhou Laboratory (Huangpu), Guangzhou 510555, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (221.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Huan Li and Zhouchen Lin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0475.html.[0m



=== Processing ../JMLR 2024/Accelerating Nuclear-norm Regularized Low-rank Matrix Optimization Through Burer-Monteiro Decomposition.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Accelerating Nuclear-norm Regularized Low-rank Matrix Optimization Through Burer-Monteiro Decomposition.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 1/23; Revised 10/24; Published 12/24[0m

Box rectangle:  [32m(103.2, 119.3) -> (509.0, 151.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAccelerating Nuclear-norm Regularized Low-rank Matrix
                Optimization Through Burer-Monteiro Decomposition[0m

Box rectangle:  [32m(90.0, 169.6) -> (522.0, 217.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChing-pei Lee
                chingpei@ism.ac.jp
                Institute of Statistical Mathematics
                10-3 Midori-cho, Tachikawa
                Tokyo 190-8562, Japan[0m

Box rectangle:  [32m(90.0, 223.1) -> (521.9, 271.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLing Liang
                liang.ling@u.nus.edu
                Department of Mathematics
                University of Maryland at College Park
                4176 Campus Drive, College Park, MD, USA 20742[0m

Box rectangle:  [32m(90.0, 276.7) -> (522.0, 324.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTianyun Tang
                ttang@u.nus.edu
                Institute of Operations Research and Analytics
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore 119076[0m

Box rectangle:  [32m(90.0, 331.9) -> (522.0, 384.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKim-Chuan Toh
                mattohkc@nus.edu.sg
                Department of Mathematics and Institute of Operations Research and Analytics
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore 119076[0m

Box rectangle:  [32m(90.0, 726.3) -> (340.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ching-pei Lee, Ling Liang, Tianyun Tang, Kim-Chuan Toh.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0049.html.[0m



=== Processing ../JMLR 2024/A Characterization of Multioutput Learnability.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Characterization of Multioutput Learnability.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 6/23 ; Revised 6/24; Published 10/24[0m

Box rectangle:  [32m(137.8, 101.6) -> (474.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Characterization of Multioutput Learnability[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVinod Raman ∗
                vkraman@umich.edu
                Department of Statistics
                University of Michigan
                Ann-Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 187.2) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnique Subedi ∗
                subedi@umich.edu
                Department of Statistics
                University of Michigan
                Ann-Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 309.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmbuj Tewari
                tewaria@umich.edu
                Department of Statistics
                Department of Electrical Engineering and Computer Science
                University of Michigan
                Ann-Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (306.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Vinod Raman, Unique Subedi, and Ambuj Tewari.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0787.html.[0m



=== Processing ../JMLR 2024/A Comparison of Continuous-Time Approximations to Stochastic Gradient Descent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Comparison of Continuous-Time Approximations to Stochastic Gradient Descent.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 2/23; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(113.3, 102.0) -> (498.7, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Comparison of Continuous-Time Approximations to[0m

Box rectangle:  [32m(205.7, 120.0) -> (406.3, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStochastic Gradient Descent[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStefan Ankirchner
                s.ankirchner@uni-jena.de
                Institute for Mathematics
                Friedrich-Schiller-University Jena
                07737 Jena, Germany[0m

Box rectangle:  [32m(90.0, 208.8) -> (522.0, 260.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStefan Perko
                stefan.perko@uni-jena.de
                Institute for Mathematics
                Friedrich-Schiller-University Jena
                07737 Jena, Germany[0m

Box rectangle:  [32m(90.0, 285.3) -> (196.8, 295.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Stephan Mandt[0m

Box rectangle:  [32m(280.3, 320.9) -> (331.7, 332.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 338.2) -> (502.1, 517.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mApplying a stochastic gradient descent (SGD) method for minimizing an objective gives
                rise to a discrete-time process of estimated parameter values. In order to better understand
                the dynamics of the estimated values, many authors have considered continuous-time ap-
                proximations of SGD. We refine existing results on the weak error of first-order ODE and
                SDE approximations to SGD for non-infinitesimal learning rates. In particular, we ex-
                plicitly compute the linear term in the error expansion of gradient flow and two of its
                stochastic counterparts, with respect to a discretization parameter h. In the example of
                linear regression, we demonstrate the general inferiority of the deterministic gradient flow
                approximation in comparison to the stochastic ones, for batch sizes which are not too
                large. Further, we demonstrate that for Gaussian features an SDE approximation with
                state-independent noise (CC) is preferred over using a state-dependent coefficient (NCC).
                The same comparison holds true for features of low kurtosis or large batch sizes. However,
                the relationship reverses for highly leptokurtic features or small batch sizes.
                Keywords:
                stochastic gradient descent, gradient flow, stochastic differential equation,
                weak approximation, Talay-Tubaro expansion[0m

Box rectangle:  [32m(90.0, 537.7) -> (180.3, 549.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 559.8) -> (504.0, 577.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConsider a d-dimensional discrete-time stochastic process χ = (χn)n∈N0 with dynamics[0m

Box rectangle:  [32m(217.7, 584.1) -> (522.0, 606.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mχn+1 = χn −h∇Rγ(n)(χn),
                n ∈N0,
                (1)[0m

Box rectangle:  [32m(90.0, 609.4) -> (522.0, 644.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mwhere (Rr)r is a family of differentiable functions from Rd to R, h is a positive real number,
                and (γ(n))n∈N0 is an i.i.d. sequence of random variables. We interpret (χh[0m

Box rectangle:  [32m(90.0, 626.9) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mn)n∈N0 as the se-
                quence of estimated parameters when applying a stochastic gradient descent (SGD) method
                for minimizing the function R(x) = E[Rγ(0)(x)]. The function R itself can be interpreted
                as empirical risk (that is training error) or population risk. We refer to h as the learning
                rate and Rγ(n) as the risk due to the n-th data point or mini batch. In the following we
                simply call χ a SGD process.[0m

Box rectangle:  [32m(90.0, 726.5) -> (254.3, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Stefan Ankirchner and Stefan Perko.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0237.html.[0m



=== Processing ../JMLR 2024/Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.2) -> (522.0, 50.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 5/23; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(116.5, 102.7) -> (495.5, 135.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdam-family Methods for Nonsmooth Optimization with
                Convergence Guarantees[0m

Box rectangle:  [32m(90.0, 154.2) -> (521.8, 200.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNachuan Xiao
                XNC@LSEC.CC.AC.CN
                Institute of Operations Research and Analytics
                National University of Singapore
                3 Research Link, Singapore, 117602[0m

Box rectangle:  [32m(90.0, 205.2) -> (521.8, 254.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaoyin Hu∗
                HXY@AMSS.AC.CN
                School of Computer and Computing Science
                Hangzhou City University
                Hangzhou, China, 310015[0m

Box rectangle:  [32m(90.0, 261.4) -> (521.8, 307.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXin Liu
                LIUXIN@LSEC.CC.AC.CN
                State Key Laboratory of Scientific and Engineering Computing
                Academy of Mathematics and Systems Science, Chinese Academy of Sciences
                Beijing, China, 100190[0m

Box rectangle:  [32m(90.0, 316.6) -> (521.8, 367.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKim-Chuan Toh
                MATTOHKC@NUS.EDU.SG
                Department of Mathematics and Institute of Operations Research and Analytics
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore, 119076[0m

Box rectangle:  [32m(90.0, 726.4) -> (319.7, 735.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nachuan Xiao, Xiaoyin Hu, Xin Liu, and Kim-Chuan Toh.[0m

Box rectangle:  [32m(90.0, 741.3) -> (509.3, 758.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0576.html.[0m



=== Processing ../JMLR 2024/Adaptive Latent Feature Sharing for Piecewise Linear Dimensionality Reduction.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Adaptive Latent Feature Sharing for Piecewise Linear Dimensionality Reduction.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 2/21; Published 3/24[0m

Box rectangle:  [32m(115.3, 101.6) -> (496.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdaptive Latent Feature Sharing for Piecewise Linear
                Dimensionality Reduction[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdam Farooq
                a.farooq6@aston.ac.uk
                Department of Mathematics
                Aston University
                Birmingham, UK[0m

Box rectangle:  [32m(90.0, 204.5) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYordan P. Raykov B
                yordan.raykov@nottingham.ac.uk
                School of Mathematical Sciences
                University of Nottingham
                Nottingham, UK[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 367.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPetar Raykov
                petar.raykov@mrc-cbu.cam.ac.uk
                MRC Cognition and Brain Sciences Unit
                University of Cambridge
                Cambridge, UK
                Max A. Little
                maxl@mit.edu
                Department of Computer Science
                University of Birmingham
                Birmingham, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (369.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Adam Farooq, Yordan P. Raykov, Petar Raykov and Max A. Little.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0146.html.[0m



=== Processing ../JMLR 2024/Adaptivity and Non-stationarity  Problem-dependent Dynamic Regret for Online Convex Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Adaptivity and Non-stationarity  Problem-dependent Dynamic Regret for Online Convex Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 7/21; Revised 11/23; Published 3/24[0m

Box rectangle:  [32m(116.5, 101.7) -> (495.6, 133.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdaptivity and Non-stationarity: Problem-dependent
                Dynamic Regret for Online Convex Optimization[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 233.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeng Zhao
                zhaop@lamda.nju.edu.cn
                Yu-Jie Zhang
                zhangyj@lamda.nju.edu.cn
                Lijun Zhang
                zhanglj@lamda.nju.edu.cn
                Zhi-Hua Zhou
                zhouzh@lamda.nju.edu.cn
                National Key Laboratory for Novel Software Technology
                Nanjing University, Nanjing 210023, China[0m

Box rectangle:  [32m(90.0, 258.5) -> (203.9, 268.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Csaba Szepesvari[0m

Box rectangle:  [32m(280.3, 294.2) -> (331.7, 306.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 310.8) -> (502.3, 368.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe investigate online convex optimization in non-stationary environments and choose dy-
                namic regret as the performance measure, defined as the difference between cumulative loss
                incurred by the online algorithm and that of any feasible comparator sequence. Let T be
                the time horizon and PT be the path length that essentially reflects the non-stationarity of
                environments, the state-of-the-art dynamic regret is O(
                p[0m

Box rectangle:  [32m(109.9, 358.7) -> (504.1, 585.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mT(1 + PT )). Although this bound
                is proved to be minimax optimal for convex functions, in this paper, we demonstrate that
                it is possible to further enhance the guarantee for some easy problem instances, particu-
                larly when online functions are smooth. Specifically, we introduce novel online algorithms
                that can exploit smoothness and replace the dependence on T in dynamic regret with
                problem-dependent quantities: the variation in gradients of loss functions, the cumulative
                loss of the comparator sequence, and the minimum of these two terms. These quantities
                are at most O(T) while could be much smaller in benign environments. Therefore, our
                results are adaptive to the intrinsic difficulty of the problem, since the bounds are tighter
                than existing results for easy problems and meanwhile safeguard the same rate in the worst
                case. Notably, our proposed algorithms can achieve favorable dynamic regret with only
                one gradient per iteration, sharing the same gradient query complexity as the static regret
                minimization methods. To accomplish this, we introduce the collaborative online ensemble
                framework. The proposed framework employs a two-layer online ensemble to handle non-
                stationarity, and uses optimistic online learning and further introduces crucial correction
                terms to enable effective collaboration within the meta-base two layers, thereby attaining
                adaptivity. We believe the framework can be useful for broader problems.
                Keywords:
                Online Learning, Online Convex Optimization, Dynamic Regret, Problem-
                dependent Bounds, Gradient Variation, Optimistic Online Mirror Descent, Online Ensemble[0m

Box rectangle:  [32m(90.0, 604.9) -> (180.3, 616.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 626.4) -> (522.2, 706.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn many real-world applications, data are inherently accumulated over time, and thus it is
                of great importance to develop a learning system that updates in an online fashion. Online
                Convex Optimization (OCO) (Hazan, 2016; Orabona, 2019) is a powerful paradigm for
                learning in such scenarios, which can be regarded as an iterative game between a player
                and an adversary. At iteration t, the player chooses a decision vector xt from a convex
                set X ⊆Rd. Subsequently, the adversary discloses a convex function ft : X 7→R, and the[0m

Box rectangle:  [32m(90.0, 726.4) -> (335.9, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou.[0m

Box rectangle:  [32m(90.0, 740.8) -> (516.4, 758.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-0748.html.[0m



=== Processing ../JMLR 2024/A Data-Adaptive RKHS Prior for Bayesian Learning of Kernels in Operators.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Data-Adaptive RKHS Prior for Bayesian Learning of Kernels in Operators.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 12/22; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(142.7, 101.6) -> (469.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Data-Adaptive RKHS Prior
                for Bayesian Learning of Kernels in Operators[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeil K. Chada
                neilchada123@gmail.com
                Department of Mathematics, City University of Hong Kong, B5431, Hong Kong SAR[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQuanjun Lang
                quanjun.lang@duke.edu
                Department of Mathematics, Duke University, Durham, NC 27707, USA[0m

Box rectangle:  [32m(90.0, 211.2) -> (518.2, 223.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFei Lu
                feilu@math.jhu.edu[0m

Box rectangle:  [32m(90.0, 230.5) -> (522.0, 256.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiong Wang
                xiongwang@jhu.edu
                Department of Mathematics, Johns Hopkins University, Baltimore, MD 21218, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (326.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Neil K. Chada, Quanjun Lang, Fei Lu and Xiong Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1491.html.[0m



=== Processing ../JMLR 2024/Additive smoothing error in backward variational inference for general state-space models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Additive smoothing error in backward variational inference for general state-space models.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 12/22; Published 01/24[0m

Box rectangle:  [32m(96.4, 102.0) -> (515.6, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdditive smoothing error in backward variational inference[0m

Box rectangle:  [32m(199.6, 120.0) -> (412.4, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mfor general state-space models[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMathis Chagneux
                m.chagneux@gmail.com
                LTCI
                T ́el ́ecom Paris
                Palaiseau, France[0m

Box rectangle:  [32m(90.0, 205.8) -> (522.0, 243.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33ḿElisabeth Gassiat
                elisabeth.gassiat@universite-paris-saclay.fr
                Universit ́e Paris-Saclay, CNRS
                Laboratoire de math ́ematiques d’Orsay[0m

Box rectangle:  [32m(90.0, 250.0) -> (522.0, 284.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPierre Gloaguen
                pierre.gloaguen@univ-ubs.fr
                Universit Bretagne-Sud
                Lorient, France[0m

Box rectangle:  [32m(90.0, 293.3) -> (522.0, 344.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSylvain Le Corff
                sylvain.le corff@sorbonne-universite.fr
                LPSM
                Sorbonne Universit ́e, UMR CNRS 8001
                Paris, France[0m

Box rectangle:  [32m(90.0, 369.8) -> (185.0, 379.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Anthony Lee[0m

Box rectangle:  [32m(280.3, 405.4) -> (331.7, 417.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 425.0) -> (502.1, 569.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe consider the problem of state estimation in general state-space models using varia-
                tional inference. For a generic variational family defined using the same backward decom-
                position as the actual joint smoothing distribution, we establish under mixing assumptions
                that the variational approximation of expectations of additive state functionals induces
                an error which grows at most linearly in the number of observations. This guarantee is
                consistent with the known upper bounds for the approximation of smoothing distributions
                using standard Monte Carlo methods. We illustrate our theoretical result with state-of-the
                art variational solutions based both on the backward parameterization and on alternatives
                using forward decompositions. This numerical study proposes guidelines for variational
                inference based on neural networks in state-space models.
                Keywords:
                Variational inference, State-space models, Smoothing, Backward decomposi-
                tion, State inference[0m

Box rectangle:  [32m(90.0, 590.4) -> (180.3, 602.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 613.4) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhen generative data models involve so-called hidden or latent states, providing statistical
                estimates of the latter given observed data - also known as state inference - is the cornerstone
                of many machine learning algorithms (Dempster et al., 1977; Kingma and Welling, 2014).
                Traditional models usually introduce low-dimensional states having directly interpretable
                meaning, while benefiting from accurate inference via exact or consistent Monte Carlo
                methods. In contrast, modern latent-data machine learning models are rooted in the so-
                called manifold hypothesis which views high dimensional data as originating from hidden[0m

Box rectangle:  [32m(90.0, 726.5) -> (386.6, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Mathis Chagneux, Elisabeth Gassiat, Pierre Gloaguen, Sylvain Le Corff.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1392.html.[0m



=== Processing ../JMLR 2024/Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 3/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(96.0, 101.6) -> (516.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdjusted Wasserstein Distributionally Robust Estimator in
                Statistical Learning[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYiling Xie
                yxie350@gatech.edu
                Xiaoming Huo
                huo@gatech.edu
                School of Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, Georgia, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (231.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yiling Xie and Xiaoming Huo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0379.html.[0m



=== Processing ../JMLR 2024/aeon  a Python Toolkit for Learning from Time Series.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/aeon  a Python Toolkit for Learning from Time Series.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-10
                Submitted 11/23; Revised 3/24; Published 9/24[0m

Box rectangle:  [32m(114.1, 101.6) -> (498.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33maeon: a Python Toolkit for Learning from Time Series[0m

Box rectangle:  [32m(90.0, 135.3) -> (522.0, 337.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew Middlehurst1
                m.b.middlehurst@soton.ac.uk
                Ali Ismail-Fawaz2
                ali-el-hadi.ismail-fawaz@uha.fr
                Antoine Guillaume3
                antoine.guillaume@novahe.fr
                Christopher Holder1,4
                c.holder@uea.ac.uk
                David Guijo-Rubio4,5
                dguijo@uco.es
                Guzal Bulatova
                guzalbulatova@gmail.com
                Leonidas Tsaprounis
                leonidas.tsap@gmail.com
                Lukasz Mentel
                lukasz.mentel@pm.me
                Martin Walter
                martin.friedrich.walter@gmail.com
                Patrick Sch ̈afer6
                patrick.schaefer@hu-berlin.de
                Anthony Bagnall1,4
                a.j.bagnall@soton.ac.uk
                The authors are the developers of aeon at the time of submission, with the following affiliations:
                1ECS, University of Southampton, United Kingdom 2IRIMAS, Universit ́e de Haute-Alsace, France
                3Novah ́e and Constellation, France
                4CMP, University of East Anglia, United Kingdom
                5DIAN,
                University of C ́ordoba, Spain 6Humboldt-Universit ̈at zu Berlin, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (520.0, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Matthew Middlehurst, Ali Ismail-Fawaz, Antoine Guillaume, Christopher Holder, David Guijo-Rubio, Guzal
                Bulatova, Leonidas Tsaprounis, Lukasz Mentel, Martin Walter, Patrick Sch ̈afer, Anthony Bagnall.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1444.html.[0m



=== Processing ../JMLR 2024/Aequitas Flow  Streamlining Fair ML Experimentation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Aequitas Flow  Streamlining Fair ML Experimentation.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.5) -> (522.0, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-7
                Submitted 5/24; Revised 9/24; Published 10/24[0m

Box rectangle:  [32m(115.3, 99.5) -> (496.8, 113.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAequitas Flow: Streamlining Fair ML Experimentation[0m

Box rectangle:  [32m(89.1, 130.8) -> (522.0, 261.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSérgio Jesus1,2
                sergio.jesus@feedzai.com
                Pedro Saleiro1
                pedro.saleiro@feedzai.com
                Inês Oliveira e Silva1
                ines.silva@feedzai.com
                Beatriz M. Jorge1
                beatriz.jorge@feedzai.com
                Rita P. Ribeiro2
                rpribeiro@fc.up.pt
                João Gama2
                jgama@fep.up.pt
                Pedro Bizarro1
                pedro.bizarro@feedzai.com
                Rayid Ghani3
                rayid@cmu.edu
                1Feedzai
                2University of Porto
                3Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 286.0) -> (199.6, 294.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Sebastian Schelter[0m

Box rectangle:  [32m(280.3, 317.2) -> (331.7, 329.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(108.8, 333.5) -> (503.4, 453.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAequitas Flow is an open-source framework and toolkit for end-to-end Fair Machine Learning
                (ML) experimentation, and benchmarking in Python. This package fills integration gaps that exist
                in other fair ML packages. In addition to the existing audit capabilities in Aequitas, the Aequitas
                Flow module provides a pipeline for fairness-aware model training, hyperparameter optimization,
                and evaluation, enabling easy-to-use and rapid experiments and analysis of results. Aimed at
                ML practitioners and researchers, the framework offers implementations of methods, datasets,
                metrics, and standard interfaces for these components to improve extensibility. By facilitating the
                development of fair ML practices, Aequitas Flow hopes to enhance the incorporation of fairness
                concepts in AI systems making AI systems more robust and fair.
                Keywords:
                Fair machine learning, experimentation, ethical artificial intelligence, open-source
                framework, python[0m

Box rectangle:  [32m(90.0, 473.0) -> (180.2, 485.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.8, 493.7) -> (523.9, 683.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeveloping Machine Learning (ML) and Artificial Intelligence (AI) systems that result in fairness
                and equity is a critical topic, especially as such systems get used in high-stakes settings such as
                hiring (Dastin, 2018), healthcare (Igoe, 2021), criminal justice (Angwin et al., 2016; Chouldechova,
                2017), and financial services (Zhang and Zhou, 2019; Bartlett et al., 2019; Jesus et al., 2022). While
                numerous studies define metrics and properties of algorithmic fairness (Chouldechova, 2017; Calders
                and Verwer, 2010; Dwork et al., 2012; Feldman et al., 2015; Hardt et al., 2016; Corbett-Davies et al.,
                2017) and propose methods for fairer models (Fish et al., 2016; Calmon et al., 2017; Zafar et al., 2017;
                Cotter et al., 2019), gaps in the implementation, user experience. and integration of existing tools
                hinder end-to-end experimentation (Lee and Singh, 2021) and benchmarking. This makes empirical
                studies and practical use challenging, scarce, and often limited in scope (Friedler et al., 2019; Lamba
                et al., 2021), ultimately affecting the adoption of fair ML methods in real-world high-stakes settings.
                This paper introduces Aequitas Flow, an open-source framework for reproducible and extensible
                end-to-end fair ML experimentation that extends Aequitas, our original bias audit toolkit. The goal
                is to help 1)researchers compare and benchmark new methods they develop against existing methods
                in a systematic and reproducible manner and 2) practitioners easily evaluate existing bias mitigation
                methods and deploy ones that best match their goals.[0m

Box rectangle:  [32m(90.0, 727.1) -> (505.8, 742.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc
                ⃝2024 Sérgio Jesus, Pedro Saleiro, Inês Oliveira e Silva, Beatriz M. Jorge, Rita P. Ribeiro, João Gama, Pedro Bizarro,
                Rayid Ghani.[0m

Box rectangle:  [32m(90.0, 748.2) -> (489.1, 763.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-0677.html.[0m



=== Processing ../JMLR 2024/A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 8/22; Revised 12/23; Published 6/24[0m

Box rectangle:  [32m(104.8, 102.0) -> (507.2, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Flexible Empirical Bayes Approach to Multiple Linear[0m

Box rectangle:  [32m(109.3, 120.0) -> (502.7, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRegression, and Connections with Penalized Regression[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoungseok Kim
                youngseok@uchicago.edu
                Department of Statistics
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 207.2) -> (522.0, 253.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWei Wang
                weiwang@galton.uchicago.edu
                Department of Statistics
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 260.8) -> (522.0, 307.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Carbonetto
                pcarbo@uchicago.edu
                Research Computing Center and Department of Human Genetics
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 316.0) -> (522.0, 367.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew Stephens
                mstephens@uchicago.edu
                Department of Statistics and Department of Human Genetics
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 392.5) -> (182.8, 402.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Boaz Nadler[0m

Box rectangle:  [32m(280.3, 426.1) -> (331.7, 438.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 444.5) -> (502.1, 636.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe introduce a new empirical Bayes approach for large-scale multiple linear regression. Our
                approach combines two key ideas: (i) the use of flexible “adaptive shrinkage” priors, which
                approximate the nonparametric family of scale mixture of normal distributions by a finite
                mixture of normal distributions; and (ii) the use of variational approximations to efficiently
                estimate prior hyperparameters and compute approximate posteriors. Combining these two
                ideas results in fast and flexible methods, with computational speed comparable to fast
                penalized regression methods such as the Lasso, and with competitive prediction accuracy
                across a wide range of scenarios. Further, we provide new results that establish conceptual
                connections between our empirical Bayes methods and penalized methods. Specifically,
                we show that the posterior mean from our method solves a penalized regression problem,
                with the form of the penalty function being learned from the data by directly solving
                an optimization problem (rather than being tuned by cross-validation).
                Our methods
                are implemented in an R package, mr.ash.alpha, available from https://github.com/
                stephenslab/mr.ash.alpha.
                Keywords:
                Empirical Bayes, variational inference, normal means, penalized linear re-
                gression, nonconvex optimization[0m

Box rectangle:  [32m(90.0, 658.0) -> (180.3, 669.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 681.1) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMultiple linear regression is one of the oldest statistical methods for relating an outcome
                variable to predictor variables, dating back at least to the eighteenth century (Stigler,[0m

Box rectangle:  [32m(90.0, 726.5) -> (380.0, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Youngseok Kim, Wei Wang, Peter Carbonetto and Matthew Stephens.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0953.html.[0m



=== Processing ../JMLR 2024/A Framework for Improving the Reliability of Black-box Variational Inference.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Framework for Improving the Reliability of Black-box Variational Inference.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-71
                Submitted 3/22; Revised 4/24; Published 7/24[0m

Box rectangle:  [32m(144.1, 101.6) -> (468.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Framework for Improving the Reliability of
                Black-box Variational Inference[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mManushi Welandawe
                manushiw@bu.edu
                Department of Mathematics & Statistics
                Boston University, USA[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael Riis Andersen
                miri@dtu.dk
                DTU Compute
                Technical University of Denmark, Denmark[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAki Vehtari
                aki.vehtari@aalto.fi
                Department of Computer Science
                Aalto University, Finland[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 331.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonathan H. Huggins
                huggins@bu.edu
                Department of Mathematics & Statistics
                Faculty of Computing & Data Sciences
                Boston University, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (433.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, and Jonathan H. Huggins.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0327.html.[0m



=== Processing ../JMLR 2024/A General Framework for the Analysis of Kernel-based Tests.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A General Framework for the Analysis of Kernel-based Tests.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 7/23; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA General Framework for the Analysis of Kernel-based Tests[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTamara Fern ́andez
                tamara.fernandez@uai.cl
                Faculty of Engineering and Science
                Universidad Adolfo Ib ́a ̃nez, Chile[0m

Box rectangle:  [32m(90.0, 177.1) -> (522.0, 216.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicol ́as Rivera
                nicolas.rivera@uv.cl
                Facultad de Ciencias
                Universidad de Valpara ́ıso, Chile[0m

Box rectangle:  [32m(90.0, 726.3) -> (262.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Tamara Fern ́andez and Nicol ́as Rivera.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0985.html.[0m



=== Processing ../JMLR 2024/A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 11/21; Revised 2/24; Published 5/24[0m

Box rectangle:  [32m(161.0, 101.6) -> (451.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Kernel Test for Causal Association via
                Noise Contrastive Backdoor Adjustment[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobert Hu
                robyhu@amazon.co.uk
                Amazon[0m

Box rectangle:  [32m(90.0, 181.5) -> (521.9, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDino Sejdinovic
                dino.sejdinovic@adelaide.edu.au
                School of Computer and Mathematical Sciences
                University of Adelaide
                Adelaide 5005, Australia[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 289.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobin J. Evans
                evans@stats.ox.ac.uk
                Department of Statistics
                University of Oxford
                Oxford OX1 3LB, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (275.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Robert Hu, Dino Sejdinovic, Robin Evans.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1409.html.[0m



=== Processing ../JMLR 2024/Almost Sure Convergence Rates Analysis and Saddle Avoidance of Stochastic Gradient Methods.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Almost Sure Convergence Rates Analysis and Saddle Avoidance of Stochastic Gradient Methods.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 11/23; Revised 6/24; Published 9/24[0m

Box rectangle:  [32m(122.1, 101.5) -> (489.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlmost Sure Convergence Rates Analysis and Saddle
                Avoidance of Stochastic Gradient Methods[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun Liu∗
                j.liu@uwaterloo.ca
                Department of Applied Mathematics, University of Waterloo, Waterloo, Canada[0m

Box rectangle:  [32m(90.0, 182.7) -> (522.1, 222.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYe Yuan∗
                yye@hust.edu.cn
                School of Artificial Intelligence and Automation, Huazhong University of Science and Technology,
                Wuhan, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (200.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jun Liu and Ye Yuan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1436.html.[0m



=== Processing ../JMLR 2024/A minimax optimal approach to high-dimensional double sparse linear regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A minimax optimal approach to high-dimensional double sparse linear regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 5/23; Revised 6/24; Published 12/24[0m

Box rectangle:  [32m(104.2, 101.6) -> (508.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA minimax optimal approach to high-dimensional double
                sparse linear regression[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYanhang Zhang
                zhangyh98@ruc.edu.cn
                School of Statistics, Renmin University of China
                100872 Beijing, China[0m

Box rectangle:  [32m(90.0, 193.2) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhifan Li ∗
                zhifanli@bimsa.cn
                Beijing Institute of Mathematical Sciences and Applications
                101408 Beijing, China[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShixiang Liu
                liushixiang stat@ruc.edu.cn
                School of Statistics, Renmin University of China
                100872 Beijing, China[0m

Box rectangle:  [32m(90.0, 278.0) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJianxin Yin †
                jyin@ruc.edu.cn
                Center for Applied Statistics and School of Statistics, Renmin University of China
                100872 Beijing, China[0m

Box rectangle:  [32m(90.0, 342.8) -> (177.3, 352.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Daniel Hsu[0m

Box rectangle:  [32m(280.3, 376.3) -> (331.7, 388.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 394.6) -> (502.2, 586.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this paper, we focus our attention on the high-dimensional double sparse linear re-
                gression, that is, a combination of element-wise and group-wise sparsity. To address this
                problem, we propose an IHT-style (iterative hard thresholding) procedure that dynamically
                updates the threshold at each step. We establish the matching upper and lower bounds for
                parameter estimation, showing the optimality of our proposal in the minimax sense. More
                importantly, we introduce a fully adaptive optimal procedure designed to address unknown
                sparsity and noise levels. Our adaptive procedure demonstrates optimal statistical accu-
                racy with fast convergence. Additionally, we elucidate the significance of the element-wise
                sparsity level s0 as the trade-offbetween IHT and group IHT, underscoring the superior
                performance of our method over both. Leveraging the beta-min condition, we establish
                that our IHT-style procedure can attain the oracle estimation rate and achieve almost full
                recovery of the true support set at both the element level and group level. Finally, we
                demonstrate the superiority of our method by comparing it with several state-of-the-art
                algorithms on both synthetic and real-world datasets.
                Keywords:
                double sparsity, iterative hard thresholding, minimax optimality, fully adap-
                tive procedure, oracle estimation rate.[0m

Box rectangle:  [32m(90.0, 607.7) -> (180.3, 619.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 630.7) -> (522.1, 668.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOver the last decade, the rapid growth of high-dimensional data has drawn broad at-
                tention to sparse learning across many scientific communities, with plenty of remarkable
                achievements in algorithms, theory, and applications. One of the well-studied problems[0m

Box rectangle:  [32m(93.7, 684.8) -> (362.2, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Yanhang Zhang and Zhifan Li contributed equally to this work.
                †. Corresponding author[0m

Box rectangle:  [32m(90.0, 726.4) -> (334.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yanhang Zhang, Zhifan Li, Shixiang Liu and Jianxin Yin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0653.html.[0m



=== Processing ../JMLR 2024/AMLB  an AutoML Benchmark.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/AMLB  an AutoML Benchmark.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 5/22; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(192.7, 101.6) -> (419.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAMLB: an AutoML Benchmark[0m

Box rectangle:  [32m(90.0, 133.7) -> (518.2, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPieter Gijsbers1
                p.gijsbers@tue.nl[0m

Box rectangle:  [32m(90.0, 151.4) -> (518.2, 164.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarcos L. P. Bueno1,4
                marcos.depaulabueno@donders.ru.nl[0m

Box rectangle:  [32m(90.0, 169.1) -> (518.2, 181.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStefan Coors2
                stefan.coors@stat.uni-muenchen.de[0m

Box rectangle:  [32m(90.0, 186.8) -> (518.2, 199.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErin LeDell3
                erin@h2o.ai[0m

Box rectangle:  [32m(90.0, 204.6) -> (518.2, 217.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mS ́ebastien Poirier3
                sebastien@h2o.ai[0m

Box rectangle:  [32m(90.0, 222.3) -> (518.2, 234.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJanek Thomas2
                janek.thomas@stat.uni-muenchen.de[0m

Box rectangle:  [32m(90.0, 240.0) -> (518.2, 252.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBernd Bischl2
                bernd.bischl@stat.uni-muenchen.de[0m

Box rectangle:  [32m(90.0, 257.7) -> (522.0, 270.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoaquin Vanschoren1
                j.vanschoren@tue.nl[0m

Box rectangle:  [32m(90.0, 290.8) -> (445.2, 345.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Eindhoven University of Technology, Eindhoven, The Netherlands
                2 Ludwig Maximilian University of Munich, Munich, Germany
                3 H2O.ai, Mountain View, CA, United States
                4 Radboud University, Nijmegen, The Netherlands[0m

Box rectangle:  [32m(90.0, 726.3) -> (493.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 P. Gijsbers, M. L. P. Bueno, S. Coors, E. LeDell, S. Poirier, J. Thomas, B. Bischl and J. Vanschoren.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0493.html.[0m



=== Processing ../JMLR 2024/A Multilabel Classification Framework for Approximate Nearest Neighbor Search.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Multilabel Classification Framework for Approximate Nearest Neighbor Search.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 3/23; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(108.3, 101.6) -> (503.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Multilabel Classification Framework for Approximate
                Nearest Neighbor Search[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVille Hyv ̈onen
                ville.2.hyvonen@aalto.fi
                Department of Computer Science
                Aalto University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mElias J ̈a ̈asaari
                ejaeaesa@andrew.cmu.edu
                Machine Learning Department
                Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 275.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTeemu Roos
                teemu.roos@helsinki.fi
                Department of Computer Science
                University of Helsinki[0m

Box rectangle:  [32m(90.0, 726.3) -> (296.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ville Hyv ̈onen, Elias J ̈a ̈asaari, and Teemu Roos.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0286.html.[0m



=== Processing ../JMLR 2024/An Algorithmic Framework for the Optimization of Deep Neural Networks Architectures and Hyperparameters.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Algorithmic Framework for the Optimization of Deep Neural Networks Architectures and Hyperparameters.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 2/23; Revised 4/24; Published 6/24[0m

Box rectangle:  [32m(103.3, 101.6) -> (508.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Algorithmic Framework for the Optimization of Deep
                Neural Networks Architectures and Hyperparameters[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJulie Keisler
                julie.keisler@edf.fr
                EDF Lab Paris-Saclay
                Bd Gaspard Monge, 91120 Palaiseau
                University of Lille & INRIA
                170 Av. de Bretagne, 59000 Lille[0m

Box rectangle:  [32m(90.0, 217.4) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEl-Ghazali Talbi
                el-ghazali.talbi@univ-lille.fr
                University of Lille & INRIA
                170 Av. de Bretagne, 59000 Lille[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSandra Claudel
                sandra.claudel@edf.fr
                EDF Lab Paris-Saclay
                Bd Gaspard Monge, 91120 Palaiseau[0m

Box rectangle:  [32m(90.0, 302.2) -> (522.0, 341.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGilles Cabriel
                gilles.cabriel@edf.fr
                EDF Lab Paris-Saclay
                Bd Gaspard Monge, 91120 Palaiseau[0m

Box rectangle:  [32m(90.0, 366.7) -> (224.4, 376.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ruslan Salakhutdinov[0m

Box rectangle:  [32m(280.3, 400.3) -> (331.7, 412.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 418.3) -> (502.1, 598.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this paper, we propose DRAGON (for DiRected Acyclic Graph OptimizatioN), an algo-
                rithmic framework to automatically generate efficient deep neural networks architectures
                and optimize their associated hyperparameters. The framework is based on evolving Di-
                rected Acyclic Graphs (DAGs), defining a more flexible search space than the existing ones
                in the literature. It allows mixtures of different classical operations: convolutions, recur-
                rences and dense layers, but also more newfangled operations such as self-attention. Based
                on this search space we propose neighbourhood and evolution search operators to opti-
                mize both the architecture and hyper-parameters of our networks. These search operators
                can be used with any metaheuristic capable of handling mixed search spaces. We tested
                our algorithmic framework with an asynchronous evolutionary algorithm on a time series
                forecasting benchmark. The results demonstrate that DRAGON outperforms state-of-the-
                art handcrafted models and AutoML techniques for time series forecasting on numerous
                datasets. DRAGON has been implemented as a python open-source package1.
                Keywords:
                neural architecture search, hyperparameters optimization, metaheuristics,
                evolutionary algorithm, time series forecasting[0m

Box rectangle:  [32m(90.0, 619.0) -> (180.3, 631.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 641.9) -> (522.0, 679.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWith the recent successes of deep learning in many research fields, deep neural networks
                (DNN) optimization stimulates the growing interest of the scientific community (Talbi,
                2021). While each new learning task requires the handcrafted design of a new DNN, auto-[0m

Box rectangle:  [32m(93.7, 695.9) -> (383.2, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. https://dragon-tutorial.readthedocs.io/en/latest/index.html[0m

Box rectangle:  [32m(90.0, 726.4) -> (365.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Julie Keisler, El-Ghazali Talbi, Sandra Claudel and Gilles Cabriel.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0166.html.[0m



=== Processing ../JMLR 2024/An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-14
                Submitted 9/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(94.4, 101.6) -> (517.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Algorithm with Optimal Dimension-Dependence for
                Zero-Order Nonsmooth Nonconvex Stochastic Optimization[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuy Kornowski
                guy.kornowski@weizmann.ac.il
                Department of Computer Science and Applied Mathematics
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOhad Shamir
                ohad.shamir@weizmann.ac.il
                Department of Computer Science and Applied Mathematics
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (247.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Guy Kornowski and Ohad Shamir.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1159.html.[0m



=== Processing ../JMLR 2024/An Analysis of Quantile Temporal-Difference Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Analysis of Quantile Temporal-Difference Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 2/23; Published 5/24[0m

Box rectangle:  [32m(113.1, 96.4) -> (499.0, 110.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Analysis of Quantile Temporal-Difference Learning[0m

Box rectangle:  [32m(90.0, 123.6) -> (522.0, 147.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMark Rowland
                markrowland@google.com
                Google DeepMind, London, UK[0m

Box rectangle:  [32m(90.0, 160.0) -> (522.0, 184.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mR ́emi Munos
                munos@google.com
                Google DeepMind, Paris, France[0m

Box rectangle:  [32m(90.0, 196.5) -> (522.0, 220.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMohammad Gheshlaghi Azar
                mazar@google.com
                Google DeepMind, Seattle, USA[0m

Box rectangle:  [32m(90.0, 232.9) -> (522.0, 257.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunhao Tang
                robintyh@google.com
                Google DeepMind, London, UK[0m

Box rectangle:  [32m(90.0, 269.3) -> (522.0, 293.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorg Ostrovski
                ostrovski@google.com
                Google DeepMind, London, UK[0m

Box rectangle:  [32m(90.0, 305.8) -> (522.0, 329.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Harutyunyan
                harutyunyan@google.com
                Google DeepMind, London, UK[0m

Box rectangle:  [32m(90.0, 342.2) -> (522.0, 366.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKarl Tuyls
                ktuyls@gmail.com
                Google DeepMind, Paris, France[0m

Box rectangle:  [32m(90.0, 378.7) -> (522.0, 402.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarc G. Bellemare
                marc.g.bellemare@gmail.com
                Reliant AI & McGill University, Montr ́eal, Canada[0m

Box rectangle:  [32m(90.0, 416.7) -> (522.0, 442.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWill Dabney
                wdabney@google.com
                Google DeepMind, Seattle, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (483.9, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mark Rowland, R ́emi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna
                Harutyunyan, Karl Tuyls, Marc G. Bellemare, Will Dabney.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0154.html.[0m



=== Processing ../JMLR 2024/An Asymptotic Study of Discriminant and Vote-Averaging Schemes for Randomly-Projected Linear Discriminants.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Asymptotic Study of Discriminant and Vote-Averaging Schemes for Randomly-Projected Linear Discriminants.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 11/22; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(97.9, 101.6) -> (514.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Asymptotic Study of Discriminant and Vote-Averaging
                Schemes for Randomly-Projected Linear Discriminants[0m

Box rectangle:  [32m(90.0, 151.9) -> (518.2, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLama B. Niyazi
                lama.niyazi@kaust.edu.sa[0m

Box rectangle:  [32m(90.0, 171.8) -> (317.8, 193.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKing Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 199.2) -> (518.2, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbla Kammoun
                abla.kammoun@kaust.edu.sa[0m

Box rectangle:  [32m(90.0, 219.1) -> (317.8, 241.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKing Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 246.6) -> (518.2, 259.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHayssam Dahrouj
                hayssam.dahrouj@gmail.com[0m

Box rectangle:  [32m(90.0, 266.5) -> (224.7, 288.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUniversity of Sharjah
                Sharjah, United Arab Emirates[0m

Box rectangle:  [32m(90.0, 294.0) -> (518.2, 306.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMohamed-Slim Alouini
                slim.alouini@kaust.edu.sa[0m

Box rectangle:  [32m(90.0, 313.9) -> (317.8, 335.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKing Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 341.4) -> (518.2, 353.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTareq Y. Al-Naffouri
                tareq.alnaffouri@kaust.edu.sa[0m

Box rectangle:  [32m(90.0, 362.9) -> (317.8, 386.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKing Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 411.6) -> (204.0, 421.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Marc Schoenauer[0m

Box rectangle:  [32m(280.3, 445.2) -> (331.7, 457.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 476.3) -> (502.1, 665.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mModern technology has contributed to the rise of high-dimensional data in various do-
                mains such as bio-informatics, chemometrics, and face recognition. In the recent literature,
                random projections and, in particular, randomly-projected ensembles based on the clas-
                sical Linear Discriminant Analysis (LDA), have been proposed for classification problems
                involving such high-dimensional data.
                In this work, we study the two main classes of
                randomly-projected LDA ensemble classifiers, namely discriminant averaging and vote av-
                eraging. Through asymptotic analysis in a growth regime where the problem dimensions
                are assumed to grow at constant rates to each other for a fixed ensemble size, we determine
                the exact mechanism through which the ensemble size affects the classification performance.
                Furthermore, we investigate whether projection selection truly matters in an ensemble set-
                ting, and, ultimately, derive the optimal form of the randomly-projected LDA ensemble.
                Motivated by these findings, we propose a framework for efficient tuning of the optimal
                classifier’s ensemble size and projection dimension based on an estimator of the classifier
                probability of misclassification which is consistent under the assumed growth regime. The
                proposed framework is shown to outperform the existing rule-of-thumb, as well as other
                methods for parameter tuning, on both real and synthetic data.[0m

Box rectangle:  [32m(109.9, 683.1) -> (502.1, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: linear discriminant analysis, random projection, high-dimensional data, small
                sample size, classification ensembles, random matrix theory[0m

Box rectangle:  [32m(90.0, 726.4) -> (504.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Lama B. Niyazi, Abla Kammoun, Hayssam Dahrouj, Mohamed-Slim Alouini, and Tareq Y. Al-Naffouri.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1367.html.[0m



=== Processing ../JMLR 2024/An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-60
                Submitted 6/22; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(100.9, 101.4) -> (511.3, 133.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Embedding Framework for the Design and Analysis of
                Consistent Polyhedral Surrogates[0m

Box rectangle:  [32m(88.8, 151.4) -> (522.0, 199.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJessie Finocchiaro∗
                jefi8453@colorado.edu
                Department of Computer Science
                Boston College
                Chestnut Hill, MA, USA[0m

Box rectangle:  [32m(88.3, 205.3) -> (522.0, 253.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRafael M. Frongillo
                raf@colorado.edu
                Department of Computer Science
                University of Colorado Boulder
                Boulder, CO, USA[0m

Box rectangle:  [32m(88.3, 260.5) -> (522.0, 313.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBo Waggoner
                bwag@colorado.edu
                Department of Computer Science
                University of Colorado Boulder
                Boulder, CO, USA[0m

Box rectangle:  [32m(90.0, 338.3) -> (181.3, 348.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Tong Zhang[0m

Box rectangle:  [32m(280.3, 374.0) -> (331.7, 385.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 390.6) -> (504.2, 593.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe formalize and study the natural approach of designing convex surrogate loss functions via
                embeddings, for discrete problems such as classification, ranking, or structured prediction.
                In this approach, one embeds each of the finitely many predictions (e.g. rankings) as a
                point in Rd, assigns the original loss values to these points, and “convexifies” the loss in
                some way to obtain a surrogate. We establish a strong connection between this approach
                and polyhedral (piecewise-linear convex) surrogate losses: every discrete loss is embedded
                by some polyhedral loss, and every polyhedral loss embeds some discrete loss. Moreover,
                an embedding gives rise to a consistent link function as well as linear surrogate regret
                bounds. Our results are constructive, as we illustrate with several examples. In particular,
                our framework gives succinct proofs of consistency or inconsistency for existing polyhedral
                surrogates, and for inconsistent surrogates, it further reveals the discrete losses for which
                these surrogates are consistent. We go on to show additional structure of embeddings,
                such as the equivalence of embedding and matching Bayes risks, and the equivalence of
                various notions of non-redudancy. Using these results, we establish that indirect elicitation,
                a necessary condition for consistency, is also sufficient when working with polyhedral
                surrogates.
                Keywords: Statistical consistency, surrogate loss functions, calibration, property elicitation[0m

Box rectangle:  [32m(90.0, 612.9) -> (180.3, 624.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 634.3) -> (524.4, 685.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn supervised learning, one tries to learn a hypothesis which fits labeled data as judged
                by a target loss function. Minimizing the target loss directly is typically computationally
                intractable for discrete prediction tasks like classification, ranking, and structured prediction.
                Instead, one typically minimizes a surrogate loss which is convex and therefore efficiently[0m

Box rectangle:  [32m(93.7, 695.9) -> (369.7, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Work done while a student at the University of Colorado Boulder[0m

Box rectangle:  [32m(89.5, 726.5) -> (315.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Jessie Finocchiaro, Rafael M. Frongillo, Bo Waggoner.[0m

Box rectangle:  [32m(90.0, 741.0) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0743.html.[0m



=== Processing ../JMLR 2024/An Entropy-Based Model for Hierarchical Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Entropy-Based Model for Hierarchical Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 1/23; Revised 6/24; Published 6/24[0m

Box rectangle:  [32m(124.0, 101.6) -> (488.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Entropy-Based Model for Hierarchical Learning[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 215.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmir R. Asadi
                asadi@statslab.cam.ac.uk
                Statistical Laboratory,
                Centre for Mathematical Sciences,
                University of Cambridge,
                Cambridge CB3 0WA
                United Kingdom[0m

Box rectangle:  [32m(90.0, 726.3) -> (174.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Amir R. Asadi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0096.html.[0m



=== Processing ../JMLR 2024/A New  Physics-Informed Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A New  Physics-Informed Continuous-Time Reinforcement Learning Algorithm with Performance Guarantees.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 1/24; Revised 7/24; Published 12/24[0m

Box rectangle:  [32m(97.8, 101.6) -> (514.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA New, Physics-Informed Continuous-Time Reinforcement
                Learning Algorithm with Performance Guarantees[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBrent A. Wallace
                bawalla2@asu.edu
                Department of Electrical Engineering
                Arizona State University
                Tempe, AZ 85287, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJennie Si
                si@asu.edu
                Department of Electrical Engineering
                Arizona State University
                Tempe, AZ 85287, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (237.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Brent A. Wallace and Jennie Si.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0017.html.[0m



=== Processing ../JMLR 2024/An Inexact Projected Regularized Newton Method for Fused Zero-norms Regularization  Problems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Inexact Projected Regularized Newton Method for Fused Zero-norms Regularization  Problems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 12/23; Revised 9/24; Published 11/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Inexact Projected Regularized Newton Method for Fused
                Zero-norms Regularization Problems[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuqia Wu
                yuqia.wu@connect.polyu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Kowloon, Hong Kong[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShaohua Pan
                shhpan@scut.edu.cn
                School of Mathematics
                South China University of Technology
                Guangzhou, China[0m

Box rectangle:  [32m(90.0, 260.3) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaoqi Yang∗
                mayangxq@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Kowloon, Hong Kong[0m

Box rectangle:  [32m(90.0, 726.3) -> (276.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuqia Wu, Shaohua Pan and Xiaoqi Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1700.html.[0m



=== Processing ../JMLR 2024/An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 2/24; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(91.7, 101.6) -> (520.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn Optimal Transport Approach for Computing Adversarial
                Training Lower Bounds in Multiclass Classification[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicol ́as Garc ́ıa Trillos
                garciatrillo@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                1300 University Avenue, Madison, Wisconsin 53706, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatt Jacobs
                majaco@ucsb.edu
                Department of Mathematics
                UC Santa Barbara
                552 University Rd, Isla Vista, CA 93117, USA[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJakwang Kim
                jakwang.kim@math.ubc.ca
                Department of Mathematics
                University of British Columbia
                1984 Mathematics Road, Vancouver, British Columbia, V6T 1Z2, Canada[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew Werenski
                matthew.werenski@tufts.edu
                Department of Computer Science
                Tufts University
                420 Joyce Cummings Center, 177 College Avenue, Medford, MA 02155, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (396.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nicol ́as Garc ́ıa Trillos, Matt Jacobs, Jakwang Kim and Matthew Werenski.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0268.html.[0m



=== Processing ../JMLR 2024/A Note on Entrywise Consistency for Mixed-data Matrix Completion.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Note on Entrywise Consistency for Mixed-data Matrix Completion.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 6/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(95.0, 101.7) -> (517.0, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Note on Entrywise Consistency for Mixed-data Matrix Completion[0m

Box rectangle:  [32m(90.0, 135.4) -> (521.8, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunxiao Chen
                Y.CHEN186@LSE.AC.UK
                Department of Statistics
                London School of Economics and Political Science
                London WC2A 2AE, UK[0m

Box rectangle:  [32m(90.0, 190.6) -> (521.8, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaoou Li
                LIXX1766@UMN.EDU
                School of Statistics
                University of Minnesota
                Minneapolis, MN 55455[0m

Box rectangle:  [32m(90.0, 726.4) -> (211.8, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yunxiao Chen and Xiaoou Li.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0834.html.[0m



=== Processing ../JMLR 2024/A PDE-based Explanation of Extreme Numerical Sensitivities and Edge of Stability in Training Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A PDE-based Explanation of Extreme Numerical Sensitivities and Edge of Stability in Training Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 2/23; Published 4/24[0m

Box rectangle:  [32m(91.2, 101.5) -> (520.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA PDE-based Explanation of Extreme Numerical Sensitivities
                and Edge of Stability in Training Neural Networks[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuxin Sun
                syuxin3@gatech.edu
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 193.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDong Lao
                lao@cs.ucla.edu
                University of California
                Los Angeles, CA 90095, USA[0m

Box rectangle:  [32m(90.0, 235.0) -> (522.0, 271.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnthony Yezzi
                ayezzi@gatech.edu
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGanesh Sundaramoorthi
                ganesh.sundaramoorthi@rtx.com
                Raytheon Technologies
                East Hartford, CT 06108, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (371.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuxin Sun, Dong Lao, Anthony Yezzi and Ganesh Sundaramoorthi..[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0137.html.[0m



=== Processing ../JMLR 2024/Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Approximate Bayesian inference from noisy likelihoods with Gaussian process emulated MCMC.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 4/21; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(93.4, 101.6) -> (518.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mApproximate Bayesian inference from noisy likelihoods with
                Gaussian process emulated MCMC[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarko J ̈arvenp ̈a ̈a
                m.j.jarvenpaa@medisin.uio.no
                Department of Biostatistics, University of Oslo, Norway[0m

Box rectangle:  [32m(90.0, 183.1) -> (522.0, 235.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJukka Corander
                jukka.corander@medisin.uio.no
                Department of Biostatistics, University of Oslo, Norway
                Department of Mathematics and Statistics, University of Helsinki, Finland
                Wellcome Sanger Institute, United Kingdom[0m

Box rectangle:  [32m(90.0, 726.3) -> (249.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Marko J ̈arvenp ̈a ̈a, Jukka Corander.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0421.html.[0m



=== Processing ../JMLR 2024/Approximate Information Tests on Statistical Submanifolds.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Approximate Information Tests on Statistical Submanifolds.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 4/19; Revised 2/24; Published 12/24[0m

Box rectangle:  [32m(95.5, 101.6) -> (516.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mApproximate Information Tests on Statistical Submanifolds[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael W. Trosset
                mtrosset@iu.edu
                Department of Statistics
                Indiana University
                Bloomington, IN 47408, USA[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCarey E. Priebe
                cep@jhu.edu
                Department of Applied Mathematics & Statistics
                Johns Hopkins University
                Baltimore, MD 21218-2682, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (257.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Michael W. Trosset, Carey E. Priebe.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/19-272.html.[0m



=== Processing ../JMLR 2024/A projected semismooth Newton method for a class of nonconvex composite programs with strong prox-regularity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A projected semismooth Newton method for a class of nonconvex composite programs with strong prox-regularity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 3/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(96.6, 101.6) -> (515.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA projected semismooth Newton method for a class of
                nonconvex composite programs with strong prox-regularity[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiang Hu
                hujiangopt@gmail.com
                Massachusetts General Hospital and Harvard Medical School
                Harvard University
                Boston, MA 02114, USA[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKangkang Deng∗
                freedeng1208@gmail.com
                Beijing International Center for Mathematical Research
                Peking University
                Beijing, 100871, China[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiayuan Wu
                1901110043@pku.edu.cn
                College of Engineering
                Peking University
                Beijing, 100871, China[0m

Box rectangle:  [32m(90.0, 314.2) -> (521.9, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQuanzheng Li
                li.quanzheng@mgh.harvard.edu
                Massachusetts General Hospital and Harvard Medical School
                Harvard University
                Boston, MA 02114, USA[0m

Box rectangle:  [32m(90.0, 392.2) -> (240.6, 402.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Animashree Anandkumar[0m

Box rectangle:  [32m(280.3, 425.8) -> (331.7, 437.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 445.6) -> (502.2, 646.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThis paper aims to develop a Newton-type method to solve a class of nonconvex composite
                programs. In particular, the nonsmooth part is possibly nonconvex. To tackle the non-
                convexity, we develop a notion of strong prox-regularity which is related to the singleton
                property and Lipschitz continuity of the associated proximal operator, and we verify it
                in various classes of functions, including weakly convex functions, indicator functions of
                proximally smooth sets, and two specific sphere-related nonconvex nonsmooth functions.
                In this case, the problem class we are concerned with covers smooth optimization problems
                on manifold and certain composite optimization problems on manifold.
                For the latter,
                the proposed algorithm is the first second-order type method. Combining with the semis-
                moothness of the proximal operator, we design a projected semismooth Newton method
                to find a root of the natural residual induced by the proximal gradient method. Due to
                the possible nonconvexity of the feasible domain, an extra projection is added to the usual
                semismooth Newton step and new criteria are proposed for the switching between the pro-
                jected semismooth Newton step and the proximal step. The global convergence is then
                established under the strong prox-regularity. Based on the BD regularity condition, we es-
                tablish local superlinear convergence. Numerical experiments demonstrate the effectiveness
                of our proposed method compared with state-of-the-art ones.[0m

Box rectangle:  [32m(109.9, 653.1) -> (502.1, 675.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                nonconvex composite optimization, strong prox-regularity, projected semis-
                mooth Newton method, superlinear convergence[0m

Box rectangle:  [32m(93.7, 695.7) -> (196.0, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Corresponding author.[0m

Box rectangle:  [32m(90.0, 726.4) -> (339.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Jiang Hu, Kangkang Deng, Jiayuan Wu and Quanzheng Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0371.html.[0m



=== Processing ../JMLR 2024/A Rainbow in Deep Network Black Boxes.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Rainbow in Deep Network Black Boxes.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 11/23; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(157.9, 101.4) -> (454.2, 115.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Rainbow in Deep Network Black Boxes[0m

Box rectangle:  [32m(90.0, 132.1) -> (522.0, 169.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFlorentin Guth∗
                florentin.guth@nyu.edu
                Center for Data Science, New York University, 60 5th Avenue, New York, NY 10011, USA
                Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA[0m

Box rectangle:  [32m(90.0, 175.4) -> (522.0, 211.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBrice Ménard
                menard@jhu.edu
                Department of Physics & Astronomy, Johns Hopkins University
                Baltimore, MD 21218, USA[0m

Box rectangle:  [32m(90.0, 217.0) -> (522.0, 253.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGaspar Rochette
                gaspar.rochette@ens.fr
                Département d’informatique, École Normale Supérieure, CNRS, PSL University
                45 rue d’Ulm, 75005 Paris, France[0m

Box rectangle:  [32m(90.0, 260.2) -> (522.0, 299.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStéphane Mallat
                stephane.mallat@ens.fr
                Collège de France, 11, place Marcelin-Berthelot 75231 Paris, France
                Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (382.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Florentin Guth, Brice Ménard, Gaspar Rochette, and Stéphane Mallat.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1573.html.[0m



=== Processing ../JMLR 2024/A Random Projection Approach to Personalized Federated Learning  Enhancing Communication Efficiency  Robustness  and Fairness.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Random Projection Approach to Personalized Federated Learning  Enhancing Communication Efficiency  Robustness  and Fairness.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-88
                Submitted 2/23; Revised 12/24; Published 12/24[0m

Box rectangle:  [32m(92.0, 101.6) -> (522.4, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Random Projection Approach to Personalized Federated
                Learning: Enhancing Communication Efficiency, Robustness,
                and Fairness[0m

Box rectangle:  [32m(88.8, 169.5) -> (522.0, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuze Han∗
                hanyuze97@ruc.edu.cn
                Center for Applied Statistics and School of Statistics
                Renmin University of China
                Beijing, China[0m

Box rectangle:  [32m(89.4, 223.4) -> (522.0, 271.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiang Li
                lx10077@pku.edu.cn
                School of Mathematical Sciences
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(89.4, 277.0) -> (522.0, 325.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShiyun Lin
                shiyunlin@stu.pku.edu.cn
                School of Mathematical Sciences
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(89.4, 332.1) -> (522.0, 384.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhihua Zhang
                zhzhang@math.pku.edu.cn
                Schools of Mathematical Sciences and of Computer Science
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (313.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuze Han, Xiang Li, Shiyun Lin, and Zhihua Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0215.html.[0m



=== Processing ../JMLR 2024/A Semi-parametric Estimation of Personalized Dose-response Function Using Instrumental Variables.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Semi-parametric Estimation of Personalized Dose-response Function Using Instrumental Variables.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 10/21; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Semi-parametric Estimation of Personalized Dose-response
                Function Using Instrumental Variables[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWei Luo
                weiluo@zju.edu.cn
                Center for Data Science
                Zhejiang University
                Hangzhou, P.R.China[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYeying Zhu
                yeying.zhu@uwaterloo.ca
                Department of Statistics and Actuarial Science
                University of Waterloo
                Waterloo, ON N2L 3G1, Canada[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuekui Zhang
                xuekui@uvic.ca
                Department of Mathematics and Statistics
                University of Victoria
                Victoria, BC V8P 5C2, Canada[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLin Lin
                l.lin@duke.edu
                Department of Biostatistics and Bioinformatics
                Duke University
                Durham, NC 27710, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (303.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Wei Luo, Yeying Zhu, Xuekui Zhang and Lin Lin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1181.html.[0m



=== Processing ../JMLR 2024/Assessing the Overall and Partial Causal Well-Specification of Nonlinear Additive Noise Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Assessing the Overall and Partial Causal Well-Specification of Nonlinear Additive Noise Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 10/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(96.2, 101.6) -> (516.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAssessing the Overall and Partial Causal Well-Specification
                of Nonlinear Additive Noise Models[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristoph Schultheiss
                christoph.schultheiss@stat.math.ethz.ch
                Seminar for Statistics
                ETH Z ̈urich
                Z ̈urich, 8092, CH[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter B ̈uhlmann
                peter.buehlmann@stat.math.ethz.ch
                Seminar for Statistics
                ETH Z ̈urich
                Z ̈urich, 8092, CH[0m

Box rectangle:  [32m(90.0, 726.3) -> (280.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Christoph Schultheiss and Peter B ̈uhlmann.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1397.html.[0m



=== Processing ../JMLR 2024/A Statistical Experimental Design Method for Constructing Deterministic Sensing Matrices for Compressed Sensing.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Statistical Experimental Design Method for Constructing Deterministic Sensing Matrices for Compressed Sensing.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-28
                Submitted 7/22; Published 9/24[0m

Box rectangle:  [32m(93.8, 101.6) -> (518.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Statistical Experimental Design Method for Constructing
                Deterministic Sensing Matrices for Compressed Sensing[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYouran Qi
                yqi28@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXu He
                hexu@amss.ac.cn
                Academy of Mathematics and Systems Science
                Chinese Academy of Sciences
                Beijing, 100190, China[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTzu-Hsiang Hung
                thung6@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Chien
                peter.chien@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (322.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Youran Qi, Xu He, Tzu-Hsiang Hung and Peter Chien.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0760.html.[0m



=== Processing ../JMLR 2024/A Survey on Multi-player Bandits.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Survey on Multi-player Bandits.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 6/22; Revised 10/23; Published 1/24[0m

Box rectangle:  [32m(185.3, 101.6) -> (426.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Survey on Multi-player Bandits[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEtienne Boursier
                etienne.boursier1@gmail.com
                INRIA, Universit ́e Paris Saclay, LMO, Orsay, France[0m

Box rectangle:  [32m(90.0, 165.2) -> (522.0, 204.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVianney Perchet
                vianney.perchet@normalesup.org
                CREST, ENSAE Paris, Palaiseau, France
                CRITEO AI Lab, Paris, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (262.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Boursier Etienne and Perchet Vianney.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0643.html.[0m



=== Processing ../JMLR 2024/A tensor factorization model of multilayer network interdependence.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A tensor factorization model of multilayer network interdependence.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 2/23; Revised 7/24; Published 9/24[0m

Box rectangle:  [32m(126.1, 101.6) -> (486.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA tensor factorization model of multilayer network
                interdependence[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIzabel Aguiar
                izabel.p.aguiar@gmail.com
                Institute for Computational and Mathematical Engineering
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDane Taylor
                dane.taylor@uwyo.edu
                School of Computing
                Department of Mathematics and Statistics
                University of Wyoming
                Laramie, WY 82071, USA[0m

Box rectangle:  [32m(90.0, 272.6) -> (522.0, 338.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohan Ugander
                jugander@stanford.edu
                Department of Management Science and Engineering
                Institute for Computational and Mathematical Engineering
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (299.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Izabel Aguiar, Dane Taylor, and Johan Ugander.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0205.html.[0m



=== Processing ../JMLR 2024/A Variational Approach to Bayesian Phylogenetic Inference.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/A Variational Approach to Bayesian Phylogenetic Inference.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 4/22; Revised 4/23; Published 5/24[0m

Box rectangle:  [32m(95.1, 101.6) -> (517.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA Variational Approach to Bayesian Phylogenetic Inference[0m

Box rectangle:  [32m(89.4, 133.6) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCheng Zhang∗
                chengzhang@math.pku.edu.cn
                School of Mathematical Sciences and Center for Statistical Science
                Peking University
                Beijing, 100871, China[0m

Box rectangle:  [32m(88.3, 189.1) -> (522.0, 296.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrederick A. Matsen IV
                matsen@fredhutch.org
                Howard Hughes Medical Institute
                Computational Biology Program
                Fred Hutchinson Cancer Research Center
                Seattle, WA 98109, USA
                Department of Genome Sciences and Department of Statistics
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (276.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Cheng Zhang and Frederick A. Matsen IV.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0348.html.[0m



=== Processing ../JMLR 2024/Axiomatic effect propagation in structural causal models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Axiomatic effect propagation in structural causal models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-71
                Submitted 3/22; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(105.3, 101.6) -> (506.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAxiomatic effect propagation in structural causal models[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRaghav Singal
                singal@dartmouth.edu
                Operations and Management Science
                Tuck School of Business at Dartmouth College
                Hanover, NH 03755, USA[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge Michailidis
                gmichail@ucla.edu
                Department of Statistics and Data Science
                University of California, Los Angeles
                Los Angeles, CA 90095, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (261.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Raghav Singal and George Michailidis.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0285.html.[0m



=== Processing ../JMLR 2024/Bagging Provides Assumption-free Stability.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Bagging Provides Assumption-free Stability.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 4/23; Revised 12/23; Published 4/24[0m

Box rectangle:  [32m(151.5, 101.6) -> (461.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBagging Provides Assumption-free Stability[0m

Box rectangle:  [32m(88.3, 133.9) -> (522.0, 205.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJake A. Soloff
                soloff@uchicago.edu
                Rina Foygel Barber
                rina@uchicago.edu
                Department of Statistics
                University of Chicago
                5747 S Ellis Ave
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(88.3, 213.0) -> (522.0, 279.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRebecca Willett
                willett@uchicago.edu
                Departments of Statistics and Computer Science
                University of Chicago
                5735 S Ellis Ave
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (328.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jake A. Soloff, Rina Foygel Barber and Rebecca Willett.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0536.html.[0m



=== Processing ../JMLR 2024/Bayesian Regression Markets.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Bayesian Regression Markets.pdf') ---[0m

Box rectangle:  [32m(90.0, 40.6) -> (522.0, 50.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 10/23; Revised 5/24; Published 6/24[0m

Box rectangle:  [32m(215.8, 98.9) -> (396.2, 117.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBayesian Regression Markets[0m

Box rectangle:  [32m(90.0, 133.3) -> (522.0, 182.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThomas Falconer
                falco@dtu.dk
                Department of Wind and Energy Systems
                Technical University of Denmark
                Kgs. Lyngby, 2800, Denmark[0m

Box rectangle:  [32m(90.0, 186.9) -> (522.0, 235.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJalal Kazempour
                jalal@dtu.dk
                Department of Wind and Energy Systems
                Technical University of Denmark
                Kgs. Lyngby, 2800, Denmark[0m

Box rectangle:  [32m(90.0, 242.1) -> (522.0, 295.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPierre Pinson
                p.pinson@imperial.ac.uk
                Dyson School of Design Engineering
                Imperial College London
                London, SW7 2DB, United Kingdom[0m

Box rectangle:  [32m(90.0, 318.8) -> (180.5, 331.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Amos Storkey[0m

Box rectangle:  [32m(283.8, 354.2) -> (328.2, 369.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 377.8) -> (502.1, 473.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlthough machine learning tasks are highly sensitive to the quality of input data, relevant datasets
                can often be challenging for firms to acquire, especially when held privately by a variety of owners.
                For instance, if these owners are competitors in a downstream market, they may be reluctant to share
                information. Focusing on supervised learning for regression tasks, we develop a regression market
                to provide a monetary incentive for data sharing. Our mechanism adopts a Bayesian framework,
                allowing us to consider a more general class of regression tasks. We present a thorough exploration
                of the market properties, and show that similar proposals in literature expose the market agents to
                sizeable financial risks, which can be mitigated in our setup.[0m

Box rectangle:  [32m(109.9, 480.3) -> (486.9, 493.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: regression, bayesian inference, collaborative analytics, data markets, game theory[0m

Box rectangle:  [32m(90.0, 515.6) -> (168.0, 531.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 543.7) -> (522.0, 705.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAs machine learning models continue to demand more data, practitioners often concentrate on chal-
                lenges associated with data processing, feature selection and engineering, in addition to model build-
                ing and validation, to optimize performance. These efforts are typically based on the assumption that
                data is readily available via some central authority, yet in practice, datasets are inherently distributed
                amongst owners with heterogeneous characteristics (e.g., privacy preferences). This has motivated
                several developments in the field of collaborative analytics, also known as federated learning (Fig-
                ure 1a), where models are trained on local servers without the need for data centralization, thereby
                preserving privacy and distributing the computational burden (Kairouz et al., 2019). However, this
                method for data sharing is incentive-free, relying on the critical assumption that owners are willing
                to collaborate (i.e., by sharing their private information) altruistically. This strong assumption may
                be violated if owners are competitors in a downstream market environment (Gal-Or, 1985). Conse-
                quently, a fruitful area of research has emerged that proposes to instead commoditize data within a[0m

Box rectangle:  [32m(90.0, 725.1) -> (287.9, 740.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Thomas Falconer, Jalal Kazempour and Pierre Pinson.[0m

Box rectangle:  [32m(90.0, 739.8) -> (489.1, 758.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1385.html.[0m



=== Processing ../JMLR 2024/Bayesian Structural Learning with Parametric Marginals for Count Data  An Application to Microbiota Systems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Bayesian Structural Learning with Parametric Marginals for Count Data  An Application to Microbiota Systems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-26
                Submitted 1/23; Revised 7/24; Published 11/24[0m

Box rectangle:  [32m(92.2, 101.6) -> (519.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBayesian Structural Learning with Parametric Marginals for
                Count Data: An Application to Microbiota Systems[0m

Box rectangle:  [32m(90.0, 152.9) -> (522.0, 201.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVeronica Vinciotti†
                veronica.vinciotti@unitn.it
                Department of Mathematics
                University of Trento
                Italy[0m

Box rectangle:  [32m(90.0, 206.4) -> (522.0, 254.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPariya Behrouzi†
                pariya.behrouzi@wur.nl
                Applied Mathematics and Statistics Group
                Wageningen University and Research
                Netherlands[0m

Box rectangle:  [32m(90.0, 261.9) -> (522.0, 314.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mReza Mohammadi
                a.mohammadi@uva.nl
                Department of Business Analytics
                Amsterdam Business School
                University of Amsterdam, Netherlands[0m

Box rectangle:  [32m(90.0, 726.3) -> (325.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Veronica Vinciotti, Pariya Berhouzi, Reza Mohammadi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0056.html.[0m



=== Processing ../JMLR 2024/BenchMARL  Benchmarking Multi-Agent Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/BenchMARL  Benchmarking Multi-Agent Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-10
                Submitted 12/23; Revised 05/24; Published 7/24[0m

Box rectangle:  [32m(103.7, 101.6) -> (508.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBenchMARL: Benchmarking Multi-Agent Reinforcement
                Learning[0m

Box rectangle:  [32m(90.0, 151.5) -> (518.2, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatteo Bettini∗,1
                mb2389@cl.cam.ac.uk[0m

Box rectangle:  [32m(90.0, 169.4) -> (518.2, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmanda Prorok1
                asp45@cl.cam.ac.uk[0m

Box rectangle:  [32m(90.0, 188.7) -> (522.0, 201.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVincent Moens2
                vmoens@meta.com[0m

Box rectangle:  [32m(90.0, 216.8) -> (507.2, 255.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Work done during an internship at PyTorch, Meta.
                1 Department of Computer Science and Technology, University of Cambridge, United Kingdom.
                2 PyTorch Team, Meta.[0m

Box rectangle:  [32m(90.0, 726.3) -> (315.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Matteo Bettini, Amanda Prorok and Vincent Moens.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1612.html.[0m



=== Processing ../JMLR 2024/Black Box Variational Inference with a Deterministic Objective  Faster  More Accurate  and Even More Black Box.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Black Box Variational Inference with a Deterministic Objective  Faster  More Accurate  and Even More Black Box.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 7/23; Published 1/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBlack Box Variational Inference with a Deterministic
                Objective: Faster, More Accurate, and Even More Black Box[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRyan Giordano∗
                rgiordano@berkeley.edu
                Department of Statistics
                University of California
                Berkeley, CA, USA[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartin Ingram∗
                martin.ingram@gmail.com
                Department of Biosciences
                University of Melbourne
                Parkville, VIC 3010, Australia[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTamara Broderick
                tbroderick@mit.edu
                Department of Electrical Engineering and Computer Science
                Massachusetts Institute of Technology
                Cambridge, MA, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (256.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Giordano and Ingram and Broderick.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1015.html.[0m



=== Processing ../JMLR 2024/Blessings and Curses of Covariate Shifts  Adversarial Learning Dynamics  Directional Convergence  and Equilibria.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Blessings and Curses of Covariate Shifts  Adversarial Learning Dynamics  Directional Convergence  and Equilibria.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 5/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(91.2, 101.6) -> (520.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBlessings and Curses of Covariate Shifts: Adversarial
                Learning Dynamics, Directional Convergence, and Equilibria[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTengyuan Liang
                tengyuan.liang@chicagobooth.edu
                Booth School of Business
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (180.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Tengyuan Liang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0651.html.[0m



=== Processing ../JMLR 2024/Boundary constrained Gaussian processes for robust physics-informed machine learning of linear partial differential equations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Boundary constrained Gaussian processes for robust physics-informed machine learning of linear partial differential equations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-61
                Submitted 11/23; Revised 7/24; Published 9/24[0m

Box rectangle:  [32m(120.6, 101.6) -> (491.5, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoundary constrained Gaussian processes for robust
                physics-informed machine learning of linear partial
                differential equations[0m

Box rectangle:  [32m(90.0, 169.8) -> (518.2, 263.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Dalton
                david.dalton@glasgow.ac.uk
                Alan Lazarus
                alan.lazarus@glasgow.ac.uk
                Hao Gao
                hao.gao@glasgow.ac.uk
                Dirk Husmeier
                dirk.husmeier@glasgow.ac.uk
                School of Mathematics and Statistics
                University of Glasgow
                Glasgow G12 8QQ, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (336.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 David Dalton, Alan Lazarus, Hao Gao and Dirk Husmeier.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1508.html.[0m



=== Processing ../JMLR 2024/Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 11/22; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(102.9, 101.6) -> (509.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBridging Distributional and Risk-sensitive Reinforcement
                Learning with Provable Regret Bounds[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHao Liang
                haoliang1@link.cuhk.edu.cn
                School of Science and Engineering
                The Chinese University of Hong Kong, Shenzhen[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 246.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhi-Quan Luo
                luozq@cuhk.edu.cn
                School of Science and Engineering
                The Chinese University of Hong Kong, Shenzhen[0m

Box rectangle:  [32m(90.0, 726.3) -> (230.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hao Liang and Zhi-Quan Luo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1253.html.[0m



=== Processing ../JMLR 2024/Causal Discovery with Generalized Linear Models through Peeling Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Causal Discovery with Generalized Linear Models through Peeling Algorithms.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 9/23; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(98.7, 82.7) -> (513.4, 115.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCausal Discovery with Generalized Linear Models through
                Peeling Algorithms[0m

Box rectangle:  [32m(71.4, 133.0) -> (540.0, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMinjie Wang
                mwang46@binghamton.edu
                Department of Mathematics and Statistics
                Binghamton University, State University of New York
                Binghamton, NY 13902, USA[0m

Box rectangle:  [32m(70.3, 186.6) -> (540.0, 234.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaotong Shen
                xshen@umn.edu
                School of Statistics
                University of Minnesota
                Minneapolis, MN 55455, USA[0m

Box rectangle:  [32m(70.3, 241.8) -> (540.0, 294.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWei Pan
                panxx014@umn.edu
                Division of Biostatistics
                University of Minnesota
                Minneapolis, MN 55455, USA[0m

Box rectangle:  [32m(72.0, 743.4) -> (262.6, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Minjie Wang, Xiaotong Shen, and Wei Pan.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1228.html.[0m



=== Processing ../JMLR 2024/Causal effects of intervening variables in settings with unmeasured confounding.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Causal effects of intervening variables in settings with unmeasured confounding.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 8/23; Revised 6/24; Published 11/24[0m

Box rectangle:  [32m(115.2, 112.5) -> (496.9, 144.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCausal effects of intervening variables in settings with
                unmeasured confounding[0m

Box rectangle:  [32m(90.0, 173.7) -> (522.0, 221.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLan Wen
                lan.wen@uwaterloo.ca
                Department of Statistics and Actuarial Science
                University of Waterloo
                Waterloo, ON N2L 3G1, Canada[0m

Box rectangle:  [32m(90.0, 238.2) -> (522.0, 286.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAaron L. Sarvet
                asarvet@umass.edu
                Department of Biostatistics and Epidemiology
                University of Massachusetts Amherst
                Amherst, MA 01003, United States[0m

Box rectangle:  [32m(90.0, 304.3) -> (522.0, 357.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMats J. Stensrud
                mats.stensrud@epfl.ch
                Department of Mathematics
                Ecole Polytechnique F ́ed ́erale de Lausanne
                Lausanne, 1015, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (287.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lan Wen, Aaron L. Sarvet, Mats J. Stensrud.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1077.html.[0m



=== Processing ../JMLR 2024/Causal-learn  Causal Discovery in Python.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Causal-learn  Causal Discovery in Python.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-7
                Submitted 7/23; Revised 11/23; Published 2/24[0m

Box rectangle:  [32m(157.7, 101.6) -> (454.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCausal-learn: Causal Discovery in Python[0m

Box rectangle:  [32m(90.0, 135.3) -> (522.0, 350.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYujia Zheng1
                yujiazh@cmu.edu
                Biwei Huang2
                bih007@ucsd.edu
                Wei Chen3
                chenweidelight@gmail.com
                Joseph Ramsey1
                jdramsey@andrew.cmu.edu
                Mingming Gong4
                mingming.gong@unimelb.edu.au
                Ruichu Cai3
                cairuichu@gmail.com
                Shohei Shimizu5,7
                shohei-shimizu@biwako.shiga-u.ac.jp
                Peter Spirtes1
                ps7z@andrew.cmu.edu
                Kun Zhang1,6
                kunz1@cmu.edu
                1 Carnegie Mellon University
                2 University of California, San Diego
                3 Guangdong University of Technology
                4 University of Melbourne
                5 Shiga University
                6 Mohamed bin Zayed University of Artificial Intelligence
                7 RIKEN[0m

Box rectangle:  [32m(90.0, 726.3) -> (517.3, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu, Peter
                Spirtes, Kun Zhang.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0970.html.[0m



=== Processing ../JMLR 2024/Characterization of translation invariant MMD on Rd and connections with Wasserstein distances.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Characterization of translation invariant MMD on Rd and connections with Wasserstein distances.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 11/22; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(101.8, 98.5) -> (510.2, 136.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCharacterization of translation invariant MMD on Rd and
                connections with Wasserstein distances[0m

Box rectangle:  [32m(90.0, 154.0) -> (522.0, 202.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThibault Modeste
                modeste@math.univ-lyon1.fr
                Institut Camille Jordan
                Universit ́e Claude Bernard Lyon 1
                CNRS UMR 5208, F-69622 Villeurbanne, France[0m

Box rectangle:  [32m(90.0, 209.2) -> (522.0, 262.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCl ́ement Dombry
                clement.dombry@univ-fcomte.fr
                Universit ́e de Franche-Comt ́e,
                CNRS, LmB (UMR 6623),
                F-25000 Besan ̧con, France[0m

Box rectangle:  [32m(90.0, 287.2) -> (196.0, 297.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Zaid Harchaoui[0m

Box rectangle:  [32m(280.3, 320.8) -> (331.7, 332.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 339.6) -> (502.1, 457.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKernel mean embeddings and maximum mean discrepancies (MMD) associated with posi-
                tive definite kernels are important tools in machine learning that allow to compare proba-
                bility measures and sample distributions. We provide a full characterization of translation
                invariant MMDs on Rd that are parametrized by a spectral measure and a semi-definite pos-
                itive symmetric matrix. Furthermore, we investigate the connections between translation
                invariant MMDs and Wasserstein distances on Rd. We show in particular that convergence
                with respect to the MMD associated with the Energy Kernel of order α ∈(0, 1) implies
                convergence with respect to the Wasserstein distance of order β < α. We also provide
                examples of kernels metrizing the Wasserstein space of order α ≥1. A short numerical
                experiment illustrates our findings in the framework of the one-sample-test.[0m

Box rectangle:  [32m(109.9, 462.4) -> (502.1, 484.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Reproducing Kernel Hilbert Space, Kernel Mean Embedding, Maximum
                Mean Discrepancy, translation invariance, Wasserstein distance.[0m

Box rectangle:  [32m(90.0, 506.1) -> (176.6, 518.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 529.7) -> (522.1, 707.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBackground. Many problems in statistics and machine learning require comparing several
                probability measures and/or sample distributions: goodness-of-fit testing compares a sam-
                ple distribution to a reference distribution (Chwialkowski et al., 2016); two-sample testing
                compares two sample distributions (Gretton et al., 2012); independence testing compares a
                joint distribution to a product distribution (Gretton et al., 2005); generative model fitting
                compares the distributions of real and fake data (Dziugaite et al., 2015; Sutherland et al.,
                2017). The different methods proposed in these references all rely on the important notion
                of Maximum Mean Discrepancy (MMD).
                MMDs are semi-metrics between probability measures and their definition relies on the the-
                ory of Reproducing Kernel Hilbert Spaces (RKHS) and Kernel Mean Embeddings (KME).
                Given a symmetric positive definite kernel k and its associated RKHS Hk, the KME is a
                map μ 7→K(μ) that assigns a function K(μ) ∈Hk to each signed measure μ in a suit-
                able subspace Mk (defined in Equation (3) below).
                The corresponding MMD between[0m

Box rectangle:  [32m(90.0, 726.4) -> (263.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Thibault Modeste & Cl ́ement Dombry.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1338.html.[0m



=== Processing ../JMLR 2024/Choosing the Number of Topics in LDA Models – A Monte Carlo Comparison of Selection Criteria.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Choosing the Number of Topics in LDA Models – A Monte Carlo Comparison of Selection Criteria.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-30
                Submitted 2/23; Published 3/24[0m

Box rectangle:  [32m(95.3, 101.6) -> (516.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChoosing the Number of Topics in LDA Models – A Monte
                Carlo Comparison of Selection Criteria[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVictor Bystrov
                victor.bystrov@uni.lodz.pl
                Faculty of Economics and Sociology
                University of Lodz
                Rewolucji 1905r. 41, 90-214 Lodz, Poland[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mViktoriia Naboka-Krell
                viktoriia.naboka@wirtschaft.uni-giessen.de
                Department of Statistics and Econometrics
                Justus Liebig University Giessen
                Licher Strasse 64, 35394 Giessen, Germany[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Staszewska-Bystrova
                anna.bystrova@uni.lodz.pl
                Faculty of Economics and Sociology
                University of Lodz
                Rewolucji 1905r. 37/39, 90-214 Lodz, Poland[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Winker
                peter.winker@wirtschaft.uni-giessen.de
                Department of Statistics and Econometrics
                Justus Liebig University Giessen
                Licher Strasse 64, 35394 Giessen, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (466.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Victor Bystrov and Viktoriia Naboka-Krell and Anna Staszewska-Bystrova and Peter Winker.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0188.html.[0m



=== Processing ../JMLR 2024/Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 7/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(114.0, 101.6) -> (498.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mClassification of Data Generated by Gaussian Mixture
                Models Using Deep ReLU Networks[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTian-Yi Zhou
                tzhou306@gatech.edu
                H. Milton Stewart School of Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaoming Huo
                huo@isye.gatech.edu
                H. Milton Stewart School of Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (244.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Tian-Yi Zhou and Xiaoming Huo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0957.html.[0m



=== Processing ../JMLR 2024/Classification with Deep Neural Networks and Logistic Loss.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Classification with Deep Neural Networks and Logistic Loss.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-117
                Submitted 1/22; Revised 7/23; Published 4/24[0m

Box rectangle:  [32m(94.3, 101.6) -> (517.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mClassification with Deep Neural Networks and Logistic Loss[0m

Box rectangle:  [32m(90.0, 133.9) -> (521.9, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZihan Zhang
                zihanzhang19@fudan.edu.cn
                Shanghai Center for Mathematical Sciences
                Fudan University, Shanghai 200433, China
                School of Data Science
                City University of Hong Kong, Kowloon, Hong Kong[0m

Box rectangle:  [32m(90.0, 199.1) -> (522.0, 271.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLei Shi∗
                leishi@fudan.edu.cn
                School of Mathematical Sciences and Shanghai
                Key Laboratory for Contemporary Applied Mathematics
                Fudan University, Shanghai 200433, China
                Shanghai Artificial Intelligence Laboratory
                701 Yunjin Road, Shanghai 200232, China[0m

Box rectangle:  [32m(90.0, 278.5) -> (521.9, 317.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDing-Xuan Zhou
                dingxuan.zhou@sydney.edu.au
                School of Mathematics and Statistics
                University of Sydney, Sydney NSW 2006, Australia[0m

Box rectangle:  [32m(90.0, 726.3) -> (280.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zihan Zhang, Lei Shi and Ding-Xuan Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0049.html.[0m



=== Processing ../JMLR 2024/Cluster-Adaptive Network A B Testing  From Randomization to Estimation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Cluster-Adaptive Network A B Testing  From Randomization to Estimation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 2/22; Revised 12/23; Published 4/24[0m

Box rectangle:  [32m(163.4, 102.4) -> (448.7, 134.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCluster-Adaptive Network A/B Testing:
                From Randomization to Estimation[0m

Box rectangle:  [32m(90.0, 152.7) -> (522.0, 200.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYang Liu
                yangliu2022@ruc.edu.cn
                Institute of Statistics and Big Data
                Renmin University of China
                Beijing, 100872, China[0m

Box rectangle:  [32m(90.0, 206.2) -> (522.0, 254.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYifan Zhou
                yzhou92@gwu.edu
                Department of Statistics
                George Washington University
                Washington, DC 22202, USA[0m

Box rectangle:  [32m(90.0, 259.8) -> (522.0, 295.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPing Li
                pingli98@gmail.com
                VecML Inc.
                Bellevue, WA 98004, USA[0m

Box rectangle:  [32m(90.0, 303.0) -> (522.0, 355.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFeifang Hu
                feifang@gwu.edu
                Department of Statistics
                George Washington University
                Washington, DC 22202, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (294.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yang Liu, Yifan Zhou, Ping Li and Feifang Hu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0192.html.[0m



=== Processing ../JMLR 2024/Commutative Scaling of Width and Depth in Deep Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Commutative Scaling of Width and Depth in Deep Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 9/23; Revised 4/24; Published 8/24[0m

Box rectangle:  [32m(155.8, 101.6) -> (456.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCommutative Scaling of Width and Depth
                in Deep Neural Networks[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 192.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSoufiane Hayou
                hayou@berkeley.edu
                Simons Institute
                UC Berkeley[0m

Box rectangle:  [32m(90.0, 726.3) -> (177.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Soufiane Hayou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1163.html.[0m



=== Processing ../JMLR 2024/Compressed and distributed least-squares regression  convergence rates with applications to federated learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Compressed and distributed least-squares regression  convergence rates with applications to federated learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-80
                Submitted 08/23; Published 09/24[0m

Box rectangle:  [32m(104.2, 101.6) -> (507.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCompressed and distributed least-squares regression:
                convergence rates with applications to federated learning[0m

Box rectangle:  [32m(90.0, 153.3) -> (352.2, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConstantin Philippenko
                constantin.philippenko@polytechnique.edu
                 ́Ecole polytechnique, Institut Polytechnique de Paris, CMAP[0m

Box rectangle:  [32m(90.0, 196.5) -> (352.2, 234.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAymeric Dieuleveut
                aymeric.dieuleveut@polytechnique.edu
                 ́Ecole polytechnique, Institut Polytechnique de Paris, CMAP[0m

Box rectangle:  [32m(90.0, 726.5) -> (300.7, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Constantin Philippenko and Aymeric Dieuleveut.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1040.html.[0m



=== Processing ../JMLR 2024/Concentration and Moment Inequalities for General Functions of Independent Random Variables with Heavy Tails.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Concentration and Moment Inequalities for General Functions of Independent Random Variables with Heavy Tails.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 6/23; Revised 5/24; Published 9/24[0m

Box rectangle:  [32m(105.6, 101.6) -> (507.1, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConcentration and Moment Inequalities for General
                Functions of Independent Random Variables with Heavy
                Tails[0m

Box rectangle:  [32m(88.8, 169.8) -> (522.0, 205.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShaojie Li
                lishaojie95@ruc.edu.cn
                Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
                Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China[0m

Box rectangle:  [32m(88.8, 212.7) -> (522.0, 252.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYong Liu∗
                liuyonggsai@ruc.edu.cn
                Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
                Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (211.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shaojie Li and Yong Liu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0744.html.[0m



=== Processing ../JMLR 2024/Conformal Inference for Online Prediction with Arbitrary Distribution Shifts.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Conformal Inference for Online Prediction with Arbitrary Distribution Shifts.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 10/22; Revised 5/24; Published 5/24[0m

Box rectangle:  [32m(101.1, 101.6) -> (511.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConformal Inference for Online Prediction with Arbitrary
                Distribution Shifts[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIsaac Gibbs
                igibbs@stanford.edu
                Department of Statistics
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmmanuel Cand`es
                candes@stanford.edu
                Departments of Statistics and Mathematics
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (252.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Isaac Gibbs and Emmanuel Cand`es.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1218.html.[0m



=== Processing ../JMLR 2024/Consistent Multiclass Algorithms for Complex Metrics and Constraints.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Consistent Multiclass Algorithms for Complex Metrics and Constraints.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-81
                Submitted 10/22; Published 11/24[0m

Box rectangle:  [32m(193.2, 101.7) -> (418.8, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConsistent Multiclass Algorithms for
                Complex Metrics and Constraints[0m

Box rectangle:  [32m(89.7, 153.4) -> (521.7, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHarikrishna Narasimhan
                HNARASIMHAN@GOOGLE.COM
                Google Research, Mountain View, USA[0m

Box rectangle:  [32m(90.0, 183.0) -> (521.8, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHarish G. Ramaswamy
                HARIGURU@DSAI.IITM.AC.IN
                Indian Institute of Technology Madras, Chennai, India[0m

Box rectangle:  [32m(90.0, 210.8) -> (521.7, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShiv Kumar Tavker∗
                TAVKER@AMAZON.COM
                Amazon Inc., Bengaluru, India[0m

Box rectangle:  [32m(90.0, 240.5) -> (521.7, 264.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDrona Khurana∗
                DRONAKHURANA1294@GMAIL.COM
                University of Colorado Boulder, USA[0m

Box rectangle:  [32m(90.0, 272.1) -> (521.7, 294.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPraneeth Netrapalli
                PNETRAPALLI@GOOGLE.COM
                Google Research India, Bengaluru, India[0m

Box rectangle:  [32m(90.0, 303.3) -> (521.8, 327.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShivani Agarwal
                ASHIVANI@SEAS.UPENN.EDU
                University of Pennsylvania, Philadelphia, USA[0m

Box rectangle:  [32m(90.0, 726.4) -> (496.4, 744.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Harikrishna Narasimhan, Harish G. Ramaswamy, Shiv Kumar Tavker, Drona Khurana, Praneeth Netrapalli and Shivani
                Agarwal.[0m

Box rectangle:  [32m(90.0, 750.2) -> (515.4, 768.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1137.html.[0m



=== Processing ../JMLR 2024/Contamination-source based K-sample clustering.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Contamination-source based K-sample clustering.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 7/23; Revised 6/24; Published 9/24[0m

Box rectangle:  [32m(133.0, 101.6) -> (479.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mContamination-source based K-sample clustering[0m

Box rectangle:  [32m(90.0, 133.9) -> (521.9, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXavier Milhaud
                xavier.milhaud@univ-amu.fr
                Aix Marseille Univ, CNRS, Centrale Marseille, I2M
                13288 Marseille cedex 9, France[0m

Box rectangle:  [32m(90.0, 175.6) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDenys Pommeret
                denys.pommeret@univ-amu.fr
                Aix Marseille Univ, CNRS, Centrale Marseille, I2M
                13288 Marseille cedex 9, France[0m

Box rectangle:  [32m(90.0, 217.2) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYahia Salhi
                yahia.salhi@univ-lyon1.fr
                Universit ́e Claude Bernard Lyon 1, UCBL, ISFA LSAF EA2429
                F-69007 Lyon, France[0m

Box rectangle:  [32m(90.0, 260.4) -> (522.0, 299.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPierre Vandekerkhove
                pierre.vandekerkhove@univ-eiffel.fr
                Universit ́e Gustave Eiffel, LAMA (UMR 8050)
                77420 Champs-sur-Marne, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (380.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xavier Milhaud, Denys Pommeret, Yahia Salhi, Pierre Vandekerkhove.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0914.html.[0m



=== Processing ../JMLR 2024/Contextual Bandits with Packing and Covering Constraints  A Modular Lagrangian Approach via Regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Contextual Bandits with Packing and Covering Constraints  A Modular Lagrangian Approach via Regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 8/24; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(93.2, 101.6) -> (519.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mContextual Bandits with Packing and Covering Constraints:
                A Modular Lagrangian Approach via Regression[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAleksandrs Slivkins
                slivkins@microsoft.com
                Microsoft Research NYC[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXingyu Zhou
                xingyu.zhou@wayne.edu
                Wayne State University, Detroit[0m

Box rectangle:  [32m(90.0, 211.2) -> (522.0, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKarthik Abinav Sankararaman
                karthikabinavs@gmail.com
                Meta[0m

Box rectangle:  [32m(90.0, 242.5) -> (522.0, 268.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDylan J. Foster
                dylanfoster@microsoft.com
                Microsoft Research NYC[0m

Box rectangle:  [32m(90.0, 293.3) -> (198.1, 303.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Kevin Jamieson[0m

Box rectangle:  [32m(280.3, 326.9) -> (331.7, 338.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 346.5) -> (502.2, 525.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe consider contextual bandits with linear constraints (CBwLC), a variant of contextual
                bandits in which the algorithm consumes multiple resources subject to linear constraints
                on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK),
                allowing for packing and covering constraints, as well as positive and negative resource con-
                sumption. We provide the first algorithm for CBwLC (or CBwK) that is based on regression
                oracles. The algorithm is simple, computationally efficient, and statistically optimal under
                mild assumptions. Further, we provide the first vanishing-regret guarantees for CBwLC (or
                CBwK) that extend beyond the stochastic environment. We side-step strong impossibility re-
                sults from prior work by identifying a weaker (and, arguably, fairer) benchmark to compare
                against. Our algorithm builds on LagrangeBwK (Immorlica et al., 2019, 2022), a Lagrangian-
                based technique for CBwK, and SquareCB (Foster and Rakhlin, 2020), a regression-based
                technique for contextual bandits. Our analysis leverages the inherent modularity of both
                techniques.
                Keywords: multi-armed bandits, contextual bandits, bandits with knapsacks, regression
                oracles, primal-dual algorithms[0m

Box rectangle:  [32m(90.0, 546.5) -> (180.3, 558.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 572.3) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOur scope. We consider a problem called contextual bandits with linear constraints (CBwLC).
                In this problem, an algorithm chooses from a fixed set of K arms and consumes d ≥1 con-
                strained resources. In each round t, the algorithm observes a context xt, chooses an arm
                at, receives a reward rt ∈[0, 1], and also consumes some bounded amount of each resource.
                (So, the outcome of choosing an arm is a (d + 1)-dimensional vector.) The consumption
                of a given resource could also be negative, corresponding to replenishment thereof. The
                algorithm proceeds for T rounds, and faces a constraint on the total consumption of each
                resource i: either a packing constraint (“at most Bi”) or a covering constraint (“at least
                Bi”) for some parameter Bi ≤T. We focus on the stochastic environment, wherein the
                context and the arms’ outcome vectors are drawn from a fixed joint distribution, indepen-[0m

Box rectangle:  [32m(90.0, 726.4) -> (428.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Aleksandrs Slivkins, Xingyu Zhou, Karthik Abinav Sankararaman, Dylan J. Foster.[0m

Box rectangle:  [32m(90.0, 740.9) -> (516.4, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-1220.html.[0m



=== Processing ../JMLR 2024/Continuous Prediction with Experts' Advice.pdf ===

[31m--- Pieces of PosixPath("../JMLR 2024/Continuous Prediction with Experts' Advice.pdf") ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 7/22; Revised 11/23; Published 8/24[0m

Box rectangle:  [32m(152.6, 101.5) -> (459.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mContinuous Prediction with Experts’ Advice[0m

Box rectangle:  [32m(88.1, 135.5) -> (522.0, 256.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicholas J. A. Harvey
                nickhar@cs.ubc.ca
                University of British Columbia
                Vancouver, BC, Canada
                Christopher Liaw
                cvliaw@google.com
                Google
                Mountain View, CA, USA
                Victor S. Portella
                victorsp@cs.ubc.ca
                University of British Columbia
                Vancouver, BC, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (340.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nicholas J. A. Harvey, Christopher Liaw, Victor S. Portella.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0803.html.[0m



=== Processing ../JMLR 2024/Convergence for nonconvex ADMM  with applications to CT imaging.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Convergence for nonconvex ADMM  with applications to CT imaging.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 7/21; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConvergence for nonconvex ADMM, with applications to CT
                imaging[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRina Foygel Barber
                rina@uchicago.edu
                Department of Statistics
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmil Y. Sidky
                sidky@uchicago.edu
                Department of Radiology
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (264.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rina Foygel Barber and Emil Y. Sidky.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0831.html.[0m



=== Processing ../JMLR 2024/Convergence of Message-Passing Graph Neural Networks with Generic Aggregation on Large Random Graphs.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Convergence of Message-Passing Graph Neural Networks with Generic Aggregation on Large Random Graphs.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 7/23; Revised 8/24; Published 11/24[0m

Box rectangle:  [32m(90.0, 101.5) -> (522.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConvergence of Message-Passing Graph Neural Networks with
                Generic Aggregation on Large Random Graphs[0m

Box rectangle:  [32m(88.8, 151.8) -> (522.0, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthieu Cordonnier
                matthieu.cordonnier@gipsa-lab.fr
                GIPSA-lab, Université Grenoble Alpes, Grenoble[0m

Box rectangle:  [32m(88.8, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicolas Keriven
                nicolas.keriven@cnrs.fr
                CNRS, IRISA, Rennes[0m

Box rectangle:  [32m(88.8, 211.1) -> (522.0, 235.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicolas Tremblay
                nicolas.tremblay@cnrs.fr
                CNRS, GIPSA-lab, Grenoble[0m

Box rectangle:  [32m(88.8, 242.4) -> (522.0, 268.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSamuel Vaiter
                samuel.vaiter@cnrs.fr
                CNRS, LJAD, Nice[0m

Box rectangle:  [32m(90.0, 726.3) -> (402.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Matthieu Cordonnier, Nicolas Keriven, Nicolas Tremblay and Samuel Vaiter.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0965.html.[0m



=== Processing ../JMLR 2024/Correction to  Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Correction to  Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-9
                Submitted 6/24; Revised 12/24; Published 12/24[0m

Box rectangle:  [32m(110.8, 101.6) -> (501.2, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCorrection to “Wasserstein distance estimates for the
                distributions of numerical approximations to ergodic
                stochastic differential equations”[0m

Box rectangle:  [32m(90.0, 169.5) -> (522.0, 205.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel Paulin∗
                dpaulin@ed.ac.uk
                School of Mathematics
                University of Edinburgh, United Kingdom[0m

Box rectangle:  [32m(90.0, 212.7) -> (522.0, 252.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter A. Whalley∗
                peter.whalley@math.ethz.ch
                Seminar for Statistics
                ETH Z ̈urich, Z ̈urich, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (254.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Daniel Paulin and Peter A. Whalley.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0895.html.[0m



=== Processing ../JMLR 2024/Countering the Communication Bottleneck in Federated Learning  A Highly Efficient Zero-Order Optimization Technique.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Countering the Communication Bottleneck in Federated Learning  A Highly Efficient Zero-Order Optimization Technique.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 7/24; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(106.8, 101.6) -> (505.3, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCountering the Communication Bottleneck in Federated
                Learning: A Highly Efficient Zero-Order Optimization
                Technique[0m

Box rectangle:  [32m(90.0, 171.4) -> (522.0, 251.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mElissa Mhanna
                elissa.mhanna@centralesupelec.fr
                Mohamad Assaad
                mohamad.assaad@centralesupelec.fr
                Laboratoire des Signaux & Syst`emes (L2S)
                Universit ́e Paris-Saclay, CNRS, CentraleSup ́elec
                3 rue Joliot Curie
                91190 Gif-sur-Yvette, France[0m

Box rectangle:  [32m(90.0, 276.5) -> (195.7, 286.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Peter Richt ́arik[0m

Box rectangle:  [32m(280.3, 310.1) -> (331.7, 322.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 328.5) -> (502.1, 568.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFederated learning (FL) is a creative technique that enables multiple edge devices to train
                a model without revealing raw data. However, several issues hinder the practical imple-
                mentation of FL, especially in wireless environments. These issues comprise the limited
                capacity of the upload transmission link between the edge devices and the aggregator, as
                well as the wireless disturbances. To address these challenges, we develop a zero-order (ZO)
                communication-efficient framework for FL. While in standard FL, each device must upload
                a long vector containing the gradient or the model per communication round, our novel
                ZO method incorporates a two-point gradient estimator, which requires uploading only two
                scalars. What also sets our approach apart is that it directly incorporates wireless pertur-
                bations into the learning, eliminating the need for additional computational resources to
                remove their impact. In this work, we overcome the technical and analytical challenges
                associated with FL problems and ZO methods, comprehensively study our algorithm, and
                prove it converges almost surely under different conditions, convexity and non-convexity,
                noise-free and noisy environments. We then find theoretical bounds on the convergence
                rate when the objective is strongly convex, non-convex, and κ-gradient-dominated that
                compete with first-order (FO) or centralized methods under the same settings. Finally, we
                provide experimental results demonstrating the effectiveness of our algorithm, considering
                relevant examples. We provide an example illustrating the amount of communication saved
                due to its efficiency compared to its FO counterpart.
                Keywords:
                Federated learning, zero-order, gradient estimate.[0m

Box rectangle:  [32m(90.0, 589.8) -> (176.6, 601.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 613.0) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFederated learning (FL) has emerged as an innovative solution for distributed machine
                learning, as indicated by McMahan et al. (2017).
                This paradigm has been adopted by
                major technology companies to implement at scale (Bonawitz et al., 2019) as it addresses
                the challenge of training models without requiring users to transmit their private data to
                a central server. Instead, data remains on the users’ devices, and model training occurs
                through collaborative interactions between these devices and the server: The devices receive
                the model from the server, utilize their data to update gradients, and then transmit these[0m

Box rectangle:  [32m(90.0, 726.5) -> (261.2, 734.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Elissa Mhanna and Mohamad Assaad.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-1189.html.[0m



=== Processing ../JMLR 2024/Critically Assessing the State of the Art in Neural Network Verification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Critically Assessing the State of the Art in Neural Network Verification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 1/23; Published 1/24[0m

Box rectangle:  [32m(163.8, 101.6) -> (448.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCritically Assessing the State of the Art
                in Neural Network Verification[0m

Box rectangle:  [32m(89.5, 153.2) -> (522.0, 206.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthias K ̈onig1
                h.m.t.konig@liacs.leidenuniv.nl
                Annelot W. Bosman1
                a.w.bosman@liacs.leidenuniv.nl
                Holger H. Hoos1,2,3
                hh@cs.rwth-aachen.de
                Jan N. van Rijn1
                j.n.van.rijn@liacs.leidenuniv.nl[0m

Box rectangle:  [32m(89.6, 222.1) -> (462.7, 260.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1Leiden Institute of Advanced Computer Science, Leiden University, The Netherlands
                2Chair for AI Methodology, RWTH Aachen University, Germany
                3University of British Columbia, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (386.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Matthias K ̈onig, Annelot W. Bosman, Holger H. Hoos, Jan N. van Rijn.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0119.html.[0m



=== Processing ../JMLR 2024/DAG-Informed Structure Learning from Multi-Dimensional Point Processes.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/DAG-Informed Structure Learning from Multi-Dimensional Point Processes.pdf') ---[0m

Box rectangle:  [32m(97.7, 101.6) -> (514.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDAG-Informed Structure Learning from Multi-Dimensional
                Point Processes[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChunming Zhang
                cmzhang@stat.wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMuhong Gao
                gaomh@amss.ac.cn
                Academy of Mathematics and System Science
                Chinese Academy of Sciences
                Beijing 100190, China[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShengji Jia
                20200026@lixin.edu.cn
                School of Statistics and Mathematics
                Shanghai Lixin University of Accounting and Finance
                Shanghai, China[0m

Box rectangle:  [32m(280.3, 339.0) -> (331.7, 350.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 357.0) -> (502.1, 596.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMotivated by inferring causal relationships among neurons using ensemble spike train data,
                this paper introduces a new technique for learning the structure of a directed acyclic graph
                (DAG) within a large network of events, applicable to diverse multi-dimensional tempo-
                ral point process (MuTPP) data.
                At the core of MuTPP lie the conditional intensity
                functions, for which we construct a generative model parameterized by the graph param-
                eters of a DAG and develop an equality-constrained estimator, departing from exhaustive
                search-based methods. We present a novel, flexible augmented Lagrangian (Flex-AL) op-
                timization scheme that ensures provable global convergence and computational efficiency
                gains over the classical AL algorithm. Additionally, we explore causal structure learning
                by integrating acyclicity-constraints and sparsity-regularization. We demonstrate: (i) in
                cases without regularization, the incorporation of the acyclicity constraint is essential for
                ensuring DAG recovery consistency; (ii) with suitable regularization, the DAG-constrained
                estimator achieves both parameter estimation and DAG reconstruction consistencies sim-
                ilar to the unconstrained counterpart, but significantly enhances empirical performance.
                Furthermore, simulation studies indicate that our proposed DAG-constrained estimator,
                when appropriately penalized, yields more accurate graphs compared to unconstrained or
                unregularized estimators.
                Finally, we apply the proposed method to two real MuTPP
                datasets.
                Keywords:
                asymptotic consistency; causal structure; constrained optimization; multi-
                variate counting process; Structural Hamming Distance.[0m

Box rectangle:  [32m(90.0, 617.4) -> (180.3, 629.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 640.1) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe multi-dimensional temporal point process (MuTPP) model provides a probabilistic
                graphical framework for event occurrences observed in continuous time. Its applications
                span various domains, including recordings of multiple neuronal spike trains, users’ on-
                line ratings in social networks, and patients’ treatment time records in hospitals, among
                others. An essential objective involves uncovering the network’s structured causal relation-[0m

Box rectangle:  [32m(90.0, 726.3) -> (300.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Chunming Zhang, Muhong Gao, and Shengji Jia.[0m

Box rectangle:  [32m(90.0, 741.0) -> (371.6, 749.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.[0m



=== Processing ../JMLR 2024/Data-driven Automated Negative Control Estimation (DANCE)  Search for  Validation of  and Causal Inference with Negative Controls.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Data-driven Automated Negative Control Estimation (DANCE)  Search for  Validation of  and Causal Inference with Negative Controls.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 9/22; Revised 1/24; Published 4/24[0m

Box rectangle:  [32m(100.2, 101.6) -> (512.0, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData-driven Automated Negative Control Estimation
                (DANCE): Search for, Validation of, and Causal Inference
                with Negative Controls[0m

Box rectangle:  [32m(90.0, 171.2) -> (227.1, 229.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErich Kummerfeld
                erichk@umn.edu
                Institute for Health Informatics
                University of Minnesota
                Minneapolis, MN 55455, USA[0m

Box rectangle:  [32m(90.0, 236.8) -> (209.3, 295.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJaewon Lim
                imjaewon@uw.edu
                Department of Biostatistics
                University of Washington
                Seattle, WA 98105, USA[0m

Box rectangle:  [32m(90.0, 303.9) -> (212.1, 368.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXu Shi
                shixu@umich.edu
                Department of Biostatistics
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (286.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Erich Kummerfeld, Jaewon Lim, and Xu Shi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1062.html.[0m



=== Processing ../JMLR 2024/Data-Efficient Policy Evaluation Through Behavior Policy Search.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Data-Efficient Policy Evaluation Through Behavior Policy Search.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-58
                Submitted 4/21; Revised 7/23; Published 10/24[0m

Box rectangle:  [32m(101.3, 101.6) -> (511.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData-Efficient Policy Evaluation Through Behavior Policy
                Search[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJosiah P. Hanna
                jphanna@cs.wisc.edu
                Computer Sciences Department
                University of Wisconsin – Madison
                Madison, WI, USA[0m

Box rectangle:  [32m(88.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYash Chandak, Philip S. Thomas
                {ychandak,pthomas}@cs.umass.edu
                College of Information and Computer Sciences
                University of Massachusetts
                Amherst, MA, USA[0m

Box rectangle:  [32m(88.3, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartha White
                whitem@ualberta.ca
                Department of Computing Science
                University of Alberta and the Alberta Machine Intelligence Institute (Amii)
                Edmonton, Alberta, CA[0m

Box rectangle:  [32m(88.4, 312.6) -> (522.0, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Stone
                pstone@cs.utexas.edu
                Department of Computer Science
                The University of Texas at Austin and Sony AI
                Austin, TX, USA[0m

Box rectangle:  [32m(88.3, 367.8) -> (522.0, 420.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScott Niekum
                sniekum@cs.umass.edu
                College of Information and Computer Sciences
                University of Massachusetts
                Amherst, MA, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (488.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Josiah P. Hanna, Yash Chandak, Philip S. Thomas, Martha White, Peter Stone, and Scott Niekum.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0346.html.[0m



=== Processing ../JMLR 2024/Data Summarization via Bilevel Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Data Summarization via Bilevel Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 9/21; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(147.2, 101.5) -> (464.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData Summarization via Bilevel Optimization[0m

Box rectangle:  [32m(90.0, 133.5) -> (522.0, 169.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZalán Borsos∗
                zalan.borsos@gmail.com
                Department of Computer Science
                ETH Zurich[0m

Box rectangle:  [32m(90.0, 175.5) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMojmír Mutný
                mojmir.mutny@inf.ethz.ch
                Department of Computer Science
                ETH Zurich[0m

Box rectangle:  [32m(90.0, 216.8) -> (522.0, 241.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarco Tagliasacchi∗
                mtagliasacchi@google.com
                Google Research[0m

Box rectangle:  [32m(90.0, 248.4) -> (522.0, 287.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndreas Krause
                krausea@ethz.ch
                Department of Computer Science
                ETH Zurich[0m

Box rectangle:  [32m(90.0, 726.3) -> (382.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zalán Borsos, Mojmír Mutný, Marco Tagliasacchi and Andreas Krause.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1132.html.[0m



=== Processing ../JMLR 2024/Data Thinning for Convolution-Closed Distributions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Data Thinning for Convolution-Closed Distributions.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 4/23; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(120.9, 82.7) -> (491.2, 97.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData Thinning for Convolution-Closed Distributions[0m

Box rectangle:  [32m(72.0, 115.1) -> (540.0, 163.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Neufeld
                aneufeld@fredhutch.org
                Public Health Sciences Division
                Fred Hutchinson Cancer Center
                Seattle, WA 98109, USA[0m

Box rectangle:  [32m(72.0, 168.6) -> (540.0, 216.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmeer Dharamshi
                adharams@uw.edu
                Department of Biostatistics
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(72.0, 222.2) -> (540.0, 270.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLucy L. Gao
                lucy.gao@ubc.ca
                Department of Statistics
                University of British Columbia
                Vancouver, British Columbia V6T 1Z4, Canada[0m

Box rectangle:  [32m(72.0, 277.4) -> (540.0, 330.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniela Witten
                dwitten@uw.edu
                Departments of Statistics and Biostatistics
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(72.0, 743.4) -> (340.2, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Anna Neufeld, Ameer Dharamshi, Lucy L. Gao, Daniela Witten.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0446.html.[0m



=== Processing ../JMLR 2024/Debiasing Evaluations That Are Biased by Evaluations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Debiasing Evaluations That Are Biased by Evaluations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-120
                Submitted 7/22; Revised 8/24; Published 11/24[0m

Box rectangle:  [32m(115.5, 101.5) -> (496.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDebiasing Evaluations That Are Biased by Evaluations[0m

Box rectangle:  [32m(90.0, 133.5) -> (518.2, 146.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJingyan Wang⋄
                jingyanw@ttic.edu[0m

Box rectangle:  [32m(90.0, 151.3) -> (518.2, 163.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIvan Stelmakh§
                istelmakh@nes.ru[0m

Box rectangle:  [32m(90.0, 169.0) -> (518.2, 181.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuting Wei∗
                ytwei@wharton.upenn.edu[0m

Box rectangle:  [32m(90.0, 188.3) -> (522.0, 336.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNihar Shah†
                nihars@cs.cmu.edu
                ⋄Toyota Technological Institute at Chicago
                Chicago, IL 60637, USA
                § New Economic School
                Moscow, Russia
                ∗Department of Statistics and Data Science
                University of Pennsylvania
                Philadelphia, PA 19104, USA
                † School of Computer Science
                Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (326.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jingyan Wang, Ivan Stelmakh, Yuting Wei, Nihar Shah.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0775.html.[0m



=== Processing ../JMLR 2024/Decentralized Natural Policy Gradient with Variance Reduction for Collaborative Multi-Agent Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Decentralized Natural Policy Gradient with Variance Reduction for Collaborative Multi-Agent Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 9/22; Published 5/24[0m

Box rectangle:  [32m(107.0, 101.6) -> (505.1, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDecentralized Natural Policy Gradient with Variance
                Reduction for Collaborative Multi-Agent Reinforcement
                Learning[0m

Box rectangle:  [32m(90.0, 169.5) -> (522.0, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJinchi Chen∗
                jcchen@ecust.edu.cn
                School of Data Science
                Fudan University
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 231.7) -> (304.7, 265.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSchool of Mathematics
                East China University of Science and Technology
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 270.9) -> (522.0, 319.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJie Feng∗
                19110980001@fudan.edu.cn
                School of Data Science
                Fudan University
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 324.8) -> (522.0, 372.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWeiguo Gao
                wggao@fudan.edu.cn
                School of Mathematical Sciences and School of Data Science
                Fudan University
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 380.0) -> (522.0, 432.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKe Wei
                kewei@fudan.edu.cn
                School of Data Science
                Fudan University
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (299.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jinchi Chen, Jie Feng, Weiguo Gao, and Ke Wei.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1036.html.[0m



=== Processing ../JMLR 2024/Decomposed Linear Dynamical Systems (dLDS) for  learning the latent components of neural dynamics.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Decomposed Linear Dynamical Systems (dLDS) for  learning the latent components of neural dynamics.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-12
                Submitted 6/23; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(123.6, 102.4) -> (488.8, 134.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDecomposed Linear Dynamical Systems (dLDS) for
                learning the latent components of neural dynamics[0m

Box rectangle:  [32m(88.8, 152.3) -> (522.0, 224.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNoga Mudrik∗
                nmudrik1@jhu.edu
                Department of Biomedical Engineering
                Center for Imaging Science
                Kavli NDI
                Johns Hopkins University
                Baltimore, MD 21218.[0m

Box rectangle:  [32m(88.8, 241.8) -> (522.0, 290.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYenho Chen∗
                yenho@gatech.edu
                Department of Biomedical Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332.[0m

Box rectangle:  [32m(88.8, 307.6) -> (522.0, 367.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEva Yezerets
                eyezere1@jhu.edu
                Department of Biomedical Engineering
                Center for Imaging Science
                Johns Hopkins University
                Baltimore, MD 21218.[0m

Box rectangle:  [32m(88.8, 385.1) -> (522.0, 433.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristopher J. Rozell
                crozell@gatech.edu
                School of Electrical and Computer Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332.[0m

Box rectangle:  [32m(88.8, 452.3) -> (522.0, 545.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdam S. Charles
                adamsc@jhu.edu
                Department of Biomedical Engineering
                Center for Imaging Science
                Mathematical Institute for Data Science
                Kavli NDI
                Johns Hopkins University
                Baltimore, MD 21218.[0m

Box rectangle:  [32m(90.0, 726.3) -> (440.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Noga Mudrik, Yenho Chen, Eva Yezerets, Christopher J. Rozell, and Adam S. Charles.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0777.html.[0m



=== Processing ../JMLR 2024/Decomposing Global Feature Effects Based on Feature Interactions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Decomposing Global Feature Effects Based on Feature Interactions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 5/23; Revised 10/24; Published 12/24[0m

Box rectangle:  [32m(113.4, 101.6) -> (498.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDecomposing Global Feature Effects Based on Feature
                Interactions[0m

Box rectangle:  [32m(90.0, 153.2) -> (522.0, 287.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJulia Herbinger1,2
                julia.herbinger@gmail.com
                Marvin N. Wright3,4,5
                wright@leibniz-bips.de
                Thomas Nagler1,2
                nagler@stat.uni-muenchen.de
                Bernd Bischl1,2
                bernd.bischl@stat.uni-muenchen.de
                Giuseppe Casalicchio1,2
                giuseppe.casalicchio@stat.uni-muenchen.de
                1 Department of Statistics, LMU Munich, Munich, Germany
                2 Munich Center for Machine Learning (MCML), Munich, Germany
                3 Leibniz Institute for Prevention Research and Epidemiology, Bremen, Germany
                4 Faculty of Mathematics and Computer Science, University of Bremen, Bremen, Germany
                5 Department of Public Health, University of Copenhagen, Copenhagen, Denmark[0m

Box rectangle:  [32m(90.0, 726.3) -> (461.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Julia Herbinger, Marvin N. Wright, Thomas Nagler, Bernd Bischl and Giuseppe Casalicchio.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0699.html.[0m



=== Processing ../JMLR 2024/Decorrelated Variable Importance.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Decorrelated Variable Importance.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 7/22; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(185.6, 101.6) -> (426.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDecorrelated Variable Importance[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIsabella Verdinelli
                isabella@stat.cmu.edu
                Department of Statistics
                Carnegie Mellon University
                5000 Forbes Ave.
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(90.0, 201.1) -> (522.0, 267.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLarry Wasserman
                larry@stat.cmu.edu
                Department of Statistics
                Carnegie Mellon University
                5000 Forbes Ave.
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (271.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Isabella Verdinelli and Larry Wasserman.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0801.html.[0m



=== Processing ../JMLR 2024/Deep Backward and Galerkin Methods for the Finite State Master Equation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Deep Backward and Galerkin Methods for the Finite State Master Equation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 2/24; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(97.3, 101.6) -> (514.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep Backward and Galerkin Methods for the Finite State
                Master Equation[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAsaf Cohen
                asafc@umich.edu
                Department of Mathematics
                University of Michigan
                Ann Arbor, MI 48104, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMathieu Lauri`ere
                mathieu.lauriere@nyu.edu
                Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
                NYU-ECNU Institute of Mathematical Sciences
                New York University Shanghai
                Shanghai, 200000, China[0m

Box rectangle:  [32m(90.0, 272.6) -> (522.0, 325.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEthan Zell
                ezell@umich.edu
                Department of Mathematics
                University of Michigan
                Ann Arbor, MI 48104, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (277.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Asaf Cohen, Mathieu Lauri`ere, Ethan Zell.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0215.html.[0m



=== Processing ../JMLR 2024/Deep Network Approximation  Beyond ReLU to Diverse Activation Functions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Deep Network Approximation  Beyond ReLU to Diverse Activation Functions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 7/23; Revised 10/23; Published 1/24[0m

Box rectangle:  [32m(104.7, 101.6) -> (507.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep Network Approximation: Beyond ReLU to Diverse
                Activation Functions[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShijun Zhang∗
                shijun.zhang@duke.edu
                Department of Mathematics
                Duke University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJianfeng Lu
                jianfeng@math.duke.edu
                Department of Mathematics
                Duke University[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 275.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHongkai Zhao
                zhao@math.duke.edu
                Department of Mathematics
                Duke University[0m

Box rectangle:  [32m(90.0, 301.1) -> (179.9, 311.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Joan Bruna[0m

Box rectangle:  [32m(280.3, 334.7) -> (331.7, 346.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 356.8) -> (502.1, 498.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThis paper explores the expressive power of deep neural networks for a diverse range
                of activation functions. An activation function set A is defined to encompass the majority
                of commonly used activation functions, such as ReLU, LeakyReLU, ReLU2, ELU, CELU, SELU,
                Softplus, GELU, SiLU, Swish, Mish, Sigmoid, Tanh, Arctan, Softsign, dSiLU, and SRS.
                We demonstrate that for any activation function ρ ∈A , a ReLU network of width N and
                depth L can be approximated to arbitrary precision by a ρ-activated network of width 3N
                and depth 2L on any bounded set. This finding enables the extension of most approximation
                results achieved with ReLU networks to a wide variety of other activation functions, albeit
                with slightly increased constants. Significantly, we establish that the (width, depth) scaling
                factors can be further reduced from (3, 2) to (1, 1) if ρ falls within a specific subset of A .
                This subset includes activation functions such as ELU, CELU, SELU, Softplus, GELU, SiLU,
                Swish, and Mish.[0m

Box rectangle:  [32m(109.9, 503.7) -> (502.1, 525.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                deep neural networks, rectified linear unit, diverse activation functions,
                expressive power, nonlinear approximation[0m

Box rectangle:  [32m(90.0, 547.8) -> (180.3, 559.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 572.2) -> (522.1, 678.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn the realm of artificial intelligence, deep neural networks have emerged as a powerful tool.
                By harnessing the potential of interconnected nodes organized into multiple layers, deep
                neural networks have showcased notable success in many challenging applications and new
                territories. The foundation of deep neural networks consists of an affine linear transforma-
                tion followed by an activation function. The activation function plays an important role in
                the successful training of deep neural networks. In recent years, the Rectified Linear Unit
                (ReLU) (Nair and Hinton, 2010) has experienced a surge in popularity and demonstrated its
                effectiveness as an activation function.[0m

Box rectangle:  [32m(97.7, 694.3) -> (194.0, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Corresponding author.[0m

Box rectangle:  [32m(90.0, 726.4) -> (292.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Shijun Zhang, Jianfeng Lu, and Hongkai Zhao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0912.html.[0m



=== Processing ../JMLR 2024/Deep Neural Network Approximation of Invariant Functions through Dynamical Systems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Deep Neural Network Approximation of Invariant Functions through Dynamical Systems.pdf') ---[0m

Box rectangle:  [32m(71.4, 51.9) -> (488.0, 59.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 8/22; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(102.7, 111.7) -> (457.0, 144.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep Neural Network Approximation of Invariant
                Functions through Dynamical Systems[0m

Box rectangle:  [32m(71.4, 162.0) -> (488.1, 222.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQianxiao Li
                qianxiao@nus.edu.sg
                Department of Mathematics
                Institute for Functional Intelligent Materials
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore, 119076[0m

Box rectangle:  [32m(71.4, 227.6) -> (488.1, 275.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTing Lin
                lintingsms@pku.edu.cn
                School of Mathematical Sciences
                Peking University
                5 Yiheyuan Road, Beijing, China, 100871[0m

Box rectangle:  [32m(71.4, 282.7) -> (488.1, 335.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZuowei Shen
                matzuows@nus.edu.sg
                Department of Mathematics
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore, 119076[0m

Box rectangle:  [32m(71.4, 713.8) -> (235.1, 722.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Qianxiao Li, Ting Lin, Zuowei Shen.[0m

Box rectangle:  [32m(71.4, 728.5) -> (463.7, 746.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/22-0982.html.[0m



=== Processing ../JMLR 2024/Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 6/22; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(112.4, 101.6) -> (499.7, 136.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep Nonparametric Estimation of Operators between
                Infinite Dimensional Spaces[0m

Box rectangle:  [32m(90.0, 158.0) -> (522.0, 232.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHao Liu1
                haoliu@hkbu.edu.hk
                Haizhao Yang2∗
                hzyang@umd.edu
                Minshuo Chen3
                minshuochen@princeton.edu
                Tuo Zhao4
                tourzhao@gatech.edu
                Wenjing Liao5∗
                wliao60@gatech.edu[0m

Box rectangle:  [32m(90.0, 236.9) -> (205.0, 248.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Co-corresponding author[0m

Box rectangle:  [32m(90.0, 250.2) -> (413.9, 263.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Department of Mathematics, Hong Kong Baptist University, Hong Kong[0m

Box rectangle:  [32m(90.0, 265.8) -> (522.0, 279.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2 Department of Mathematics and Department of Computer Science, University of Maryland College[0m

Box rectangle:  [32m(90.0, 285.0) -> (137.7, 295.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPark, USA[0m

Box rectangle:  [32m(90.0, 297.0) -> (449.8, 310.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m3 Department of Electrical and Computer Engineering, Princeton University, USA[0m

Box rectangle:  [32m(90.0, 312.6) -> (465.4, 326.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m4 School of Industrial and Systems Engineering, Georgia Institute of Technology, USA[0m

Box rectangle:  [32m(90.0, 328.1) -> (365.7, 341.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m5 School of Mathematics, Georgia Institute of Technology, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (375.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hao Liu, Haizhao Yang, Minshuo Chen, Tuo Zhao and Wenjing Liao.[0m

Box rectangle:  [32m(90.0, 742.4) -> (517.0, 761.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0719.html.[0m



=== Processing ../JMLR 2024/Deep Nonparametric Quantile Regression under Covariate Shift.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Deep Nonparametric Quantile Regression under Covariate Shift.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 6/24; Revised 12/24; Published 12/24[0m

Box rectangle:  [32m(99.4, 101.6) -> (512.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep Nonparametric Quantile Regression under Covariate
                Shift[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 450.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXingdong Feng
                feng.xingdong@mail.shufe.edu.cn
                School of Statistics and Data Science &
                Institute of Data Science and Statistics
                Shanghai University of Finance and Economics
                Shanghai, China
                Xin He
                he.xin17@mail.shufe.edu.cn
                School of Statistics and Data Science
                Shanghai University of Finance and Economics
                Shanghai, China
                Yuling Jiao
                yulingjiaomath@whu.edu.cn
                School of Artificial Intelligence
                Hubei Key Laboratory of Computational Science
                Wuhan University
                Wuhan, China
                Lican Kang
                kanglican@whu.edu.cn
                Institute for Math and AI
                Wuhan University
                Wuhan, China
                Caixing Wang
                wang.caixing@stu.sufe.edu.cn
                School of Statistics and Data Science
                Shanghai University of Finance and Economics
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 475.3) -> (192.9, 485.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Xiaotong Shen[0m

Box rectangle:  [32m(280.3, 510.8) -> (331.7, 522.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 528.7) -> (502.2, 670.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThis work focuses on addressing the challenges posed by covariate shift in nonparamet-
                ric quantile regression using deep neural networks. We propose a two-stage pre-training
                reweighted method that leverages importance weighting to mitigate the effects of distri-
                bution shift.
                In the first stage, density ratios are estimated with a neural network by
                minimizing least squares. In the second stage, a deep neural network estimator is obtained
                using pre-training weights. Theoretical analysis is provided, offering non-asymptotic er-
                ror bounds for the unweighted, reweighted, and pre-training reweighted estimators. We
                consider scenarios with both bounded and unbounded density ratios. Notably, we employ
                a novel proof technique to bound the generalization error, characterized by the size and
                weights bound of ReLU neural networks. This enables us to establish fast rates of con-
                vergence under the adaptive self-calibration condition, distinguishing our approach from
                those relying on local Rademacher complexity techniques. Additionally, we derive the ap-[0m

Box rectangle:  [32m(93.7, 682.1) -> (522.1, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m*. Xin He and Lican Kang are the corresponding authors. All authors contributed equally to this paper,
                and their names are listed in alphabetical order.[0m

Box rectangle:  [32m(90.0, 726.5) -> (375.8, 734.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Xingdong Feng, Xin He, Yuling Jiao, Lican Kang and Caixing Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0906.html.[0m



=== Processing ../JMLR 2024/Depth Degeneracy in Neural Networks  Vanishing Angles in Fully Connected ReLU Networks on Initialization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Depth Degeneracy in Neural Networks  Vanishing Angles in Fully Connected ReLU Networks on Initialization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 3/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(92.4, 101.6) -> (519.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDepth Degeneracy in Neural Networks: Vanishing Angles in
                Fully Connected ReLU Networks on Initialization[0m

Box rectangle:  [32m(90.0, 153.3) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCameron Jakub
                cjakub@uoguelph.ca
                Department of Mathematics & Statistics
                University of Guelph[0m

Box rectangle:  [32m(90.0, 196.5) -> (522.0, 234.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMihai Nica
                nicam@uoguelph.ca
                Department of Mathematics & Statistics
                University of Guelph[0m

Box rectangle:  [32m(90.0, 726.3) -> (239.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Cameron Jakub and Mihai Nica.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0350.html.[0m



=== Processing ../JMLR 2024/Desiderata for Representation Learning  A Causal Perspective.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Desiderata for Representation Learning  A Causal Perspective.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 9/21; Revised 3/24; Published 8/24[0m

Box rectangle:  [32m(89.3, 77.5) -> (522.7, 91.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDesiderata for Representation Learning: A Causal Perspective[0m

Box rectangle:  [32m(71.4, 104.6) -> (540.0, 128.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYixin Wang
                yixinw@umich.edu
                Department of Statistics, University of Michigan, Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(71.5, 142.7) -> (540.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael I. Jordan
                jordan@cs.berkeley.edu
                EECS and Statistics, University of California, Berkeley, CA 94720, USA
                INRIA, 75013 Paris, France[0m

Box rectangle:  [32m(72.0, 743.4) -> (231.3, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yixin Wang and Michael I. Jordan.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.3, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-1074.html.[0m



=== Processing ../JMLR 2024/Differentially Private Data Release for Mixed-type Data via Latent Factor Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Differentially Private Data Release for Mixed-type Data via Latent Factor Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 11/22; Revised 10/23; Published 4/24[0m

Box rectangle:  [32m(93.9, 101.6) -> (518.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDifferentially Private Data Release for Mixed-type Data via
                Latent Factor Models[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 382.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYanqing Zhang
                zhangyanqing@ynu.edu.cn
                Yunnan Key Laboratory of Statistical Modeling and Data Analysis
                Yunnan University
                Southwest United Graduate School
                Kunming 650500, China
                Qi Xu
                qxu6@uci.edu
                Department of Statistics
                University of California, Irvine
                Irvine, CA 92697, USA
                Niansheng Tang
                nstang@ynu.edu.cn
                Yunnan Key Laboratory of Statistical Modeling and Data Analysis
                Yunnan University
                Kunming 650500, China
                Annie Qu
                aqu2@uci.edu
                Department of Statistics
                University of California, Irvine
                Irvine, CA 92697, USA[0m

Box rectangle:  [32m(90.0, 407.6) -> (183.4, 417.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Po-Ling Loh[0m

Box rectangle:  [32m(280.3, 443.1) -> (331.7, 455.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 475.3) -> (502.2, 676.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDifferential privacy is a particular data privacy-preserving technology which enables syn-
                thetic data or statistical analysis results to be released with a minimum disclosure of private
                information from individual records. The tradeoff between privacy-preserving and utility
                guarantee is always a challenge for differential privacy technology, especially for synthetic
                data generation. In this paper, we propose a differentially private data synthesis algorithm
                for mixed-type data with correlation based on latent factor models. The proposed method
                can add a relatively small amount of noise to synthetic data under a given level of privacy
                protection while capturing correlation information. Moreover, the proposed algorithm can
                generate synthetic data preserving the same data type as mixed-type original data, which
                greatly improves the utility of synthetic data. The key idea of our method is to perturb the
                factor matrix and factor loading matrix to construct a synthetic data generation model,
                and to utilize link functions with privacy protection to ensure consistency of synthetic data
                type with original data. The proposed method can generate privacy-preserving synthetic
                data at low computation cost even when the original data is high-dimensional. In theory,
                we establish differentially private properties of the proposed method. Our numerical stud-
                ies also demonstrate superb performance of the proposed method on the utility guarantee
                of the statistical analysis based on privacy-preserved synthetic data.[0m

Box rectangle:  [32m(109.9, 695.1) -> (419.1, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Correlation, Factor model, Link function, Synthetic data[0m

Box rectangle:  [32m(90.0, 726.4) -> (325.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yanqing Zhang, Qi Xu, Niansheng Tang and Annie Qu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1324.html.[0m



=== Processing ../JMLR 2024/Differentially private methods for managing model uncertainty in linear regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Differentially private methods for managing model uncertainty in linear regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 12/21; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(135.7, 101.6) -> (476.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDifferentially Private Methods for Managing
                Model Uncertainty in Linear Regression Models[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mV ́ıctor Pe ̃na
                victor.pena.pizarro@upc.edu
                Department d’Estad ́ıstica i Investigaci ́o Operativa
                Universitat Polit`ecnica de Catalunya
                Barcelona, Spain[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndr ́es F. Barrientos
                abarrientos@fsu.edu
                Department of Statistics
                Florida State University
                Tallahassee, FL 32306, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (248.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 V ́ıctor Pe ̃na, Andr ́es F. Barrientos.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1536.html.[0m



=== Processing ../JMLR 2024/Differentially Private Topological Data Analysis.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Differentially Private Topological Data Analysis.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 5/23; Revised 11/23; Published 5/24[0m

Box rectangle:  [32m(140.0, 113.5) -> (472.1, 127.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDifferentially Private Topological Data Analysis[0m

Box rectangle:  [32m(90.0, 145.5) -> (522.0, 193.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTaegyu Kang∗
                kang426@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(90.0, 199.1) -> (522.0, 247.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSehwan Kim∗
                sehwan_kim@hphci.harvard.edu
                Department of Population Medicine
                Harvard Medical School/Harvard Pilgrim Health Care Institute
                Boston, MA, 02215, USA[0m

Box rectangle:  [32m(90.0, 252.7) -> (522.0, 301.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJinwon Sohn∗
                sohn24@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(90.0, 307.8) -> (522.0, 361.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJordan Awan†
                jawan@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (344.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Taegyu Kang, Sehwan Kim, Jinwon Sohn, and Jordan Awan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0585.html.[0m



=== Processing ../JMLR 2024/Distributed Estimation on Semi-Supervised Generalized Linear Model.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Distributed Estimation on Semi-Supervised Generalized Linear Model.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 6/22; Revised 9/23; Published 3/24[0m

Box rectangle:  [32m(107.8, 101.6) -> (504.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributed Estimation on Semi-Supervised Generalized
                Linear Model[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiyuan Tu
                tujy.19@gmail.com
                School of Mathematics
                Shanghai University of Finance and Economics, Shanghai, 200433, China[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWeidong Liu
                weidongl@sjtu.edu.cn
                School of Mathematical Sciences
                MoE Key Lab of Artificial Intelligence
                Shanghai Jiao Tong University, Shanghai, 200240, China[0m

Box rectangle:  [32m(90.0, 248.7) -> (522.0, 301.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaojun Mao
                maoxj@sjtu.edu.cn
                School of Mathematical Sciences
                Ministry of Education Key Laboratory of Scientific and Engineering Computing
                Shanghai Jiao Tong University, Shanghai, 200240, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (280.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiyuan Tu, Weidong Liu, and Xiaojun Mao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0670.html.[0m



=== Processing ../JMLR 2024/Distributed Gaussian Mean Estimation under Communication Constraints  Optimal Rates and Communication-Efficient Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Distributed Gaussian Mean Estimation under Communication Constraints  Optimal Rates and Communication-Efficient Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-63
                Submitted 3/21; Revised 12/22; Published 2/24[0m

Box rectangle:  [32m(134.0, 102.0) -> (478.0, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributed Gaussian Mean Estimation under
                Communication Constraints: Optimal Rates and[0m

Box rectangle:  [32m(176.5, 137.9) -> (435.5, 152.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCommunication-Efficient Algorithms[0m

Box rectangle:  [32m(90.0, 171.6) -> (522.0, 230.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mT. Tony Cai
                tcai@wharton.upenn.edu
                Department of Statistics and Data Science
                The Wharton School
                University of Pennsylvania
                Philadelphia, PA 19104, USA[0m

Box rectangle:  [32m(90.0, 238.7) -> (522.0, 303.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHongji Wei
                weihongji1206@gmail.com
                Department of Statistics and Data Science
                The Wharton School
                University of Pennsylvania
                Philadelphia, PA 19104, USA[0m

Box rectangle:  [32m(90.0, 328.8) -> (209.6, 338.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Andrea Montanari[0m

Box rectangle:  [32m(280.3, 362.4) -> (331.7, 374.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 381.0) -> (502.1, 474.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributed estimation of a Gaussian mean under communication constraints is studied
                in a decision theoretical framework. Minimax rates of convergence, which characterize the
                tradeoffbetween communication costs and statistical accuracy, are established under the
                independent protocols. Communication-efficient and statistically optimal procedures are
                developed. In the univariate case, the optimal rate depends only on the total communica-
                tion budget, so long as each local machine has at least one bit. However, in the multivariate
                case, the minimax rate depends on the specific allocations of the communication budgets
                among the local machines.[0m

Box rectangle:  [32m(109.9, 477.6) -> (502.1, 597.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlthough optimal estimation of a Gaussian mean is relatively simple in the conventional
                setting, it is quite involved under communication constraints, both in terms of the optimal
                procedure design and the lower bound argument. An essential step is the decomposition
                of the minimax estimation problem into two stages, localization and refinement.
                This
                critical decomposition provides a framework for both the lower bound analysis and optimal
                procedure design. The optimality results and techniques developed in the present paper can
                be useful for solving other problems such as distributed nonparametric function estimation
                and sparse signal recovery.
                Keywords:
                Communication constraints, distributed learning, Gaussian mean, minimax
                lower bound, optimal rate of convergence[0m

Box rectangle:  [32m(90.0, 618.0) -> (180.3, 630.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 640.5) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn the conventional statistical decision theoretical framework, the focus is on the cen-
                tralized setting where all the data are collected together and directly available. The main
                goal is to develop optimal (estimation, testing, detection, ...)
                procedures, where opti-
                mality is understood with respect to the sample size and parameter space. Communica-
                tion/computational costs are not part of the consideration.[0m

Box rectangle:  [32m(90.0, 726.5) -> (226.9, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 T. Tony Cai and Hongji Wei.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0316.html.[0m



=== Processing ../JMLR 2024/Distributed Kernel-Driven Data Clustering.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Distributed Kernel-Driven Data Clustering.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (520.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 5/23; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(174.5, 101.7) -> (437.5, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributed Kernel-Driven Data Clustering[0m

Box rectangle:  [32m(90.0, 137.0) -> (521.8, 188.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIoannis Schizas
                IOANNIS.D.SCHIZAS.CIV@ARMY.MIL
                US Army Combat Capabilities Development Command
                Army Research Lab
                Aberdeen Proving Ground, MD 21005, USA[0m

Box rectangle:  [32m(90.0, 726.4) -> (149.4, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 I. Schizas.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0669.html.[0m



=== Processing ../JMLR 2024/Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Distributionally Robust Model-Based Offline Reinforcement Learning with Near-Optimal Sample Complexity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-91
                Submitted 12/22; Revised 12/23; Published 7/24[0m

Box rectangle:  [32m(93.6, 101.6) -> (518.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributionally Robust Model-Based Offline Reinforcement
                Learning with Near-Optimal Sample Complexity[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLaixi Shi
                laixis@caltech.edu
                Computing Mathematical Sciences
                California Institute of Technology
                Pasadena, CA, 91125, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuejie Chi
                yuejiechi@cmu.edu
                Department of Electrical and Computer Engineering
                Carnegie Mellon University
                Pittsburgh, PA, 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (211.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Laixi Shi and Yuejie Chi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1482.html.[0m



=== Processing ../JMLR 2024/Distribution Learning via Neural Differential Equations  A Nonparametric Statistical Perspective.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Distribution Learning via Neural Differential Equations  A Nonparametric Statistical Perspective.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-61
                Submitted 10/23; Published 6/24[0m

Box rectangle:  [32m(97.5, 101.6) -> (514.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistribution Learning via Neural Differential Equations: A
                Nonparametric Statistical Perspective[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoussef Marzouk
                ymarz@mit.edu
                Institute for Data, Systems, and Society
                Massachusetts Institute of Technology
                Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhi Ren
                zren@mit.edu
                Department of Mathematics
                Massachusetts Institute of Technology
                Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSven Wang
                svenwang@mit.edu
                Institute for Data, Systems, and Society
                Massachusetts Institute of Technology
                Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJakob Zech
                jakob.zech@uni-heidelberg.de
                Interdisciplinary Center for Scientific Computing
                Heidelberg University
                Heidelberg 69120, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (328.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Youssef Marzouk, Zhi Ren, Sven Wang, and Jakob Zech.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1280.html.[0m



=== Processing ../JMLR 2024/DoWhy-GCM  An Extension of DoWhy for Causal Inference in Graphical Causal Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/DoWhy-GCM  An Extension of DoWhy for Causal Inference in Graphical Causal Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-7
                Submitted 11/22; Revised 12/23; Published 5/24[0m

Box rectangle:  [32m(94.9, 101.7) -> (517.1, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDoWhy-GCM: An Extension of DoWhy for Causal Inference in Graphical
                Causal Models[0m

Box rectangle:  [32m(90.0, 153.4) -> (519.3, 164.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPatrick Blöbaum
                BLOEBP@AMAZON.COM[0m

Box rectangle:  [32m(90.0, 171.1) -> (519.3, 182.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Götz
                PEGO@AMAZON.COM[0m

Box rectangle:  [32m(90.0, 188.8) -> (519.3, 199.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKailash Budhathoki
                KAIBUD@AMAZON.COM[0m

Box rectangle:  [32m(90.0, 206.5) -> (519.3, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAtalanti A. Mastakouri
                ATALANTI@AMAZON.COM[0m

Box rectangle:  [32m(90.0, 225.8) -> (521.7, 250.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDominik Janzing
                JANZIND@AMAZON.COM
                Amazon Web Services[0m

Box rectangle:  [32m(90.0, 726.4) -> (425.2, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Patrick Blöbaum, Peter Götz, Kailash Budhathoki, Atalanti A. Mastakouri and Dominik Janzing.[0m

Box rectangle:  [32m(90.0, 741.2) -> (491.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1258.html.[0m



=== Processing ../JMLR 2024/Dropout Regularization Versus l2-Penalization in the Linear Model.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Dropout Regularization Versus l2-Penalization in the Linear Model.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 6/23; Revised 4/24; Published 7/24[0m

Box rectangle:  [32m(124.6, 101.6) -> (452.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDropout Regularization Versus l2-Penalization
                in the Linear Model[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 233.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGabriel Clara
                g.clara@utwente.nl
                Sophie Langer
                s.langer@utwente.nl
                Johannes Schmidt-Hieber
                a.j.schmidt-hieber@utwente.nl
                Faculty of Electrical Engineering, Mathematics, and Computer Science
                University of Twente
                7522 NB, Enschede, The Netherlands[0m

Box rectangle:  [32m(90.0, 726.3) -> (345.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Gabriel Clara, Sophie Langer, and Johannes Schmidt-Hieber.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0803.html.[0m



=== Processing ../JMLR 2024/Effect-Invariant Mechanisms for Policy Generalization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Effect-Invariant Mechanisms for Policy Generalization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 6/23; Revised 11/23; Published 01/24[0m

Box rectangle:  [32m(118.5, 101.5) -> (493.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEffect-Invariant Mechanisms for Policy Generalization[0m

Box rectangle:  [32m(90.0, 133.5) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSorawit Saengkyongam∗
                sorawit.saengkyongam@stat.math.ethz.ch
                Seminar for Statistics
                ETH Zürich
                Zürich, Switzerland[0m

Box rectangle:  [32m(90.0, 187.4) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNiklas Pfister
                np@math.ku.dk
                Department of Mathematical Sciences
                University of Copenhagen
                Copenhagen, Denmark[0m

Box rectangle:  [32m(90.0, 241.0) -> (522.0, 289.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPredrag Klasnja
                klasnja@umich.edu
                School of Information
                University of Michigan
                Ann Arbor, MI, USA[0m

Box rectangle:  [32m(90.0, 294.6) -> (522.0, 354.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSusan Murphy
                samurphy@g.harvard.edu
                Department of Statistics
                Department of Computer Science
                Harvard University
                Cambridge, MA, USA[0m

Box rectangle:  [32m(90.0, 361.4) -> (522.0, 414.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonas Peters∗
                jonas.peters@stat.math.ethz.ch
                Seminar for Statistics
                ETH Zürich
                Zürich, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (436.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sorawit Saengkyongam, Niklas Pfister, Predrag Klasnja, Susan Murphy, Jonas Peters.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0802.html.[0m



=== Processing ../JMLR 2024/Efficient Active Manifold Identification via Accelerated Iteratively Reweighted Nuclear Norm Minimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Efficient Active Manifold Identification via Accelerated Iteratively Reweighted Nuclear Norm Minimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 4/23 Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(115.3, 101.5) -> (496.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEfficient Active Manifold Identification via Accelerated
                Iteratively Reweighted Nuclear Norm Minimization[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHao Wang
                haw309@gmail.com
                School of Information Science and Technology
                ShanghaiTech University
                Shanghai, 201210, China[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYe Wang
                wangye_77@outlook.com
                School of Information Science and Technology
                ShanghaiTech University
                Shanghai, 201210, China[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 354.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiangyu Yang*
                yangxy@henu.edu.cn
                School of Mathematics and Statistics
                Henan University
                Kaifeng, 475000, China
                Center for Applied Mathematics of Henan Province
                Henan University
                Zhengzhou, 450046, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (269.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hao Wang, Ye Wang and Xiangyu Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0449.html.[0m



=== Processing ../JMLR 2024/Efficient Convex Algorithms for Universal Kernel Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Efficient Convex Algorithms for Universal Kernel Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 4/23; Revised 1/24; Published 6/24[0m

Box rectangle:  [32m(97.6, 101.6) -> (514.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEfficient Convex Algorithms for Universal Kernel Learning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAleksandr Talitckii
                atalitck@asu.edu
                Department of Mechanical and Aerospace Engineering
                Arizona State University
                Tempe, AZ 85281-1776, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBrendon Colbert
                bkcolbe1@asu.edu
                Department of Mechanical and Aerospace Engineering
                Arizona State University
                Tempe, AZ 85281-1776, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew M. Peet
                mpeet@asu.edu
                Department of Mechanical and Aerospace Engineering
                Arizona State University
                Tempe, AZ 85281-1776, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (342.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Aleksandr Talitckii, Brendon Colbert and Matthew M. Peet.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0528.html.[0m



=== Processing ../JMLR 2024/Efficient Modality Selection in Multimodal Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Efficient Modality Selection in Multimodal Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 4/23; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(124.9, 101.5) -> (487.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEfficient Modality Selection in Multimodal Learning[0m

Box rectangle:  [32m(90.0, 133.5) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYifei He∗
                yifeihe3@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 163.2) -> (522.0, 187.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRunxiang Cheng∗
                rcheng12@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 192.9) -> (522.0, 217.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGargi Balasubramaniam∗
                gargib2@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 222.9) -> (522.0, 247.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYao-Hung Hubert Tsai
                yaohungt@cs.cmu.edu
                Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 254.1) -> (522.0, 279.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHan Zhao
                hanzhao@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 726.3) -> (458.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yifei He, Runxiang Cheng, Gargi Balasubramaniam, Yao-Hung Hubert Tsai and Han Zhao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0439.html.[0m



=== Processing ../JMLR 2024/Empirical Design in Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Empirical Design in Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-63
                Submitted 2/23; Revised10/24; Published 10/24[0m

Box rectangle:  [32m(151.9, 101.5) -> (460.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmpirical Design in Reinforcement Learning[0m

Box rectangle:  [32m(88.3, 135.5) -> (522.0, 228.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrew Patterson
                ap3@ualberta.ca
                Samuel Neumann
                sfneuman@ualberta.ca
                Martha White†
                whitem@ualberta.ca
                Adam White†
                amw8@ualberta.ca
                Department of Computing Science and Alberta Machine Intelligence Institute (Amii)
                University of Alberta, Edmonton, Canada
                †Canada CIFAR AI Chair[0m

Box rectangle:  [32m(90.0, 726.3) -> (252.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Patterson, Neumann, White, White.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0183.html.[0m



=== Processing ../JMLR 2024/ENNS  Variable Selection  Regression  Classification  and Deep Neural Network for High-Dimensional Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/ENNS  Variable Selection  Regression  Classification  and Deep Neural Network for High-Dimensional Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 8/21; Revised 11/23; Published 10/24[0m

Box rectangle:  [32m(105.4, 101.6) -> (506.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mENNS: Variable Selection, Regression, Classification and
                Deep Neural Network for High-Dimensional Data[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKaixu Yang
                kaixuyang@gmail.com
                LinkedIn Corporation,
                Mountain View, California,
                USA.[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArkaprabha Ganguli
                aganguli@anl.gov
                Argonne National Laboratory
                Lemont, Illinois,
                USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTapabrata Maiti
                maiti@msu.edu
                Michigan State University
                East Lansing, Michigan USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (311.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kaixu Yang, Arkaprabha Ganguli, Tapabrata Maiti.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0893.html.[0m



=== Processing ../JMLR 2024/Entropic Gromov-Wasserstein Distances  Stability and Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Entropic Gromov-Wasserstein Distances  Stability and Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 1/24; Published 10/24[0m

Box rectangle:  [32m(165.8, 101.5) -> (448.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEntropic Gromov-Wasserstein Distances:
                Stability and Algorithms[0m

Box rectangle:  [32m(88.8, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGabriel Rioux
                ger84@cornell.edu
                Center for Applied Mathematics
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(88.8, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZiv Goldfeld
                goldfeld@cornell.edu
                School of Electrical and Computer Engineering
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(88.8, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKengo Kato
                kk976@cornell.edu
                Department of Statistics and Data Science
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (290.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Gabriel Rioux, Ziv Goldfeld, and Kengo Kato.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0039.html.[0m



=== Processing ../JMLR 2024/Estimating the Minimizer and the Minimum Value of a Regression Function under Passive Design.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Estimating the Minimizer and the Minimum Value of a Regression Function under Passive Design.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 12/22; Revised 10/23; Published 1/24[0m

Box rectangle:  [32m(109.9, 102.0) -> (502.1, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEstimating the Minimizer and the Minimum Value of a[0m

Box rectangle:  [32m(156.8, 120.0) -> (455.2, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRegression Function under Passive Design[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 188.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArya Akhavan
                aria.akhavanfoomani@iit.it
                Istituto Italiano di Tecnologia
                CREST, ENSAE, IP Paris[0m

Box rectangle:  [32m(90.0, 195.3) -> (522.0, 217.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavit Gogolashvili
                davit.gogolashvili@eurecom.fr
                Data Science Department, EURECOM, France[0m

Box rectangle:  [32m(90.0, 226.5) -> (522.0, 250.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexandre B. Tsybakov
                alexandre.tsybakov@ensae.fr
                CREST, ENSAE, IP Paris[0m

Box rectangle:  [32m(90.0, 275.9) -> (179.6, 285.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Aarti Singh[0m

Box rectangle:  [32m(280.3, 311.5) -> (331.7, 323.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 328.3) -> (502.1, 498.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe propose a new method for estimating the minimizer x∗and the minimum value f ∗of
                a smooth and strongly convex regression function f from the observations contaminated
                by random noise.
                Our estimator zn of the minimizer x∗is based on a version of the
                projected gradient descent with the gradient estimated by a regularized local polynomial
                algorithm. Next, we propose a two-stage procedure for estimation of the minimum value
                f ∗of regression function f. At the first stage, we construct an accurate enough estimator
                of x∗, which can be, for example, zn. At the second stage, we estimate the function value
                at the point obtained in the first stage using a rate optimal nonparametric procedure. We
                derive non-asymptotic upper bounds for the quadratic risk and optimization risk of zn,
                and for the risk of estimating f ∗. We establish minimax lower bounds showing that, under
                certain choice of parameters, the proposed algorithms achieve the minimax optimal rates
                of convergence on the class of smooth and strongly convex functions.
                Keywords: Nonparametric regression, Stochastic optimization, Minimax optimality, Pas-
                sive design, Local polynomial estimator[0m

Box rectangle:  [32m(90.0, 519.3) -> (180.3, 531.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 542.4) -> (522.0, 607.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEstimating the minimum value and the minimizer of an unknown function from observation
                of its noisy values on a finite set of points is a key problem in many applications. Let
                D = {x1, . . . , xn} ⊂Rd be a design set and let Θ be a compact and convex subset of Rd.
                Assume that we observe noisy values of an unknown regression function f : Rd →R at
                points of the design set:[0m

Box rectangle:  [32m(234.2, 614.9) -> (522.0, 627.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33myi = f(xi) + ξi,
                i = 1, . . . , n,
                (1)[0m

Box rectangle:  [32m(90.0, 640.5) -> (522.0, 715.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mwhere ξi’s are independent zero mean errors. Our goal is to estimate the minimum value
                of the regression function f∗= minx∈Θ f(x) and its location x∗= arg minx∈Θ f(x) when
                x∗is unique. As accuracy measures of an estimator ˆxn of x∗we consider the optimization
                risk E[f(ˆxn) −f∗] and the quadratic risk E[󰀂ˆxn −x∗󰀂2], where 󰀂· 󰀂denotes the Euclidean
                norm. The accuracy of an estimator Tn of f∗will be measured by the risk E|Tn −f∗|. We[0m

Box rectangle:  [32m(90.0, 726.5) -> (354.3, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Arya Akhavan, Davit Gogolashvili and Alexandre B. Tsybakov.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1396.html.[0m



=== Processing ../JMLR 2024/Estimating the Replication Probability of Significant Classification Benchmark Experiments.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Estimating the Replication Probability of Significant Classification Benchmark Experiments.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 1/24; Revised 6/24; Published 9/24[0m

Box rectangle:  [32m(121.0, 101.6) -> (491.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEstimating the Replication Probability
                of Significant Classification Benchmark Experiments[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 287.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel Berrar
                daniel.berrar@open.ac.uk
                Machine Learning Research Group
                School of Mathematics and Statistics
                The Open University
                Milton Keynes MK7 6AA, United Kingdom
                and
                Department of Information and Communications Engineering
                School of Engineering
                Tokyo Institute of Technology
                2-12-1-S3-70 Ookayama, Meguro-ku, Tokyo 152-8550, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (171.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Daniel Berrar.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0158.html.[0m



=== Processing ../JMLR 2024/Estimation of Sparse Gaussian Graphical Models with Hidden Clustering Structure.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Estimation of Sparse Gaussian Graphical Models with Hidden Clustering Structure.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 4/20; Published 9/24[0m

Box rectangle:  [32m(114.4, 101.6) -> (497.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEstimation of Sparse Gaussian Graphical Models with
                Hidden Clustering Structure[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMeixia Lin
                meixia lin@sutd.edu.sg
                Engineering Systems and Design
                Singapore University of Technology and Design
                8 Somapah Road, Singapore, 487372[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDefeng Sun
                defeng.sun@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Hung Hom, Kowloon, Hong Kong[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKim-Chuan Toh
                mattohkc@nus.edu.sg
                Department of Mathematics and Institute of Operations Research and Analytics
                National University of Singapore
                10 Lower Kent Ridge Road, Singapore, 119076[0m

Box rectangle:  [32m(90.0, 313.9) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChengjing Wang∗
                renascencewang@hotmail.com
                School of Mathematics
                Southwest Jiaotong University
                Chengdu 611756, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (354.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Meixia Lin, Defeng Sun, Kim-Chuan Toh and Chengjing Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/20-354.html.[0m



=== Processing ../JMLR 2024/Estimation of the Order of Non-Parametric Hidden Markov Models using the Singular Values of an Integral Operator.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Estimation of the Order of Non-Parametric Hidden Markov Models using the Singular Values of an Integral Operator.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 10/23; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(94.4, 101.6) -> (517.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEstimation of the Order of Non-Parametric Hidden Markov
                Models using the Singular Values of an Integral Operator[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarie Du Roy de Chaumaray
                marie.du-roy-de-chaumaray@univ-rennes2.fr
                Mathematical Research Institute of Rennes IRMAR
                Rennes University, Rennes, France[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSalima El Kolei
                salima.el-kolei@ensai.fr
                Univ. Rennes, Ensai, CNRS, CREST
                UMR 9194, F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarie-Pierre Etienne
                marie-pierre.etienne@institut-agro.fr
                Mathematical Research Institute of Rennes IRMAR
                Rennes University, Rennes, France[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthieu Marbac
                matthieu.marbac-lourdelle@ensai.fr
                Univ. Rennes, Ensai, CNRS, CREST
                UMR 9194, F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (459.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Marie Du Roy de Chaumaray, Salima El Kolei, Marie-Pierre Etienne and Matthieu Marbac.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1372.html.[0m



=== Processing ../JMLR 2024/Euler Characteristic Tools for Topological Data Analysis.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Euler Characteristic Tools for Topological Data Analysis.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 3/23; Revised 7/24; Published 7/24[0m

Box rectangle:  [32m(106.1, 101.6) -> (506.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEuler Characteristic Tools for Topological Data Analysis[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOlympio Hacquard
                olympio.hacquard@universite-paris-saclay.fr
                Laboratoire de Math ́ematiques d’Orsay
                Universit ́e Paris-Saclay, CNRS, Inria
                91400 Orsay France[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVadim Lebovici
                vadim.lebovici@maths.ox.ac.uk
                Mathematical Institute
                University of Oxford,
                Oxford, United Kingdom[0m

Box rectangle:  [32m(90.0, 267.1) -> (331.7, 286.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Sayan Mukherjee
                Abstract[0m

Box rectangle:  [32m(109.9, 291.1) -> (502.1, 445.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this article, we study Euler characteristic techniques in topological data analysis. Point-
                wise computing the Euler characteristic of a family of simplicial complexes built from data
                gives rise to the so-called Euler characteristic profile. We show that this simple descriptor
                achieves state-of-the-art performance in supervised tasks at a meager computational cost.
                Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles.
                These integral transforms mix Euler characteristic techniques with Lebesgue integration to
                provide highly efficient compressors of topological signals. As a consequence, they show
                remarkable performances in unsupervised settings. On the qualitative side, we provide nu-
                merous heuristics on the topological and geometric information captured by Euler profiles
                and their hybrid transforms. Finally, we prove stability results for these descriptors as well
                as asymptotic guarantees in random settings.
                Keywords:
                Topological Data Analysis, Machine Learning, Multiparameter Persistence,
                Euler characteristic profiles, Hybrid transforms[0m

Box rectangle:  [32m(90.0, 465.4) -> (176.6, 477.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 486.9) -> (522.1, 701.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mExtracting topological information from data of various natures follows a machinery that
                finds its origins in the works of Edelsbrunner et al. (2000). The main idea consists of building
                a one-parameter family of topological spaces on top of data and tracking the evolution of its
                topology, typically via homological computations. This multi-scale topological information
                is recorded in the form of what is called a persistence diagram; see Edelsbrunner et al. (2000);
                Edelsbrunner and Harer (2022). The space of persistence diagrams is a metric space for
                the so-called bottleneck distance, (Cohen-Steiner et al., 2007), but it cannot be isometrically
                embedded into a Hilbert space (Carri`ere and Bauer, 2018; Bubenik and Wagner, 2020).
                At the cost of losing some information, these diagrams are still often turned into vectors
                to perform various learning tasks such as classification, clustering, or regression.
                Most
                commonly used techniques include persistence images (Adams et al., 2017), landscapes
                (Bubenik et al., 2015), and more recently measure-oriented vectorizations in Royer et al.
                (2021) and neural network methods from Carri`ere et al. (2020); Reinauer et al. (2021). An
                overview of topological methods in machine learning has been presented in the survey of
                Hensel et al. (2021). These methods have demonstrated their efficiency in a wide variety of
                applications and types of data, such as health applications (Rieck et al., 2020; Fern ́andez[0m

Box rectangle:  [32m(90.0, 726.4) -> (268.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Olympio Hacquard and Vadim Lebovici.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0353.html.[0m



=== Processing ../JMLR 2024/Evidence Estimation in Gaussian Graphical Models Using a Telescoping Block Decomposition of the Precision Matrix.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Evidence Estimation in Gaussian Graphical Models Using a Telescoping Block Decomposition of the Precision Matrix.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-43
                Submitted 3/23; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(94.5, 101.6) -> (517.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEvidence Estimation in Gaussian Graphical Models Using a
                Telescoping Block Decomposition of the Precision Matrix[0m

Box rectangle:  [32m(88.2, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnindya Bhadra
                bhadra@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(88.2, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKsheera Sagar
                kkeralap@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(88.2, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Rowe
                rowe4@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(88.8, 312.6) -> (521.9, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSayantan Banerjee
                sayantanb@iimidr.ac.in
                Operations Management & Quantitative Techniques Area
                Indian Institute of Management Indore
                Indore, MP 453556, India[0m

Box rectangle:  [32m(88.1, 367.8) -> (522.0, 420.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJyotishka Datta
                jyotishka@vt.edu
                Department of Statistics
                Virginia Polytechnic Institute and State University
                Blacksburg, VA 24060, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (167.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bhadra et al.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0254.html.[0m



=== Processing ../JMLR 2024/Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Existence and Minimax Theorems for Adversarial Surrogate Risks in Binary Classification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 4/23; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(92.6, 101.6) -> (519.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mExistence and Minimax Theorems for Adversarial Surrogate
                Risks in Binary Classification[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNatalie S. Frank
                nf1066@nyu.edu
                Courant Institute
                New York University
                New York, NY 10012, USA[0m

Box rectangle:  [32m(90.0, 220.6) -> (522.0, 273.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonathan Niles-Weed
                jnw@cims.nyu.edu
                Courant Institute and Center for Data Science
                New York University
                New York, NY 10012, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (272.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Natalie S. Frank & Jonathan Niles-Weed.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0456.html.[0m



=== Processing ../JMLR 2024/Exploration  Exploitation  and Engagement in Multi-Armed Bandits with Abandonment.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Exploration  Exploitation  and Engagement in Multi-Armed Bandits with Abandonment.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 11/22; Revised 8/23; Published 2/24[0m

Box rectangle:  [32m(146.2, 101.5) -> (465.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mExploration, Exploitation, and Engagement in
                Multi-Armed Bandits with Abandonment[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZixian Yang
                zixian@umich.edu
                Electrical Engineering and Computer Science
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(89.5, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXin Liu ̊
                liuxin7@shanghaitech.edu.cn
                School of Information Science and Technology
                ShanghaiTech University
                Shanghai, China[0m

Box rectangle:  [32m(88.3, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLei Ying
                leiying@umich.edu
                Electrical Engineering and Computer Science
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (252.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zixian Yang, Xin Liu, and Lei Ying.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1251.html.[0m



=== Processing ../JMLR 2024/Exploration of the Search Space of Gaussian Graphical Models for Paired Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Exploration of the Search Space of Gaussian Graphical Models for Paired Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 3/23; Revised 11/23; Published 4/24[0m

Box rectangle:  [32m(111.8, 101.6) -> (500.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mExploration of the Search Space of Gaussian Graphical
                Models for Paired Data[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlberto Roverato
                alberto.roverato@unipd.it
                Department of Statistical Sciences
                University of Padova, Italy.[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 273.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDung Ngoc Nguyen
                ngocdung.nguyen@csiro.au
                Department of Statistical Sciences
                University of Padova, Italy.
                CSIRO Agriculture and Food
                Canberra, ACT, Australia.[0m

Box rectangle:  [32m(90.0, 726.4) -> (276.7, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alberto Roverato and Dung Ngoc Nguyen.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0295.html.[0m



=== Processing ../JMLR 2024/Exponential Tail Local Rademacher Complexity Risk Bounds Without the Bernstein Condition.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Exponential Tail Local Rademacher Complexity Risk Bounds Without the Bernstein Condition.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-43
                Submitted 1/23; Revised 6/24; Published 11/24[0m

Box rectangle:  [32m(118.1, 101.6) -> (494.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mExponential Tail Local Rademacher Complexity Risk
                Bounds Without the Bernstein Condition[0m

Box rectangle:  [32m(90.0, 153.3) -> (414.3, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVarun Kanade Department of Computer Science, University of Oxford[0m

Box rectangle:  [32m(90.0, 171.0) -> (398.9, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPatrick Rebeschini Department of Statistics, University of Oxford[0m

Box rectangle:  [32m(90.0, 188.6) -> (196.5, 201.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTomas Vaˇskeviˇcius∗[0m

Box rectangle:  [32m(90.0, 726.3) -> (323.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Varun Kanade, Patrick Rebeschini, Tomas Vaˇskeviˇcius.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0063.html.[0m



=== Processing ../JMLR 2024/Fairness guarantees in multi-class classification with demographic parity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fairness guarantees in multi-class classification with demographic parity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 3/23; Revised 11/23; Published 3/24[0m

Box rectangle:  [32m(126.2, 101.6) -> (486.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFairness Guarantees in
                Multi-class Classification with Demographic Parity[0m

Box rectangle:  [32m(89.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristophe Denis
                christophe.denis@univ-eiffel.fr
                LPSM, UMR-CNRS 8001,
                Sorbonne Universit ́e
                4, Place Jussieu, 75005 Paris, France[0m

Box rectangle:  [32m(88.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRomuald Elie
                romuald.elie@univ-eiffel.fr
                LAMA, UMR-CNRS 8050,
                Universit ́e Gustave Eiffel
                5 Bd Descartes, 77454 Marne-la-Vall ́ee cedex 2, France[0m

Box rectangle:  [32m(88.3, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMohamed Hebiri
                mohamed.hebiri@univ-eiffel.fr
                LAMA, UMR-CNRS 8050,
                Universit ́e Gustave Eiffel
                5 Bd Descartes, 77454 Marne-la-Vall ́ee cedex 2, France[0m

Box rectangle:  [32m(88.8, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFran ̧cois Hu
                francois.hu@milliman.com
                R&D Department, AI Lab,
                Milliman France
                14 Av. de la Grande Arm ́ee, 75017 Paris, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (372.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Christophe Denis, Romuald Elie, Mohamed Hebiri and Fran ̧cois HU.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0000.html.[0m



=== Processing ../JMLR 2024/Fairness in Survival Analysis with Distributionally Robust Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fairness in Survival Analysis with Distributionally Robust Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-85
                Submitted 7/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(176.0, 101.5) -> (436.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFairness in Survival Analysis with
                Distributionally Robust Optimization[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShu Hu∗
                hu968@purdue.edu
                Department of Computer and Information Technology
                Purdue University
                Indianapolis, IN, 46202, USA[0m

Box rectangle:  [32m(90.0, 206.6) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge H. Chen∗†
                georgechen@cmu.edu
                Heinz College of Information Systems and Public Policy
                Carnegie Mellon University
                Pittsburgh, PA, 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (227.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shu Hu and George H. Chen.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0888.html.[0m



=== Processing ../JMLR 2024/False discovery proportion envelopes with m-consistency.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/False discovery proportion envelopes with m-consistency.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 23 (2024) 1-52
                Submitted 7/23; Revised 6/24; Published 9/24[0m

Box rectangle:  [32m(106.9, 101.6) -> (505.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFalse discovery proportion envelopes with m-consistency[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIqraa Meah
                iqraa.meah@inserm.fr
                Center for Research in Epidemiology and StatisticS (CRESS)
                Universit ́e Paris Cit ́e and Universit ́e Sorbonne Paris Nord, Inserm, INRAE
                F-75004 Paris, France[0m

Box rectangle:  [32m(90.0, 187.5) -> (521.9, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGilles Blanchard
                gilles.blanchard@universite-paris-saclay.fr
                Institut Math ́ematiques de Orsay (IMO)
                CNRS, Inria, Universit ́e Paris-Saclay
                F-91405 Orsay Cedex[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 309.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEtienne Roquain
                etienne.roquain@sorbonne-universite.fr
                Laboratoire de Probabilit ́es, Statistique et Mod ́elisation,
                CNRS, Sorbonne Universit ́e, Universit ́e de Paris.
                Sorbonne Universit ́e
                4 place Jussieu, 75005, Paris[0m

Box rectangle:  [32m(90.0, 726.3) -> (311.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Iqraa Meah, Gilles Blanchard and Etienne Roquain.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v23/23-1025.html.[0m



=== Processing ../JMLR 2024/Faster Randomized Methods for Orthogonality Constrained Problems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Faster Randomized Methods for Orthogonality Constrained Problems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 8/21; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(99.1, 101.5) -> (512.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFaster Randomized Methods for Orthogonality Constrained
                Problems[0m

Box rectangle:  [32m(90.0, 153.4) -> (522.0, 219.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoris Shustin
                borisshy@gmail.com
                Haim Avron
                haimav@tauex.tau.ac.il
                Department of Applied Mathematics
                Tel Aviv University
                Tel Aviv, 69978, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (220.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Boris Shustin, Haim Avron.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1022.html.[0m



=== Processing ../JMLR 2024/Faster Rates of Differentially Private Stochastic Convex Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Faster Rates of Differentially Private Stochastic Convex Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 1/22; Revised 9/23; Published 4/24[0m

Box rectangle:  [32m(108.2, 101.6) -> (503.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFaster Rates of Differentially Private Stochastic Convex
                Optimization[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJinyan Su∗
                js3673@cornell.edu
                Cornell University[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLijie Hu
                lijie.hu@kaust.edu.sa
                Provable Responsible AI and Data Analytics Lab
                Division of CEMSE
                King Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 248.7) -> (522.0, 315.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDi Wang
                di.wang@kaust.edu.sa
                Provable Responsible AI and Data Analytics Lab
                Division of CEMSE
                King Abdullah University of Science and Technology
                Thuwal, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 726.3) -> (230.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jinyan Su, Lijie Hu, Di Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0079.html.[0m



=== Processing ../JMLR 2024/Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 10/21; Revised 12/22 & 10/23; Published 1/24[0m

Box rectangle:  [32m(96.6, 101.6) -> (515.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFast Policy Extragradient Methods for Competitive Games
                with Entropy Regularization[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShicong Cen
                shicongc@andrew.cmu.edu
                Department of Electrical and Computer Engineering
                Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuting Wei
                ytwei@wharton.upenn.edu
                Department of Statistics and Data Science, The Wharton School
                University of Pennsylvania[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 275.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuejie Chi
                yuejiechi@cmu.edu
                Department of Electrical and Computer Engineering
                Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 726.4) -> (271.8, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shicong Cen, Yuting Wei and Yuejie Chi.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1205.html.[0m



=== Processing ../JMLR 2024/Fast Rates in Pool-Based Batch Active Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fast Rates in Pool-Based Batch Active Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 12/22; Revised 3/24; Published 9/24[0m

Box rectangle:  [32m(134.6, 101.6) -> (477.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFast Rates in Pool-Based Batch Active Learning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mClaudio Gentile
                cgentile@google.com
                Google Research
                New York City, NY, USA[0m

Box rectangle:  [32m(90.0, 175.6) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhilei Wang
                zhileiwang92@gmail.com
                WorldQuant LLC
                New York City, NY, USA[0m

Box rectangle:  [32m(90.0, 218.8) -> (522.0, 244.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTong Zhang
                tozhang@illinois.edu
                University of Illinois Urbana-Champaign, IL[0m

Box rectangle:  [32m(90.0, 726.3) -> (306.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Claudio Gentile and Zhilei Wang and Tong Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1409.html.[0m



=== Processing ../JMLR 2024/Fat-Shattering Dimension of k-fold Aggregations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fat-Shattering Dimension of k-fold Aggregations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-29
                Submitted 10/21; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(134.4, 99.7) -> (482.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFat-Shattering Dimension of k-fold Aggregations∗[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 188.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIdan Attias
                idanatti@post.bgu.ac.il
                Aryeh Kontorovich
                karyeh@cs.bgu.ac.il
                Department of Computer Science
                Ben-Gurion University of the Negev, Beer Sheva, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (252.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Idan Attias and Aryeh Kontorovich.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1193.html.[0m



=== Processing ../JMLR 2024/FedCBO  Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/FedCBO  Reaching Group Consensus in Clustered Federated Learning through Consensus-based Optimization.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 6/23; Revised 4/24; Published 7/24[0m

Box rectangle:  [32m(90.5, 82.7) -> (521.6, 115.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFedCBO: Reaching Group Consensus in Clustered Federated
                Learning through Consensus-based Optimization[0m

Box rectangle:  [32m(72.0, 133.0) -> (540.0, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJos ́e A. Carrillo
                carrillo@maths.ox.ac.uk
                Mathematical Institute
                University of Oxford
                Oxford OX2 6GG, UK[0m

Box rectangle:  [32m(72.0, 186.6) -> (540.0, 234.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicol ́as Garc ́ıa Trillos
                garciatrillo@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                1300 University Avenue, Madison, Wisconsin 53706, USA[0m

Box rectangle:  [32m(72.0, 240.2) -> (540.0, 288.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSixu Li
                sli739@wisc.edu
                Department of Statistics
                University of Wisconsin-Madison
                1300 University Avenue, Madison, Wisconsin 53706, USA[0m

Box rectangle:  [32m(72.0, 295.3) -> (539.9, 348.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuhua Zhu
                yuhua.zhu@stat.ucla.edu
                Department of Statistics and Data Science,
                University of California, Los Angeles
                Los Angeles, California 90095-1554, USA[0m

Box rectangle:  [32m(72.0, 743.4) -> (338.2, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jos ́e A. Carrillo, Nicol ́as Garc ́ıa Trillos, Sixu Li and Yuhua Zhu.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0764.html.[0m



=== Processing ../JMLR 2024/Federated Automatic Differentiation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Federated Automatic Differentiation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-39
                Submitted 2/23; Revised 4/24; Published 11/24[0m

Box rectangle:  [32m(179.9, 101.5) -> (432.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFederated Automatic Differentiation[0m

Box rectangle:  [32m(88.8, 133.9) -> (522.0, 169.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeith Rush
                krush@google.com
                Google Research
                Seattle, WA, USA[0m

Box rectangle:  [32m(88.8, 175.5) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZachary Charles
                zachcharles@google.com
                Google Research
                Seattle, WA, USA[0m

Box rectangle:  [32m(88.8, 218.7) -> (522.0, 258.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZachary Garrett
                zachgarrett@google.com
                Google Research
                Seattle, WA, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (322.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Keith Rush and Zachary Charles and Zachary Garrett.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0223.html.[0m



=== Processing ../JMLR 2024/Fermat Distances  Metric Approximation  Spectral Convergence  and Clustering Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fermat Distances  Metric Approximation  Spectral Convergence  and Clustering Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 7/23; Published 6/24[0m

Box rectangle:  [32m(130.4, 102.0) -> (481.6, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFermat Distances: Metric Approximation,
                Spectral Convergence, and Clustering Algorithms[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicol ́as Garc ́ıa Trillos
                garciatrillo@wisc.edu
                Department of Statistics
                University of Wisconsin
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 207.2) -> (522.0, 253.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Little
                little@math.utah.edu
                Department of Mathematics, Utah Center for Data Science
                University of Utah
                Salt Lake City, UT 84112, USA[0m

Box rectangle:  [32m(90.0, 260.8) -> (522.0, 307.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel McKenzie
                dmckenzie@mines.edu
                Department of Applied Mathematics and Statistics
                Colorado School of Mines
                Golden, CO 80401, USA[0m

Box rectangle:  [32m(90.0, 316.0) -> (522.0, 367.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJames M. Murphy
                jm.murphy@tufts.edu
                Department of Mathematics
                Tufts University
                Medford, MA 02155, USA[0m

Box rectangle:  [32m(90.0, 392.5) -> (203.7, 402.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Sayan Mukherjee[0m

Box rectangle:  [32m(280.3, 428.1) -> (331.7, 440.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 444.8) -> (502.1, 683.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe analyze the convergence properties of Fermat distances, a family of density-driven
                metrics defined on Riemannian manifolds with an associated probability measure. Fermat
                distances may be defined either on discrete samples from the underlying measure, in which
                case they are random, or in the continuum setting, where they are induced by geodesics
                under a density-distorted Riemannian metric. We prove that discrete, sample-based Fermat
                distances converge to their continuum analogues in small neighborhoods with a precise rate
                that depends on the intrinsic dimensionality of the data and the parameter governing the
                extent of density weighting in Fermat distances. This is done by leveraging novel geometric
                and statistical arguments in percolation theory that allow for non-uniform densities and
                curved domains. Our results are then used to prove that discrete graph Laplacians based on
                discrete, sample-driven Fermat distances converge to corresponding continuum operators.
                In particular, we show the discrete eigenvalues and eigenvectors converge to their continuum
                analogues at a dimension-dependent rate, which allows us to interpret the efficacy of discrete
                spectral clustering using Fermat distances in terms of the resulting continuum limit. The
                perspective afforded by our discrete-to-continuum Fermat distance analysis leads to new
                clustering algorithms for data and related insights into efficient computations associated
                to density-driven spectral clustering. Our theoretical analysis is supported with numerical
                simulations and experiments on synthetic and real image data.
                Keywords:
                clustering, data-driven metrics, discrete-to-continuum limits, Fermat dis-
                tances, spectral graph theory[0m

Box rectangle:  [32m(90.0, 726.5) -> (387.1, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Nicolas Garc ́ıa Trillos, Anna Little, Daniel Mckenzie, James M. Murphy.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0939.html.[0m



=== Processing ../JMLR 2024/FineMorphs  Affine-Diffeomorphic Sequences for Regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/FineMorphs  Affine-Diffeomorphic Sequences for Regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 6/23; Revised 8/24; Published 8/24[0m

Box rectangle:  [32m(93.0, 101.6) -> (519.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFineMorphs: Affine-Diffeomorphic Sequences for Regression[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 215.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichele Lohr
                mbierba2@jhu.edu
                Laurent Younes
                laurent.younes@jhu.edu
                Department of Applied Mathematics and Statistics
                Center for Imaging Science
                The Johns Hopkins University
                Baltimore, MD 21218-2683, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (246.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Michele Lohr and Laurent Younes.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0824.html.[0m



=== Processing ../JMLR 2024/Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Finite-time Analysis of Globally Nonstationary Multi-Armed Bandits.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 8/21; Revised 10/23; Published 4/24[0m

Box rectangle:  [32m(176.3, 101.6) -> (435.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFinite-time Analysis of Globally
                Nonstationary Multi-Armed Bandits[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJunpei Komiyama
                junpei@komiyama.info
                Stern School of Business, New York University, USA[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.1, 217.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEdouard Fouch ́e
                edouard.fouche@gmail.com
                Institute for Program Structures and Data Organization, Karlsruhe Institute of Technology, Ger-
                many[0m

Box rectangle:  [32m(90.0, 224.8) -> (522.0, 264.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJunya Honda
                honda@i.kyoto-u.ac.jp
                Department of Systems Science, Graduate School of Informatics, Kyoto University, Japan
                Center for Advanced Intelligence Project, RIKEN, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (324.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Junpei Komiyama, Edouard Fouch ́e, and Junya Honda.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0916.html.[0m



=== Processing ../JMLR 2024/Fisher information dissipation for time-inhomogeneous stochastic differential equations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fisher information dissipation for time-inhomogeneous stochastic differential equations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 2/24; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(112.7, 107.5) -> (499.5, 139.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFisher information dissipation for time-inhomogeneous
                stochastic differential equations[0m

Box rectangle:  [32m(90.0, 163.8) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQi Feng
                qfeng2@fsu.edu
                Department of Mathematics
                Florida State University
                Tallahassee, FL 32306, USA[0m

Box rectangle:  [32m(90.0, 223.4) -> (522.0, 271.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXinzhe Zuo
                zxz@math.ucla.edu
                Department of Mathematics
                University of California, Los Angeles
                Los Angeles, CA 90095, USA[0m

Box rectangle:  [32m(90.0, 284.5) -> (522.0, 337.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWuchen Li
                wuchen@mailbox.sc.edu
                Department of Mathematics
                University of South Carolina
                Columbia, SC 29208, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (242.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Qi Feng, Xinzhe Zuo, Wuchen Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0206.html.[0m



=== Processing ../JMLR 2024/Fixed points of nonnegative neural networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fixed points of nonnegative neural networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 40.5) -> (522.0, 50.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 2/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(152.4, 99.6) -> (459.6, 117.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFixed points of nonnegative neural networks[0m

Box rectangle:  [32m(90.0, 133.7) -> (522.0, 182.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTomasz J. Piotrowski
                tpiotrowski@umk.pl
                Faculty of Physics, Astronomy and Informatics,
                Nicolaus Copernicus University,
                Grudziądzka 5/7, 87-100 Toruń, Poland[0m

Box rectangle:  [32m(90.0, 187.3) -> (522.0, 224.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRenato L. G. Cavalcante
                renato.cavalcante@hhi.fraunhofer.de
                Fraunhofer Heinrich Hertz Institute,
                Einsteinufer 37, 10587 Berlin, Germany[0m

Box rectangle:  [32m(90.0, 230.5) -> (522.0, 284.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMateusz Gabor
                mateusz.gabor@pwr.edu.pl
                Faculty of Electronics, Photonics, and Microsystems,
                Wrocław University of Science and Technology,
                Wybrzeze Wyspianskiego 27, 50-370 Wrocław, Poland[0m

Box rectangle:  [32m(90.0, 307.1) -> (211.5, 319.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Aryeh Kontorovich[0m

Box rectangle:  [32m(280.3, 342.5) -> (331.7, 357.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 364.1) -> (502.1, 520.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe use fixed point theory to analyze nonnegative neural networks, which we define as
                neural networks that map nonnegative vectors to nonnegative vectors. We first show that
                nonnegative neural networks with nonnegative weights and biases can be recognized as
                monotonic and (weakly) scalable mappings within the framework of nonlinear Perron-
                Frobenius theory.
                This fact enables us to provide conditions for the existence of fixed
                points of nonnegative neural networks having inputs and outputs of the same dimension,
                and these conditions are weaker than those recently obtained using arguments in convex
                analysis. Furthermore, we prove that the shape of the fixed point set of nonnegative neural
                networks with nonnegative weights and biases is an interval, which under mild conditions
                degenerates to a point. These results are then used to obtain the existence of fixed points
                of more general nonnegative neural networks. From a practical perspective, our results
                contribute to the understanding of the behavior of autoencoders, and we also offer valuable
                mathematical machinery for future developments in deep equilibrium models.[0m

Box rectangle:  [32m(109.9, 525.4) -> (502.1, 549.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Nonnegative neural networks, nonlinear Perron-Frobenius theory, monotonic
                and (weakly) scalable mappings, fixed point analysis, convergence analysis.[0m

Box rectangle:  [32m(90.0, 571.5) -> (180.3, 586.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 597.7) -> (522.0, 706.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural networks consist of multiple layers that are able to extract patterns of input signals
                for decision-making without any human intervention. This fact has profound consequences
                in a wide range of wireless communications, image recognition, and speech processing tasks,
                where neural networks set high standards of performance LeCun et al. (2015); Goodfellow
                et al. (2016); Samek et al. (2021). Neural networks have also been recently introduced as
                efficient iterative regularization methods in inverse problems, which opens a vista of new
                applications, especially in medical imaging Adler and Öktem (2017); Benning and Burger
                (2018); Ongie et al. (2020).[0m

Box rectangle:  [32m(90.0, 725.3) -> (360.2, 735.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Tomasz J. Piotrowski, Renato L. G. Cavalcante, Mateusz Gabor.[0m

Box rectangle:  [32m(90.0, 739.7) -> (517.0, 759.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0167.html.[0m



=== Processing ../JMLR 2024/Flexible Bayesian Product Mixture Models for Vector Autoregressions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Flexible Bayesian Product Mixture Models for Vector Autoregressions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 6/22; Revised 11/23; Published 4/24[0m

Box rectangle:  [32m(115.0, 101.6) -> (497.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFlexible Bayesian Product Mixture Models for Vector
                Autoregressions[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSuprateek Kundu
                skundu2@mdanderson.org
                Department of Biostatistics, The University of Texas MD Anderson Cancer Center
                University of Texas
                Houston, TX 77030, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoshua Lukemire
                joshua.lukemire@emory.edu
                Department of Biostatistics and Bioinformatics
                Emory University
                Atlanta, GA 30322, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (267.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Suprateek Kundu and Joshua Lukemire.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0717.html.[0m



=== Processing ../JMLR 2024/Fortuna  A Library for Uncertainty Quantification in Deep Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fortuna  A Library for Uncertainty Quantification in Deep Learning.pdf') ---[0m

Box rectangle:  [32m(97.3, 101.6) -> (514.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFortuna: A Library for Uncertainty Quantification in Deep
                Learning[0m

Box rectangle:  [32m(90.0, 151.6) -> (518.2, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGianluca Detommaso1
                detomma@amazon.de[0m

Box rectangle:  [32m(90.0, 169.4) -> (518.2, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlberto Gasparin2
                albgas@amazon.de[0m

Box rectangle:  [32m(90.0, 187.1) -> (518.2, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichele Donini1
                donini@amazon.de[0m

Box rectangle:  [32m(90.0, 204.8) -> (518.2, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthias Seeger1
                matthis@amazon.de[0m

Box rectangle:  [32m(90.0, 222.5) -> (518.2, 235.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrew Gordon Wilson3
                wilsmman@amazon.com[0m

Box rectangle:  [32m(90.0, 240.2) -> (518.2, 252.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCedric Archambeau1
                cedrica@amazon.de[0m

Box rectangle:  [32m(90.0, 260.6) -> (222.1, 299.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1AWS, Berlin, Germany
                2Amazon, Berlin, Germany
                3AWS & New York University[0m

Box rectangle:  [32m(280.3, 324.6) -> (331.7, 336.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 344.0) -> (502.2, 425.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe present Fortuna, an open-source library for uncertainty quantification in deep learning.
                Fortuna supports a range of calibration techniques, such as conformal prediction that can
                be applied to any trained neural network to generate reliable uncertainty estimates, and
                scalable Bayesian inference methods that can be applied to deep neural networks trained
                from scratch for improved uncertainty quantification and accuracy. By providing a coherent
                framework for advanced uncertainty quantification methods, Fortuna simplifies the process
                of benchmarking and helps practitioners build robust AI systems.[0m

Box rectangle:  [32m(90.0, 448.0) -> (180.3, 460.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 472.3) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVirtually every application of machine learning ultimately involves decision making under
                uncertainty. Predictive uncertainty lets us evaluate the trustworthiness of model predictions,
                prompts human intervention, or determines whether a model can be safely deployed in the
                real-world. Proper uncertainty estimation is crucial for ensuring the reliability and safety
                of machine learning applications.
                Unfortunately, deep neural networks are often overconfident. In classification, overcon-
                fidence means that the estimated probability of the predicted class is significantly higher
                than the actual proportion of correctly classified input data points (Guo et al., 2017). Over-
                confidence is problematic because it impacts decisions and ensuing actions. For example, a
                doctor may have requested an additional test if she were to know that a diagnosis made by
                an AI was less confident, and a self-driving car may have asked a human driver to take over
                if it was unsure about the existence of an obstacle in front of the car. Hence, calibrated
                uncertainty estimates are vital for assessing the reliability of machine learning systems,
                triggering human intervention, or judging whether a model can be safely deployed.
                There are many techniques for estimating and calibrating uncertainty estimates, includ-
                ing temperature scaling (Guo et al., 2017), conformal prediction (Vovk et al., 2005) and
                Bayesian inference (Gelman et al., 2013). While there are existing open-source implementa-[0m

Box rectangle:  [32m(90.0, 726.3) -> (344.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Detommaso, Gasparin, Donini, Seeger, Wilson, Archambeau.[0m

Box rectangle:  [32m(90.0, 741.0) -> (371.6, 749.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.[0m



=== Processing ../JMLR 2024/Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-30
                Submitted 5/23; Revised 4/24; Published 1/25[0m

Box rectangle:  [32m(130.5, 101.5) -> (481.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFourier Neural Operators for Arbitrary Resolution
                Climate Data Downscaling[0m

Box rectangle:  [32m(89.4, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQidong Yang
                qy707@nyu.edu
                Mila Quebec AI Institute, Montreal, Canada
                New York University, New York, USA[0m

Box rectangle:  [32m(88.3, 194.9) -> (282.4, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlex Hernandez-Garcia
                Mila Quebec AI Institute, Montreal, Canada
                University of Montreal, Montreal, Canada[0m

Box rectangle:  [32m(89.4, 236.5) -> (287.2, 271.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPaula Harder
                Fraunhofer ITWM, Kaiserslautern, Germany
                Mila Quebec AI Institute, Montreal, Canada[0m

Box rectangle:  [32m(88.3, 278.1) -> (282.4, 312.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVenkatesh Ramesh
                Mila Quebec AI Institute, Montreal, Canada
                University of Montreal, Montreal, Canada[0m

Box rectangle:  [32m(89.5, 319.7) -> (227.1, 342.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPrasanna Sattigeri
                IBM Research, New York, USA[0m

Box rectangle:  [32m(89.5, 349.4) -> (196.5, 372.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniela Szwarcman
                IBM Research, Brazil[0m

Box rectangle:  [32m(89.5, 379.1) -> (227.1, 401.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCampbell D. Watson
                IBM Research, New York, USA[0m

Box rectangle:  [32m(89.4, 410.4) -> (282.4, 448.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Rolnick
                Mila Quebec AI Institute, Montreal, Canada
                McGill University, Montreal, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (494.9, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Qidong Yang, Alex Hernandez-Garcia, Paula Harder, Venkatesh Ramesh, Prasanna Sattigeri, Daniela
                Szwarcman, Campbell D. Watson, David Rolnick.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0597.html.[0m



=== Processing ../JMLR 2024/Fréchet Random Forests for Metric Space Valued Regression with Non Euclidean Predictors.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Fréchet Random Forests for Metric Space Valued Regression with Non Euclidean Predictors.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 10/20; Revised 2/24; Published 12/24[0m

Box rectangle:  [32m(90.9, 101.6) -> (521.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFr ́echet Random Forests for Metric Space Valued Regression
                with Non Euclidean Predictors[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLouis Capitaine
                louis.capitaine@epoch-intelligence.fr
                epoch intelligence & Univ. Bordeaux, INSERM, INRIA, BPH, U1219, F-33000 Bordeaux, France[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJ ́er ́emie Bigot
                jeremie.bigot@math.u-bordeaux.fr
                Univ. Bordeaux, CNRS, Bordeaux INP, IMB, UMR 5251, F-33400 Talence, France[0m

Box rectangle:  [32m(90.0, 211.2) -> (522.0, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRodolphe Thi ́ebaut
                rodolphe.thiebaut@u-bordeaux.fr
                Univ. Bordeaux, INSERM, INRIA, BPH, U1219, F-33000 Bordeaux, France[0m

Box rectangle:  [32m(90.0, 242.5) -> (522.0, 268.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobin Genuer
                robin.genuer@u-bordeaux.fr
                Univ. Bordeaux, INSERM, INRIA, BPH, U1219, F-33000 Bordeaux, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (369.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Louis Capitaine, J ́er ́emie Bigot, Rodolphe Thi ́ebaut, Robin Genuer.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/20-1173.html.[0m



=== Processing ../JMLR 2024/From continuous-time formulations to discretization schemes  tensor trains and robust regression for BSDEs and parabolic PDEs.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/From continuous-time formulations to discretization schemes  tensor trains and robust regression for BSDEs and parabolic PDEs.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 7/23; Revised 3/24; Published 6/24[0m

Box rectangle:  [32m(98.5, 101.6) -> (513.7, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrom continuous-time formulations to
                discretization schemes: tensor trains and robust regression
                for BSDEs and parabolic PDEs[0m

Box rectangle:  [32m(90.0, 169.8) -> (522.0, 205.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLorenz Richter
                richter@zib.de
                Zuse Institute Berlin, Germany
                dida Datenschmiede GmbH, Germany[0m

Box rectangle:  [32m(90.0, 211.4) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLeon Sallandt
                leon.sallandt@gmail.com
                Technische Universit ̈at Berlin, Germany[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 268.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNikolas N ̈usken
                nikolas.nusken@kcl.ac.uk
                King’s College London, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (308.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lorenz Richter, Leon Sallandt and Nikolas N ̈usken.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0982.html.[0m



=== Processing ../JMLR 2024/From Small Scales to Large Scales  Distance-to-Measure Density based Geometric Analysis of Complex Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/From Small Scales to Large Scales  Distance-to-Measure Density based Geometric Analysis of Complex Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 11/22; Published 7/24[0m

Box rectangle:  [32m(106.6, 101.6) -> (505.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrom Small Scales to Large Scales: Distance-to-Measure
                Density based Geometric Analysis of Complex Data[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKatharina Proksch∗
                k.proksch@utwente.nl
                Faculty of Electrical Engineering, Mathematics & Computer Science
                University of Twente
                Hallenweg 19, 7522NH Enschede, Netherlands[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristoph A. Weitkamp∗
                cweitka@mathematik.uni-goettingen.de
                Institute for Mathematical Stochastics
                University of G ̈ottingen
                Goldschmidtstraße 7, 37077 G ̈ottingen, Germany[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThomas Staudt
                thomas.staudt@uni-goettingen.de
                Institute for Mathematical Stochastics
                University of G ̈ottingen
                Goldschmidtstraße 7, 37077 G ̈ottingen, Germany[0m

Box rectangle:  [32m(90.0, 312.6) -> (522.0, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBenoˆıt Lelandais
                benoit.lelandais@pasteur.fr
                Institut Pasteur
                Imaging and Modeling Unit
                UMR 3691, CNRS, Paris, France[0m

Box rectangle:  [32m(90.0, 367.8) -> (522.0, 420.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristophe Zimmer
                czimmer@pasteur.fr
                Institut Pasteur
                Imaging and Modeling Unit
                UMR 3691, CNRS, Paris, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (519.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Katharina Proksch, Christoph Alexander Weikamp, Thomas Staudt, Benoˆıt Lelandais, Christophe Zimmer .[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1344.html.[0m



=== Processing ../JMLR 2024/Functional Directed Acyclic Graphs.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Functional Directed Acyclic Graphs.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 9/22; Revised 11/23; Published 3/24[0m

Box rectangle:  [32m(179.3, 101.6) -> (432.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFunctional Directed Acyclic Graphs[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKuang-Yao Lee
                kuang-yao.lee@temple.edu
                Department of Statistics, Operations, and Data Science
                Temple University
                Philadelphia, PA 19122, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLexin Li
                lexinli@berkeley.edu
                Department of Biostatistics and Epidemiology
                University of California at Berkeley
                Berkeley, CA 94720, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBing Li
                bxl9@psu.edu
                Department of Statistics
                Pennsylvannia State University
                University Park, PA 16802, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (263.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kuang-Yao Lee, Lexin Li, and Bing Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1038.html.[0m



=== Processing ../JMLR 2024/Functional optimal transport  regularized map estimation and domain adaptation for functional data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Functional optimal transport  regularized map estimation and domain adaptation for functional data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 3/22; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(90.3, 101.5) -> (521.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFunctional optimal transport: regularized map estimation and
                domain adaptation for functional data[0m

Box rectangle:  [32m(89.4, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiacheng Zhu∗
                jzhu.zjc@gmail.com
                Department of Mechanical Engineering, Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(89.4, 193.1) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAritra Guha∗
                ag997x@att.com
                Data Science & AI Research, AT&T Chief Data Office
                Bedminster, NJ 07921, USA[0m

Box rectangle:  [32m(89.3, 234.7) -> (522.0, 271.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDat Do∗
                dodat@umich.edu
                Department of Statistics, University of Michigan
                Ann Arbor, MI 48105, USA[0m

Box rectangle:  [32m(89.4, 276.7) -> (522.0, 312.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMengdi Xu
                mengdixu@andrew.cmu.edu
                Department of Mechanical Engineering, Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(89.3, 318.3) -> (522.0, 354.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuanLong Nguyen
                xuanlong@umich.edu
                Department of Statistics, University of Michigan
                Ann Arbor, MI 48105, USA[0m

Box rectangle:  [32m(89.4, 361.5) -> (522.0, 400.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDing Zhao
                dingzhao@cmu.edu
                Department of Mechanical Engineering, Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (434.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiacheng Zhu, Aritra Guha, Dat Do, Mengdi Xu, XuanLong Nguyen, and Ding Zhao.[0m

Box rectangle:  [32m(90.0, 740.9) -> (506.0, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0217.html.[0m



=== Processing ../JMLR 2024/Functions with average smoothness  structure  algorithms  and learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Functions with average smoothness  structure  algorithms  and learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 2/23; Revised 3/24; Published 4/24[0m

Box rectangle:  [32m(179.3, 101.6) -> (432.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFunctions with average smoothness:
                structure, algorithms, and learning[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYair Ashlagi
                ashlagi@post.bgu.ac.il
                Computer Science Department
                Ben-Gurion University
                Beer Sheva, Israel[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLee-Ad Gottlieb
                leead@ariel.ac.il
                Computer Science Department
                Ariel University
                Shomron, Israel[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAryeh Kontorovich
                karyeh@cs.bgu.ac.il
                Computer Science Department
                Ben-Gurion University
                Beer Sheva, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (307.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yair Ashlagi, Lee-Ad Gottlieb, Aryeh Kontorovich.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0182.html.[0m



=== Processing ../JMLR 2024/Gaussian Interpolation Flows.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Gaussian Interpolation Flows.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 11/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(202.5, 101.6) -> (409.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGaussian Interpolation Flows[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuan Gao
                yuan0.gao@connect.polyu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 199.5) -> (522.0, 247.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJian Huang
                j.huang@polyu.edu.hk
                Departments of Data Science and AI, and Applied Mathematics
                The Hong Kong Polytechnic University
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 266.6) -> (522.0, 319.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuling Jiao
                yulingjiaomath@whu.edu.cn
                School of Mathematics and Statistics
                and Hubei Key Laboratory of Computational Science
                Wuhan University, Wuhan, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (269.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuan Gao, Jian Huang, and Yuling Jiao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1515.html.[0m



=== Processing ../JMLR 2024/Gaussian Mixture Models with Rare Events.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Gaussian Mixture Models with Rare Events.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 9/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(150.3, 101.6) -> (461.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGaussian Mixture Models with Rare Events[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuetong Li
                2001110929@stu.pku.edu.cn
                Guanghua School of Management
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 187.2) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJing Zhou ∗
                jing.zhou@ruc.edu.cn
                Center for Applied Statistics, School of Statistics
                Renmin University of China
                Beijing, China[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHansheng Wang
                hansheng@gsm.pku.edu.cn
                Guanghua School of Management
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (286.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xuetong Li, Jing Zhou, and Hansheng Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1245.html.[0m



=== Processing ../JMLR 2024/Generalization and Stability of Interpolating Neural Networks with Minimal Width.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Generalization and Stability of Interpolating Neural Networks with Minimal Width.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 4/23; Revised 2/24; Published 4/24[0m

Box rectangle:  [32m(121.8, 101.6) -> (490.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeneralization and Stability of Interpolating Neural
                Networks with Minimal Width[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHossein Taheri
                hossein@ucsb.edu
                Department of Electrical and Computer Engineering
                University of California
                Santa Barbara, CA, USA[0m

Box rectangle:  [32m(88.1, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristos Thrampoulidis
                cthrampo@ece.ubc.ca
                Department of Electrical and Computer Engineering
                University of British Columbia
                Vancouver, Canada[0m

Box rectangle:  [32m(90.0, 285.0) -> (215.9, 295.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Pradeep Ravikumar[0m

Box rectangle:  [32m(280.3, 320.5) -> (331.7, 332.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 339.3) -> (503.5, 540.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe investigate the generalization and optimization properties of shallow neural-network
                classifiers trained by gradient descent in the interpolating regime. Specifically, in a realizable
                scenario where model weights can achieve arbitrarily small training error ε and their distance
                from initialization is g(ε), we demonstrate that gradient descent with n training data achieves
                training error O(g(1/T)2/T) and generalization error O(g(1/T)2/n) at iteration T, provided
                there are at least m = Ω(g(1/T)4) hidden neurons. We then show that our realizable
                setting encompasses a special case where data are separable by the model’s neural tangent
                kernel. For this and logistic-loss minimization, we prove the training loss decays at a rate
                of  ̃O(1/T) given polylogarithmic number of neurons m = Ω(log4(T)). Moreover, with
                m = Ω(log4(n)) neurons and T ≈n iterations, we bound the test loss by  ̃O(1/n). Our
                results differ from existing generalization outcomes using the algorithmic-stability framework,
                which necessitate polynomial width and yield suboptimal generalization rates. Central to
                our analysis is the use of a new self-bounded weak-convexity property, which leads to a
                generalized local quasi-convexity property for sufficiently parameterized neural-network
                classifiers. Eventually, despite the objective’s non-convexity, this leads to convergence and
                generalization-gap bounds that resemble those found in the convex setting of linear logistic
                regression.[0m

Box rectangle:  [32m(109.9, 545.8) -> (503.5, 567.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: Generalization Error, Neural Networks, Optimization, Over-parameterization,
                Interpolation.[0m

Box rectangle:  [32m(90.0, 589.4) -> (180.3, 601.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(87.3, 613.0) -> (523.3, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural networks have remarkable expressive capabilities and can memorize a complete
                dataset even with mild overparameterization. In practice, using gradient descent (GD)
                on neural networks with logistic or cross-entropy loss can result in the objective reaching
                zero training error and close to zero training loss. Zero training error, often referred to as
                “interpolating” the data, indicates perfect classification of the dataset. Despite their strong
                memorization ability, these networks also exhibit remarkable generalization capabilities to
                new data. This has motivated a surge of studies in recent years exploring the optimization[0m

Box rectangle:  [32m(89.1, 726.4) -> (282.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Hossein Taheri and Christos Thrampoulidis.[0m

Box rectangle:  [32m(90.0, 740.7) -> (506.3, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/23-0422.html.[0m



=== Processing ../JMLR 2024/Generalization on the Unseen  Logic Reasoning and Degree Curriculum.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Generalization on the Unseen  Logic Reasoning and Degree Curriculum.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-58
                Submitted 2/24; Published 11/24[0m

Box rectangle:  [32m(95.8, 101.6) -> (516.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeneralization on the Unseen, Logic Reasoning and Degree
                Curriculum[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmmanuel Abbe
                emmanuel.abbe@epfl.ch
                EPFL, Apple,
                Lausanne, VD, Switzerland[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSamy Bengio
                bengio@apple.com
                Apple,
                Cupertino, CA, USA[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAryo Lotfi
                aryo.lotfi@epfl.ch
                EPFL,
                Lausanne, VD, Switzerland[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKevin Rizk
                kevin.rizk@epfl.ch
                EPFL,
                Lausanne, VD, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (342.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0220.html.[0m



=== Processing ../JMLR 2024/Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-61
                Submitted 8/23; Revised 4/24; Published 6/24[0m

Box rectangle:  [32m(104.2, 101.6) -> (507.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeneralized Independent Noise Condition for Estimating
                Causal Structure with Latent Variables[0m

Box rectangle:  [32m(89.4, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFeng Xie∗
                fengxie@btbu.edu.cn
                Department of Applied Statistics, Beijing Technology and Business University
                Beijing, 102488, China[0m

Box rectangle:  [32m(89.5, 193.2) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBiwei Huang∗
                bih007@ucsd.edu
                Halicioglu Data Science Institute (HDSI), University of California San Diego
                La Jolla, San Diego, California, 92093, USA[0m

Box rectangle:  [32m(88.8, 235.1) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhengming Chen
                chenzhengming1103@gmail.com
                School of Computer Science, Guangdong University of Technology
                Guangzhou, 510006, China
                Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence
                Abu Dhabi, UAE[0m

Box rectangle:  [32m(88.8, 300.6) -> (522.0, 336.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuichu Cai
                cairuichu@gmail.com
                School of Computer Science, Guangdong University of Technology
                Guangzhou, 510006, China[0m

Box rectangle:  [32m(89.4, 342.3) -> (522.0, 378.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mClark Glymour
                cg09@andrew.cmu.edu
                Department of Philosophy, Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(89.4, 383.9) -> (522.0, 419.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhi Geng
                zhigeng@btbu.edu.cn
                Department of Applied Statistics, Beijing Technology and Business University
                Beijing, 102488, China[0m

Box rectangle:  [32m(89.3, 426.8) -> (522.0, 493.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKun Zhang †
                kunz1@cmu.edu
                Department of Philosophy, Carnegie Mellon University
                Pittsburgh, PA 15213, USA
                Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence
                Abu Dhabi, UAE[0m

Box rectangle:  [32m(90.0, 726.3) -> (485.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, and Kun Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1052.html.[0m



=== Processing ../JMLR 2024/Generative Adversarial Ranking Nets.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Generative Adversarial Ranking Nets.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 4/23; Revised 3/24; Published 4/24[0m

Box rectangle:  [32m(173.3, 101.6) -> (438.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGenerative Adversarial Ranking Nets[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYinghua Yao∗
                eva.yh.yao@gmail.com
                Center for Frontier AI Research, A*STAR, Singapore
                and
                Institute of High Performance Computing, A*STAR, Singapore[0m

Box rectangle:  [32m(90.0, 199.1) -> (522.0, 247.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuangang Pan†
                yuangang.pan@gmail.com
                Center for Frontier AI Research, A*STAR, Singapore
                and
                Institute of High Performance Computing, A*STAR, Singapore[0m

Box rectangle:  [32m(90.0, 265.0) -> (522.0, 313.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJing Li
                j.lee9383@gmail.com
                Center for Frontier AI Research, A*STAR, Singapore
                and
                Institute of High Performance Computing, A*STAR, Singapore[0m

Box rectangle:  [32m(90.0, 330.5) -> (522.0, 378.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIvor W. Tsang
                ivor.tsang@gmail.com
                Center for Frontier AI Research, A*STAR, Singapore
                and
                Institute of High Performance Computing, A*STAR, Singapore[0m

Box rectangle:  [32m(90.0, 397.7) -> (522.0, 423.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXin Yao
                xinyao@ln.edu.hk
                Department of Computing and Decision Sciences, Lingnan University, Hong Kong[0m

Box rectangle:  [32m(90.0, 726.3) -> (367.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yinghua Yao, Yuangang Pan, Jing Li, Ivor W. Tsang and Xin Yao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0461.html.[0m



=== Processing ../JMLR 2024/Geometric Learning with Positively Decomposable Kernels.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Geometric Learning with Positively Decomposable Kernels.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 10/23; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(97.4, 101.6) -> (514.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeometric Learning with Positively Decomposable Kernels[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNatha ̈el Da Costa
                nathael.dacosta@gmail.com
                Cyrus Mostajeran
                cyrus.mostajeran@gmail.com
                Juan-Pablo Ortega
                juan-pablo.ortega@ntu.edu.sg
                Division of Mathematical Sciences
                School of Physical and Mathematical Sciences
                Nanyang Technological University
                21 Nanyang Link, 637371, Singapore[0m

Box rectangle:  [32m(90.0, 225.0) -> (522.0, 277.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSalem Said
                salem.said@univ-grenoble-alpes.fr
                Laboratoire Jean Kuntzman
                Universit ́e Grenoble-Alpes
                Grenoble, 38400, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (396.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Natha ̈el Da Costa, Cyrus Mostajeran, Juan-Pablo Ortega, and Salem Said.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1400.html.[0m



=== Processing ../JMLR 2024/GGD  Grafting Gradient Descent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/GGD  Grafting Gradient Descent.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-87
                Submitted 10/22; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(187.8, 101.6) -> (424.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGGD: Grafting Gradient Descent[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYanjing Feng
                yjfeng@mail.nankai.edu.cn
                NITFID, School of Statistics and Data science
                Nankai University
                Tianjin 300071, China[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYongdao Zhou
                ydzhou@nankai.edu.cn
                NITFID, School of Statistics and Data science
                Nankai University
                Tianjin 300071, China[0m

Box rectangle:  [32m(90.0, 267.1) -> (187.2, 277.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Moritz Hardt[0m

Box rectangle:  [32m(280.3, 300.7) -> (331.7, 312.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 318.7) -> (502.1, 522.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSimple random sampling has been widely used in traditional stochastic optimization algo-
                rithms. Although the gradient sampled by simple random sampling is a descent direction in
                expectation, it may have a relatively high variance which will cause the descent curve wig-
                gling and slow down the optimization process. In this paper, we propose a novel stochastic
                optimization method called grafting gradient descent (GGD), which combines the strength
                from minibatching and importance sampling, and provide the convergence results of GGD.
                We show that the grafting gradient possesses a doubly robust property which ensures
                that the performance of GGD method is superior to the worse one of SGD with impor-
                tance sampling method and mini-batch SGD method. Combined with advanced variance
                reduction techniques such as stochastic variance reduced gradient and adaptive stepsize
                methods such as Adam, these composite GGD-based methods and their theoretical bounds
                are provided. The real data studies also show that GGD achieves an intermediate per-
                formance among SGD with importance sampling and mini-batch SGD, and outperforms
                original SGD method. Then the proposed GGD is a better and more robust stochastic
                optimization framework in practice.
                Keywords:
                stochastic optimization, importance sampling, minibatching, variance reduc-
                tion, adaptive stepsize method[0m

Box rectangle:  [32m(90.0, 543.1) -> (176.6, 555.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 565.9) -> (522.1, 590.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOne fundamental problem studied in machine learning is how to fit the model to large data
                set. The most popular approach is via the empirical risk minimization (ERM), that is,[0m

Box rectangle:  [32m(318.4, 604.5) -> (334.2, 624.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mn
                X[0m

Box rectangle:  [32m(263.7, 608.3) -> (314.8, 626.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m
                f(x) = 1[0m

Box rectangle:  [32m(319.4, 608.3) -> (370.6, 639.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mi=1
                fi(x)
                
                ,[0m

Box rectangle:  [32m(241.4, 615.8) -> (261.4, 634.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmin
                x∈Rd[0m

Box rectangle:  [32m(308.8, 623.3) -> (315.4, 634.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mn[0m

Box rectangle:  [32m(90.0, 653.7) -> (522.0, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mwhere x is the d parameters in a pre-defined model, and fi(x) is the loss function of the
                sample i, i = 1, 2, ..., n, such as the square error loss or hinge error loss. Optimizing the
                objective function f is of paramount importance. The most well-known method is via the
                stochastic gradient descent, whose update rule for the model parameters x can be written[0m

Box rectangle:  [32m(90.0, 726.4) -> (243.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yanjing Feng and Yongdao Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1236.html.[0m



=== Processing ../JMLR 2024/Goal-Space Planning with Subgoal Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Goal-Space Planning with Subgoal Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 1/24; Revised 9/24; Published 10/24[0m

Box rectangle:  [32m(155.6, 101.6) -> (456.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGoal-Space Planning with Subgoal Models[0m

Box rectangle:  [32m(90.0, 133.6) -> (518.2, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChunlok Lo∗
                chunlok@ualberta.ca[0m

Box rectangle:  [32m(90.0, 151.3) -> (518.2, 164.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKevin Roice∗
                roice@ualberta.ca[0m

Box rectangle:  [32m(90.0, 169.0) -> (518.2, 181.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mParham Mohammad Panahi∗
                parham1@ualberta.ca[0m

Box rectangle:  [32m(90.0, 187.1) -> (518.2, 199.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScott M. Jordan
                sjordan@ualberta.ca[0m

Box rectangle:  [32m(89.5, 204.5) -> (518.2, 217.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdam White†
                amw8@ualberta.ca[0m

Box rectangle:  [32m(90.0, 222.5) -> (518.2, 234.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mG ́abor Mihucz
                mihucz@ualberta.ca[0m

Box rectangle:  [32m(90.0, 240.2) -> (518.2, 252.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFarzane Aminmansour
                aminmans@ualberta.ca[0m

Box rectangle:  [32m(89.3, 259.2) -> (522.0, 325.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartha White†
                whitem@ualberta.ca
                †Canada CIFAR AI Chair
                Alberta Machine Intelligence Institute (Amii)
                Department of Computing Science, University of Alberta
                Edmonton, Alberta, Canada[0m

Box rectangle:  [32m(90.0, 726.4) -> (376.4, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lo, Roice, Panahi, Jordan, White, Mihucz, Aminmansour and White.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0040.html.[0m



=== Processing ../JMLR 2024/Gradient-free optimization of highly smooth functions  improved analysis and a new algorithm.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Gradient-free optimization of highly smooth functions  improved analysis and a new algorithm.pdf') ---[0m

Box rectangle:  [32m(73.4, 46.1) -> (501.8, 54.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 6/23; Published 11/24[0m

Box rectangle:  [32m(88.4, 105.9) -> (486.9, 138.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGradient-free optimization of highly smooth functions: improved
                analysis and a new algorithm[0m

Box rectangle:  [32m(73.0, 157.6) -> (501.6, 192.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArya Akhavan
                ARIA.AKHAVANFOOMANI@IIT.IT
                CSML, Istituto Italiano di Tecnologia
                CMAP,  ́Ecole Polytechnique, IP Paris[0m

Box rectangle:  [32m(73.1, 199.2) -> (501.6, 221.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEvgenii Chzhen
                EVGENII.CHZHEN@CNRS.FR
                CNRS, LMO, Universit ́e Paris-Saclay[0m

Box rectangle:  [32m(73.1, 228.8) -> (501.6, 263.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMassimiliano Pontil
                MASSIMILIANO.PONTIL@IIT.IT
                CSML, Istituto Italiano di Tecnologia
                University College London[0m

Box rectangle:  [32m(73.0, 272.1) -> (501.6, 296.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexandre B. Tsybakov
                ALEXANDRE.TSYBAKOV@ENSAE.FR
                CREST, ENSAE, IP Paris[0m

Box rectangle:  [32m(73.4, 321.5) -> (235.6, 331.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Krishnakumar Balasubramanian[0m

Box rectangle:  [32m(265.4, 355.3) -> (309.9, 367.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(92.9, 392.8) -> (483.7, 606.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThis work studies minimization problems with zero-order noisy oracle information under the
                assumption that the objective function is highly smooth and possibly satisfies additional properties.
                We consider two kinds of zero-order projected gradient descent algorithms, which differ in the form
                of the gradient estimator. The first algorithm uses a gradient estimator based on randomization over
                the l2 sphere due to Bach and Perchet (2016). We present an improved analysis of this algorithm on
                the class of highly smooth and strongly convex functions studied in the prior work, and we derive
                rates of convergence for two more general classes of non-convex functions. Namely, we consider
                highly smooth functions satisfying the Polyak-Łojasiewicz condition and the class of highly smooth
                functions with no additional property. The second algorithm is based on randomization over the
                l1 sphere, and it extends to the highly smooth setting the algorithm that was recently proposed
                for Lipschitz convex functions in Akhavan et al. (2022). We show that, in the case of noiseless
                oracle, this novel algorithm enjoys better bounds on bias and variance than the l2 randomization
                and the commonly used Gaussian randomization algorithms, while in the noisy case both l1 and l2
                algorithms benefit from similar improved theoretical guarantees. The improvements are achieved
                thanks to a new proof techniques based on Poincar ́e type inequalities for uniform distributions on
                the l1 or l2 spheres. The results are established under weak (almost adversarial) assumptions on
                the noise. Moreover, we provide minimax lower bounds proving optimality or near optimality of
                the obtained upper bounds in several cases.[0m

Box rectangle:  [32m(93.4, 629.8) -> (483.6, 651.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: smooth optimization, zero-order oracle, gradient-free optimization, stochastic zero-
                order algorithms, minimax optimality, Polyak-Łojasiewicz condition[0m

Box rectangle:  [32m(72.8, 673.3) -> (361.3, 681.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Arya Akhavan, Evgenii Chzhen, Massimiliano Pontil, and Alexandre B. Tsybakov.[0m

Box rectangle:  [32m(73.4, 687.3) -> (498.8, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0733.html.[0m



=== Processing ../JMLR 2024/Gradual Domain Adaptation  Theory and Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Gradual Domain Adaptation  Theory and Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 9/23; Revised 9/24; Published 11/24[0m

Box rectangle:  [32m(115.5, 101.6) -> (496.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGradual Domain Adaptation: Theory and Algorithms[0m

Box rectangle:  [32m(88.3, 133.6) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYifei He∗
                yifeihe3@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(88.3, 163.3) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHaoxiang Wang∗
                hwang264@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(88.3, 193.3) -> (522.0, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBo Li
                bol@uchicago.edu
                University of Chicago[0m

Box rectangle:  [32m(88.3, 224.5) -> (522.0, 250.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHan Zhao
                hanzhao@illinois.edu
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 275.4) -> (200.6, 285.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Quentin Berthet[0m

Box rectangle:  [32m(280.3, 310.9) -> (331.7, 322.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.6, 328.0) -> (504.0, 590.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnsupervised domain adaptation (UDA) adapts a model from a labeled source domain
                to an unlabeled target domain in a one-offway. Though widely applied, UDA faces a
                great challenge whenever the distribution shift between the source and the target is large.
                Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains
                to gradually adapt from the source to the target domain. In this work, we first theoretically
                analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved
                generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads
                to an interesting insight: to minimize the generalization error on the target domain, the
                sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic
                between the source and target domains. The insight is particularly useful under the situation
                where intermediate domains are missing or scarce, which is often the case in real-world
                applications. Based on the insight, we propose Generative Gradual DOmain Adaptation
                with Optimal Transport (GOAT), an algorithmic framework that can generate intermediate
                domains in a data-dependent way. More concretely, we first generate intermediate domains
                along the Wasserstein geodesic between two given consecutive domains in a feature space,
                then apply gradual self-training to adapt the source-trained classifier to the target along the
                sequence of intermediate domains. Empirically, we demonstrate that our GOAT framework
                can improve the performance of standard GDA when the given intermediate domains are
                scarce, significantly broadening the real-world application scenarios of GDA. Our code is
                available at https://github.com/uiuctml/GOAT.
                Keywords:
                Gradual Domain Adaptation, Distribution Shift, Optimal Transport, Out-of-
                distribution Generalization[0m

Box rectangle:  [32m(90.0, 610.7) -> (180.3, 622.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.6, 632.6) -> (524.2, 684.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mModern machine learning models suffer from data distribution shifts across various settings
                and datasets (Gulrajani and Lopez-Paz, 2021; Sagawa et al., 2021; Koh et al., 2021; Hendrycks
                et al., 2021; Wiles et al., 2022), i.e., trained models may face a significant performance drop
                when the test data come from a distribution largely shifted from the training data distribution.[0m

Box rectangle:  [32m(105.3, 694.3) -> (188.1, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Equal contribution.[0m

Box rectangle:  [32m(89.1, 726.4) -> (296.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yifei He, Haoxiang Wang, Bo Li and Han Zhao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1180.html.[0m



=== Processing ../JMLR 2024/Granger Causal Inference in Multivariate Hawkes Processes  by Minimum Message Length.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Granger Causal Inference in Multivariate Hawkes Processes  by Minimum Message Length.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-26
                Submitted 8/23; Revised 4/24; Published 4/24[0m

Box rectangle:  [32m(94.1, 101.6) -> (518.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGranger Causal Inference in Multivariate Hawkes Processes
                by Minimum Message Length[0m

Box rectangle:  [32m(90.0, 151.9) -> (521.9, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKateˇrina Hlav ́aˇckov ́a-Schindler
                katerina.schindlerova@univie.ac.at
                Faculty of Computer Science, University of Vienna
                Vienna, Austria, and Institute of Computer Science
                Czech Academy of Sciences, Prague, Czechia[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Melnykova
                anna.melnykova@univ-avignon.fr
                Laboratory of Mathematics
                University of Avignon, Avignon, France[0m

Box rectangle:  [32m(90.0, 248.7) -> (522.0, 287.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIrene Tubikanec
                irene.tubikanec@aau.at
                Department of Statistics
                University of Klagenfurt, Klagenfurt, Austria[0m

Box rectangle:  [32m(90.0, 726.3) -> (300.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hlav ́aˇckov ́a-Schindler, Melnykova and Tubikanec.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1066.html.[0m



=== Processing ../JMLR 2024/Graphical Dirichlet Process for Clustering Non-Exchangeable Grouped Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Graphical Dirichlet Process for Clustering Non-Exchangeable Grouped Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 8/23; Revised 9/24; Published 10/24[0m

Box rectangle:  [32m(156.7, 101.6) -> (455.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGraphical Dirichlet Process for Clustering
                Non-Exchangeable Grouped Data[0m

Box rectangle:  [32m(90.0, 151.9) -> (521.9, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArhit Chakrabarti
                arhit.chakrabarti@stat.tamu.edu
                Department of Statistics
                Texas A&M University
                College Station, TX 77843-3143, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYang Ni
                yni@stat.tamu.edu
                Department of Statistics
                CPRIT Single Cell Data Science Core
                Texas A&M University
                College Station, TX 77843-3143, USA[0m

Box rectangle:  [32m(90.0, 271.0) -> (522.0, 342.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEllen Ruth A. Morris
                ellenruth@tamu.edu
                Department of Nutrition
                Program in Integrative Nutrition & Complex Diseases
                Current address: Texas A&M Veterinary Medical Diagnostic Laboratory
                Texas A&M University
                College Station, TX 77843-4471, USA[0m

Box rectangle:  [32m(90.0, 348.5) -> (522.0, 420.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael L. Salinas
                mlsalinas4@tamu.edu
                Department of Nutrition
                Program in Integrative Nutrition & Complex Diseases
                CPRIT Single Cell Data Science Core
                Texas A&M University
                College Station, TX 77843-2253, USA[0m

Box rectangle:  [32m(90.0, 426.0) -> (522.0, 497.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobert S. Chapkin
                robert.chapkin@ag.tamu.edu
                Department of Nutrition
                Program in Integrative Nutrition & Complex Diseases
                CPRIT Single Cell Data Science Core
                Texas A&M University
                College Station, TX 77843-2253, USA[0m

Box rectangle:  [32m(90.0, 505.0) -> (522.0, 557.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBani K. Mallick
                bmallick@stat.tamu.edu
                Department of Statistics
                Texas A&M University
                College Station, TX 77843-3143, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (504.5, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Arhit Chakrabarti, Yang Ni, Ellen Ruth A. Morris, Michael L. Salinas, Robert S. Chapkin, and Bani K.
                Mallick.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1048.html.[0m



=== Processing ../JMLR 2024/Grokking phase transitions in learning local rules with gradient descent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Grokking phase transitions in learning local rules with gradient descent.pdf') ---[0m

Box rectangle:  [32m(90.0, 36.0) -> (521.9, 44.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 10/22; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(113.2, 94.7) -> (499.0, 126.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGrokking phase transitions in learning local rules with
                gradient descent[0m

Box rectangle:  [32m(90.0, 157.1) -> (522.0, 206.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBojan ˇZunkoviˇc
                bojan.zunkovic@fri.uni-lj.si
                University of Ljubljana
                Faculty of Computer and Information Science
                1000 Ljubljana, Slovenia[0m

Box rectangle:  [32m(90.0, 225.4) -> (522.0, 278.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEnej Ilievski
                enej.ilievski@fmf.uni-lj.si
                University of Ljubljana
                Faculty of Mathematics and Physics
                1000 Ljubljana, Slovenia[0m

Box rectangle:  [32m(90.0, 705.9) -> (244.1, 715.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bojan ˇZunkoviˇc and Enej Ilievski.[0m

Box rectangle:  [32m(90.0, 722.4) -> (517.0, 739.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1228.html.[0m



=== Processing ../JMLR 2024/Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 1/24; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(159.2, 101.7) -> (452.8, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuaranteed Nonconvex Factorization Approach
                for Tensor Train Recovery[0m

Box rectangle:  [32m(90.0, 153.4) -> (521.8, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhen Qin
                QIN.660@OSU.EDU
                Department of Computer Science and Engineering
                Ohio State University
                Columbus, Ohio 43201, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (521.8, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael B. Wakin
                MWAKIN@MINES.EDU
                Department of Electrical Engineering
                Colorado School of Mines
                Golden, Colorado 80401, USA[0m

Box rectangle:  [32m(90.0, 262.1) -> (521.8, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhihui Zhu
                ZHU.3440@OSU.EDU
                Department of Computer Science and Engineering
                Ohio State University
                Columbus, Ohio 43201, USA[0m

Box rectangle:  [32m(90.0, 338.7) -> (169.7, 348.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Shiqian Ma[0m

Box rectangle:  [32m(283.8, 374.4) -> (328.2, 386.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 399.1) -> (502.1, 672.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTensor train (TT) decomposition represents an order-N tensor using O(N) order-3 tensors (i.e.,
                factors of small dimension), achieved through products among these factors. Due to its compact
                representation, TT decomposition has been widely used in the fields of signal processing, machine
                learning, and quantum physics. It offers benefits such as reduced memory requirements, enhanced
                computational efficiency, and decreased sampling complexity. Nevertheless, existing optimization
                algorithms with guaranteed performance concentrate exclusively on using the TT format for re-
                ducing the optimization space in recovery problems, while still operating on the entire tensor in
                each iteration. There is a lack of comprehensive theoretical analysis for optimization involving the
                factors directly, despite the proven efficacy of such factorization methods in practice. In this paper,
                we provide the first convergence guarantee for the factorization approach in a TT-based recovery
                problem. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we op-
                timize over the so-called left-orthogonal TT format which enforces orthonormality among most
                of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent
                (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factoriza-
                tion/decomposition problem and establish the local linear convergence of RGD. Notably, the rate
                of convergence only experiences a linear decline as the tensor order increases. We then study the
                sensing problem that aims to recover a TT format tensor from linear measurements. Assuming
                the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper
                initialization, which could be obtained through spectral initialization, RGD also converges to the
                ground-truth tensor at a linear rate. Furthermore, we expand our analysis to encompass scenarios
                involving Gaussian noise in the measurements. We prove that RGD can reliably recover the ground
                truth at a linear rate, with the recovery error exhibiting only polynomial growth in relation to the
                tensor order N. We conduct various experiments to validate our theoretical findings.[0m

Box rectangle:  [32m(109.9, 683.3) -> (502.1, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Tensor-train decomposition, factorization approach, Riemannian gradient descent,
                linear convergence[0m

Box rectangle:  [32m(90.0, 726.7) -> (261.9, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Zhen Qin, Michael B. Wakin, and Zhihui Zhu.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-0029.html.[0m



=== Processing ../JMLR 2024/Hamiltonian Monte Carlo for efficient Gaussian sampling  long and random steps.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Hamiltonian Monte Carlo for efficient Gaussian sampling  long and random steps.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-30
                Submitted 11/23; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(101.6, 101.6) -> (510.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHamiltonian Monte Carlo for efficient Gaussian sampling:
                long and random steps[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSimon Apers
                apers@irif.fr
                IRIF
                Universit ́e Paris Cit ́e, CNRS
                Paris, F-75013, France[0m

Box rectangle:  [32m(90.0, 205.4) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSander Gribling
                s.j.gribling@tilburguniversity.edu
                Department of Econometrics and Operations Research
                Tilburg University
                Tilburg, 5000 LE, the Netherlands[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mD ́aniel Szil ́agyi
                szilagyi.d@gmail.com
                IRIF
                Universit ́e Paris Cit ́e
                Paris, F-75013, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (306.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Simon Apers, Sander Gribling and D ́aniel Szil ́agyi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1521.html.[0m



=== Processing ../JMLR 2024/Heterogeneity-aware Clustered Distributed Learning for Multi-source Data Analysis.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Heterogeneity-aware Clustered Distributed Learning for Multi-source Data Analysis.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-60
                Submitted 1/23; Revised 2/24; Published 6/24[0m

Box rectangle:  [32m(107.5, 101.6) -> (504.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHeterogeneity-aware Clustered Distributed Learning for
                Multi-source Data Analysis[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanxing Chen
                yxchen research@163.com
                Department of Statistics and Data Science, School of Economics
                Xiamen University
                Xiamen, 361005, China[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQingzhao Zhang
                qzzhang@xmu.edu.cn
                Department of Statistics and Data Science, School of Economics
                The Wang Yanan Institute for Studies in Economics
                Xiamen University
                Xiamen, 361005, China[0m

Box rectangle:  [32m(90.0, 271.0) -> (522.0, 319.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShuangge Ma
                shuangge.ma@yale.edu
                Department of Biostatistics
                Yale University
                New Haven, CT 06520, USA[0m

Box rectangle:  [32m(90.0, 325.8) -> (522.0, 378.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKuangnan Fang∗
                xmufkn@163.com
                Department of Statistics and Data Science, School of Economics
                Xiamen University
                Xiamen, 361005, China[0m

Box rectangle:  [32m(90.0, 417.7) -> (185.7, 427.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Dan Alistarh[0m

Box rectangle:  [32m(280.3, 451.3) -> (331.7, 463.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 471.3) -> (502.2, 672.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn diverse fields ranging from finance to omics, it is increasingly common that data is
                distributed with multiple individual sources (referred to as “clients” in some studies). In-
                tegrating raw data, although powerful, is often not feasible, for example, when there are
                considerations on privacy protection. Distributed learning techniques have been developed
                to integrate summary statistics as opposed to raw data.
                In many existing distributed
                learning studies, it is stringently assumed that all the clients have the same model. To
                accommodate data heterogeneity, some federated learning methods allow for client-specific
                models. In this article, we consider the scenario that clients form clusters, those in the same
                cluster have the same model, and different clusters have different models. Further consid-
                ering the clustering structure can lead to a better understanding of the “interconnections”
                among clients and reduce the number of parameters. To this end, we develop a novel pe-
                nalization approach. Specifically, group penalization is imposed for regularized estimation
                and selection of important variables, and fusion penalization is imposed to automatically
                cluster clients. An effective ADMM algorithm is developed, and the estimation, selection,
                and clustering consistency properties are established under mild conditions. Simulation
                and data analysis further demonstrate the practical utility and superiority of the proposed
                approach.[0m

Box rectangle:  [32m(93.7, 695.7) -> (285.7, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Kuangnan Fang is the corresponding author.[0m

Box rectangle:  [32m(90.0, 726.4) -> (380.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yuanxing Chen, Qingzhao Zhang, Shuangge Ma, and Kuangnan Fang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0059.html.[0m



=== Processing ../JMLR 2024/Heterogeneous-Agent Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Heterogeneous-Agent Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 4/23; Revised 10/23; Published 1/24[0m

Box rectangle:  [32m(146.5, 101.5) -> (465.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHeterogeneous-Agent Reinforcement Learning[0m

Box rectangle:  [32m(90.0, 135.1) -> (522.0, 304.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYifan Zhong1,2,∗
                zhongyifan@stu.pku.edu.cn
                Jakub Grudzien Kuba3,∗
                jakub.grudzien@new.ox.ac.uk
                Xidong Feng4,∗
                xidong.feng.20@ucl.ac.uk
                Siyi Hu5
                siyi.hu@student.uts.edu.au
                Jiaming Ji1
                jiamg.ji@stu.pku.edu.cn
                Yaodong Yang1,†
                yaodong.yang@pku.edu.cn
                1 Institute for Artificial Intelligence, Peking University
                2 Beijing Institute for General Artificial Intelligence
                3 University of Oxford
                4 University College London
                5 ReLER, AAII, University of Technology Sydney
                ∗Equal contribution
                † Corresponding author[0m

Box rectangle:  [32m(90.0, 726.3) -> (458.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and Yaodong Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0488.html.[0m



=== Processing ../JMLR 2024/High Probability and Risk-Averse Guarantees for a Stochastic Accelerated Primal-Dual Method.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/High Probability and Risk-Averse Guarantees for a Stochastic Accelerated Primal-Dual Method.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 7/23; Revised 2/24; Published 5/24[0m

Box rectangle:  [32m(134.2, 101.5) -> (477.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHigh Probability and Risk-Averse Guarantees
                for a Stochastic Accelerated Primal-Dual Method[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYassine Laguel
                yassine.laguel@univ-cotedazur.fr
                Laboratoire Jean Alexandre Dieudonné
                Université Côte d’Azur
                Nice, France.[0m

Box rectangle:  [32m(88.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNecdet Serhat Aybat
                nsa10@psu.edu
                Department of Industrial and Manufacturing Engineering
                Pennsylvania State University
                University Park, PA, USA[0m

Box rectangle:  [32m(89.4, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMert Gürbüzbalaban
                mgurbuzbalaban@business.rutgers.edu
                Department of Management Science and Information Systems
                Rutgers Business School, Rutgers University.
                Piscataway, NJ, USA.[0m

Box rectangle:  [32m(90.0, 338.5) -> (185.3, 348.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Martin Jaggi[0m

Box rectangle:  [32m(280.3, 374.1) -> (331.7, 386.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.3, 390.9) -> (503.4, 593.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe consider stochastic strongly-convex-strongly-concave (SCSC) saddle point (SP) problems
                which frequently arise in applications ranging from distributionally robust learning to game
                theory and fairness in machine learning. We focus on the recently developed stochastic
                accelerated primal-dual algorithm (SAPD), which admits optimal complexity in several
                settings as an accelerated algorithm. We provide high probability guarantees for convergence
                to a neighborhood of the saddle point that reflects accelerated convergence behavior. We
                also provide an analytical formula for the limiting covariance matrix of the iterates for
                a class of stochastic SCSC quadratic problems where the gradient noise is additive and
                Gaussian. This allows us to develop lower bounds for this class of quadratic problems which
                show that our analysis is tight in terms of the high probability bound dependence on the
                problem parameters. We also provide a risk-averse convergence analysis characterizing the
                “Conditional Value at Risk”, the “Entropic Value at Risk”, and the χ2-divergence of the
                distance to the saddle point for the iterate sequence, highlighting the trade-offs between
                the bias and the risk associated with an approximate solution obtained by terminating the
                algorithm at any iteration.
                Keywords:
                stochastic min-max optimization, high-probability guarantees, risk measures,
                accelerated primal-dual methods[0m

Box rectangle:  [32m(90.0, 613.0) -> (180.2, 625.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.4, 634.3) -> (515.4, 645.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe consider strongly convex/strongly concave (SCSC) saddle point problems of the form:[0m

Box rectangle:  [32m(209.3, 654.7) -> (523.3, 673.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmin
                xPX max
                yPY Lpx, yq fi fpxq ` Φpx, yq  ́ gpyq,
                (1.1)[0m

Box rectangle:  [32m(89.6, 678.4) -> (523.5, 706.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mwhere X and Y are finite-dimensional Euclidean spaces, f : X Ñ RYt`8u and g :
                YÑRYt`8u are closed convex functions, and Φ : X ˆ Y Ñ R is a smooth convex-concave[0m

Box rectangle:  [32m(89.1, 726.4) -> (369.4, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yassine Laguel and Necdet Serhat Aybat and Mert Gürbüzbalaban.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0864.html.[0m



=== Processing ../JMLR 2024/High Probability Convergence Bounds for Non-convex Stochastic Gradient Descent with Sub-Weibull Noise.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/High Probability Convergence Bounds for Non-convex Stochastic Gradient Descent with Sub-Weibull Noise.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 4/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(113.9, 101.6) -> (498.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHigh Probability Convergence Bounds for Non-convex
                Stochastic Gradient Descent with Sub-Weibull Noise[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLiam Madden
                liam@ece.ubc.ca
                Department of Electrical and Computer Engineering
                University of British Columbia[0m

Box rectangle:  [32m(89.4, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmiliano Dall’Anese
                edallane@bu.edu
                Department of Electrical and Computer Engineering
                Boston University[0m

Box rectangle:  [32m(88.3, 248.7) -> (522.0, 287.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStephen Becker
                stephen.becker@colorado.edu
                Department of Applied Mathematics
                University of Colorado Boulder[0m

Box rectangle:  [32m(90.0, 726.3) -> (331.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Liam Madden, Emiliano Dall’Anese, and Stephen Becker.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0466.html.[0m



=== Processing ../JMLR 2024/Homeomorphic Projection to Ensure Neural-Network Solution Feasibility for Constrained Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Homeomorphic Projection to Ensure Neural-Network Solution Feasibility for Constrained Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 11/23; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(116.4, 101.6) -> (495.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHomeomorphic Projection to Ensure Neural-Network
                Solution Feasibility for Constrained Optimization[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEnming Liang
                eliang4-c@my.cityu.edu.hk
                Department of Data Science
                City University of Hong Kong
                Hong Kong, China[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMinghua Chen∗
                minghua.chen@cityu.edu.hk
                Department of Data Science
                City University of Hong Kong
                Hong Kong, China[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSteven H. Low
                slow@caltech.edu
                Departments of EE and CMS
                California Institute of Technology
                California, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (308.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Enming Liang, Minghua Chen, and Steven H. Low.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1577.html.[0m



=== Processing ../JMLR 2024/How Two-Layer Neural Networks Learn  One (Giant) Step at a Time.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/How Two-Layer Neural Networks Learn  One (Giant) Step at a Time.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 11/23; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(162.1, 101.6) -> (450.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHow Two-Layer Neural Networks Learn,
                One (Giant) Step at a Time[0m

Box rectangle:  [32m(90.0, 153.7) -> (518.2, 166.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYatin Dandi∗§
                yatin.dandi@epfl.ch[0m

Box rectangle:  [32m(90.0, 171.4) -> (518.2, 184.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFlorent Krzakala∗
                florent.krzakala@epfl.ch[0m

Box rectangle:  [32m(90.0, 189.1) -> (518.2, 201.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBruno Loureiro†
                bruno.loureiro@di.ens.fr[0m

Box rectangle:  [32m(90.0, 206.8) -> (518.2, 219.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLuca Pesce∗
                luca.pesce@epfl.ch[0m

Box rectangle:  [32m(90.0, 224.5) -> (518.2, 237.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLudovic Stephan‡
                ludovic.stephan@ensai.fr[0m

Box rectangle:  [32m(90.0, 242.2) -> (352.0, 278.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Information, Learning and Physics (IdePHICS) Laboratory
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne
                Route Cantonale, 1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 283.9) -> (291.8, 320.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m†D ́epartement d’Informatique
                 ́Ecole Normale Sup ́erieure - PSL & CNRS
                45 rue d’Ulm, F-75230 Paris cedex 05, France[0m

Box rectangle:  [32m(90.0, 325.5) -> (255.3, 349.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m‡Univ Rennes, Ensai, CNRS, CREST
                UMR 9194 F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 356.7) -> (335.5, 396.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m§Statistical Physics Of Computation (SPOC) Laboratory
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne
                Route Cantonale, 1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 421.5) -> (222.8, 431.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Mahdi Soltanolkotabi[0m

Box rectangle:  [32m(280.3, 455.1) -> (331.7, 467.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 479.9) -> (502.1, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFor high-dimensional Gaussian data, we investigate theoretically how the features of a
                two-layer neural network adapt to the structure of the target function through a few large
                batch gradient descent steps, leading to an improvement in the approximation capacity
                with respect to the initialization. First, we compare the influence of batch size to that of
                multiple (but finitely many) steps. For a single gradient step, a batch of size n = O(d)
                is both necessary and sufficient to align with the target function, although only a single
                direction can be learned. In contrast, n = O(d2) is essential for neurons to specialize in
                multiple relevant directions of the target with a single gradient step. Even in this case, we
                show there might exist “hard” directions requiring n = O(dl) samples to be learned, where
                lis known as the leap index of the target. Second, we show that the picture drastically
                improves over multiple gradient steps: a batch size of n = O(d) is indeed sufficient to learn
                multiple target directions satisfying a staircase property, where more and more directions
                can be learned over time.
                Finally, we discuss how these directions allow for a drastic
                improvement in the approximation capacity and generalization error over the initialization,
                illustrating a separation of scale between the random features/lazy regime and the feature
                learning regime. Our technical analysis leverages a combination of techniques related to
                concentration, projection-based conditioning, and Gaussian equivalence, which we believe
                are of independent interest. By pinning down the conditions necessary for specialization
                and learning, our results highlight the intertwined role of the structure of the task to[0m

Box rectangle:  [32m(90.0, 726.4) -> (426.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1543.html.[0m



=== Processing ../JMLR 2024/Identifiability and Asymptotics in Learning Homogeneous Linear ODE Systems from Discrete Observations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Identifiability and Asymptotics in Learning Homogeneous Linear ODE Systems from Discrete Observations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 10/22; Revised 5/24; Published 5/24[0m

Box rectangle:  [32m(101.6, 101.6) -> (510.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIdentifiability and Asymptotics in Learning Homogeneous
                Linear ODE Systems from Discrete Observations[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 223.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanyuan Wang
                yuanyuanw2@student.unimelb.edu.au
                Wei Huang
                wei.huang@unimelb.edu.au
                Mingming Gong ∗
                mingming.gong@unimelb.edu.au
                Xi Geng
                xi.geng@unimelb.edu.au
                School of Mathematics and Statistics
                University of Melbourne, Melbourne, Australia[0m

Box rectangle:  [32m(88.4, 229.4) -> (521.9, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTongliang Liu
                tongliang.liu@sydney.edu.au
                School of Computer Science, Faculty of Engineering
                The University of Sydney, Sydney, Australia[0m

Box rectangle:  [32m(88.8, 271.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKun Zhang
                kunz1@cmu.edu
                Carnegie Mellon University, Pittsburgh, PA, USA
                Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE[0m

Box rectangle:  [32m(88.4, 314.2) -> (522.0, 353.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDacheng Tao
                dacheng.tao@sydney.edu.au
                School of Computer Science, Faculty of Engineering
                The University of Sydney, Sydney, Australia[0m

Box rectangle:  [32m(90.0, 726.3) -> (486.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuanyuan Wang, Wei Huang, Mingming Gong, Xi Geng, Tongliang Liu, Kun Zhang, Dacheng Tao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1159.html.[0m



=== Processing ../JMLR 2024/Identifying Causal Eﬀects using Instrumental Time Series  Nuisance IV and Correcting for the Past.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Identifying Causal Eﬀects using Instrumental Time Series  Nuisance IV and Correcting for the Past.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.5) -> (522.1, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 3/22; Revised 7/24; Published 9/24[0m

Box rectangle:  [32m(99.3, 99.6) -> (512.8, 131.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIdentifying Causal Effects using Instrumental Time Series:
                Nuisance IV and Correcting for the Past[0m

Box rectangle:  [32m(90.0, 147.9) -> (518.6, 159.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNikolaj Thams
                thams@math.ku.dk[0m

Box rectangle:  [32m(90.0, 164.6) -> (518.6, 175.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRikke Søndergaard
                rsn@math.ku.dk[0m

Box rectangle:  [32m(90.0, 181.3) -> (518.6, 192.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSebastian Weichwald
                sweichwald@math.ku.dk[0m

Box rectangle:  [32m(90.0, 198.0) -> (518.6, 209.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonas Peters
                jonas.peters@math.ku.dk[0m

Box rectangle:  [32m(90.0, 217.7) -> (241.8, 250.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDepartment of Mathematical Sciences
                University of Copenhagen
                Denmark[0m

Box rectangle:  [32m(90.0, 275.2) -> (190.6, 284.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Vanessa Didelez[0m

Box rectangle:  [32m(280.3, 306.4) -> (331.7, 318.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 322.7) -> (502.2, 508.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInstrumental variable (IV) regression relies on instruments to infer causal effects from observational
                data with unobserved confounding. We consider IV regression in time series models, such as vector
                auto-regressive (VAR) processes. Direct applications of i.i.d. techniques are generally inconsistent
                as they do not correctly adjust for dependencies in the past. In this paper, we outline the difficulties
                that arise due to time structure and propose methodology for constructing identifying equations
                that can be used for consistent parametric estimation of causal effects in time series data. One
                method uses extra nuisance covariates to obtain identifiability (an idea that can be of interest
                even in the i.i.d. case). We further propose a graph marginalization framework that allows us to
                apply nuisance IV and other IV methods in a principled way to time series. Our methods make
                use of a version of the global Markov property, which we prove holds for VAR(p) processes. For
                VAR(1) processes, we prove identifiability conditions that relate to Jordan forms and are different
                from the well-known rank conditions in the i.i.d. case (they do not require as many instruments as
                covariates, for example). We provide methods, prove their consistency, and show how the inferred
                causal effect can be used for distribution generalization. Simulation experiments corroborate our
                theoretical results. We provide ready-to-use Python code.
                Keywords:
                causality, time series, instrumental variables, VAR processes, Markov property,
                distribution generalization[0m

Box rectangle:  [32m(90.0, 528.0) -> (180.3, 540.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 548.7) -> (522.1, 702.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPredicting a response variable Y from observations of covariates X may be insufficient to answer
                a scientific question at hand. Instead, we may wish to model how the response variable Y reacts
                to an intervention on X. Such modeling requires causal knowledge. For example, for i.i.d. data
                from a linear model Y := βX + g(H, εY ), it is well-known that an ordinary least squares (OLS)
                regression of Y on X generally yields a biased estimator of the linear causal effect β from X on Y
                when an unobserved variable H confounds X and Y . Instead, we may obtain unbiased estimates of
                β by utilising instrumental variables (IVs) I that correlate with the covariates X, are independent of
                H, and affect Y only indirectly through X. IV regression, pioneered by Wright (1928) and Reiersøl
                (1945), is well-established in econometrics (Angrist et al., 1996; Staiger and Stock, 1997; Angrist and
                Krueger, 2001), statistics (Bowden and Turkington, 1985) and epidemiology (Hern ́an and Robins,
                2006; Didelez et al., 2010). One approach for IV estimation in the linear i.i.d. model is the two-stage
                least squares (TSLS) estimator (Angrist and Imbens, 1995), which first estimates the effect from I
                to X (stage 1) and then regresses Y on the fitted values from the first regression (stage 2). Another[0m

Box rectangle:  [32m(90.0, 727.1) -> (367.5, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc
                ⃝2024 Nikolaj Thams, Rikke Søndergaard, Sebastian Weichwald, Jonas Peters.[0m

Box rectangle:  [32m(90.0, 740.3) -> (489.2, 755.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0262.html.[0m



=== Processing ../JMLR 2024/Improved Random Features for Dot Product Kernels.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Improved Random Features for Dot Product Kernels.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-75
                Submitted 2/22; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(118.1, 101.6) -> (494.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImproved Random Features for Dot Product Kernels[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 163.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonas Wacker
                jonas.wacker@gmail.com
                Motonobu Kanagawa
                motonobu.kanagawa@eurecom.fr[0m

Box rectangle:  [32m(90.0, 169.1) -> (522.0, 194.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData Science Department, EURECOM, France
                Maurizio Filippone
                maurizio.filippone@kaust.edu.sa[0m

Box rectangle:  [32m(90.0, 200.5) -> (274.2, 210.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStatistics Program, KAUST, Saudi Arabia[0m

Box rectangle:  [32m(90.0, 235.6) -> (210.9, 245.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Jean-Philippe Vert[0m

Box rectangle:  [32m(280.3, 271.2) -> (331.7, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 289.0) -> (502.1, 539.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDot product kernels, such as polynomial and exponential (softmax) kernels, are among
                the most widely used kernels in machine learning, as they enable modeling the interac-
                tions between input features, which is crucial in applications like computer vision, natural
                language processing, and recommender systems. We make several novel contributions for
                improving the efficiency of random feature approximations for dot product kernels, to make
                these kernels more useful in large scale learning. First, we present a generalization of exist-
                ing random feature approximations for polynomial kernels, such as Rademacher and Gaus-
                sian sketches and TensorSRHT, using complex-valued random features. We show empiri-
                cally that the use of complex features can significantly reduce the variances of these approx-
                imations. Second, we provide a theoretical analysis for understanding the factors affecting
                the efficiency of various random feature approximations, by deriving closed-form expres-
                sions for their variances. These variance formulas elucidate conditions under which certain
                approximations (e.g., TensorSRHT) achieve lower variances than others (e.g., Rademacher
                sketches), and conditions under which the use of complex features leads to lower vari-
                ances than real features. Third, by using these variance formulas, which can be evaluated
                in practice, we develop a data-driven optimization approach to improve random feature
                approximations for general dot product kernels, which is also applicable to the Gaussian
                kernel. We describe the improvements brought by these contributions with extensive ex-
                periments on a variety of tasks and datasets.
                Keywords:
                Random features, randomized sketches, dot product kernels, polynomial
                kernels, large scale learning[0m

Box rectangle:  [32m(90.0, 560.1) -> (142.6, 572.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mContents[0m

Box rectangle:  [32m(90.0, 586.4) -> (522.0, 597.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1
                Introduction
                3[0m

Box rectangle:  [32m(90.0, 612.0) -> (522.1, 651.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2
                Preliminaries
                6
                2.1
                Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
                6
                2.2
                Positive Definite Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
                6[0m

Box rectangle:  [32m(90.0, 665.9) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m3
                Polynomial Sketches
                7
                3.1
                Real-valued Polynomial Sketches . . . . . . . . . . . . . . . . . . . . . . . .
                8
                3.2
                Complex-valued Polynomial Sketches . . . . . . . . . . . . . . . . . . . . . .
                9[0m

Box rectangle:  [32m(90.0, 726.4) -> (345.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Jonas Wacker, Motonobu Kanagawa and Maurizio Filippone.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0118.html.[0m



=== Processing ../JMLR 2024/Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Improving Lipschitz-Constrained Neural Networks by Learning Activation Functions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-30
                Submitted 11/22; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(116.1, 101.6) -> (496.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImproving Lipschitz-Constrained Neural Networks by
                Learning Activation Functions[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 274.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStanislas Ducotterd
                stanislas.ducotterd@epfl.ch
                Alexis Goujon
                alexis.goujon@epfl.ch
                Pakshal Bohra
                pakshal.bohra@epfl.ch
                Dimitris Perdios
                dimitris.perdios@epfl.ch
                Sebastian Neumayer
                sebastian.neumayer@epfl.ch
                Michael Unser
                michael.unser@epfl.ch
                Biomedical Imaging Group,
                 ́Ecole polytechnique f ́ed ́erale de Lausanne (EPFL),
                CH-1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (463.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Stanislas Ducotterd, Alexis Goujon, Pakshal Bohra, Sebastian Neumayer and Michael Unser.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1347.html.[0m



=== Processing ../JMLR 2024/Improving physics-informed neural networks with meta-learned optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Improving physics-informed neural networks with meta-learned optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-26
                Submitted 3/23; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(148.7, 101.6) -> (463.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImproving physics-informed neural networks
                with meta-learned optimization[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlex Bihlo
                abihlo@mun.ca
                Department of Mathematics and Statistics
                Memorial University of Newfoundland,
                St. John’s (NL) A1C 5S7, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (159.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alex Bihlo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0356.html.[0m



=== Processing ../JMLR 2024/Individual-centered Partial Information in Social Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Individual-centered Partial Information in Social Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-60
                Submitted 1/23; Revised 5/24; Published 6/24[0m

Box rectangle:  [32m(97.6, 101.6) -> (514.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIndividual-centered Partial Information in Social Networks[0m

Box rectangle:  [32m(90.0, 133.9) -> (526.1, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiao Han
                xhan011@ustc.edu.cn ∗[0m

Box rectangle:  [32m(90.0, 148.1) -> (341.6, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInternational Institute of Finance, School of Management
                University of Science and Technology of China
                Hefei, 230026, China[0m

Box rectangle:  [32m(90.0, 187.5) -> (521.9, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mY. X. Rachel Wang
                rachel.wang@sydney.edu.au
                School of Mathematics and Statistics
                University of Sydney
                NSW, 2006, Australia[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQing Yang
                yangq@ustc.edu.cn
                International Institute of Finance, School of Management
                University of Science and Technology of China
                Hefei, 230026, China[0m

Box rectangle:  [32m(90.0, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXin Tong
                xint@marshall.usc.edu
                Department of Data Sciences and Operations, Marshall School of Business
                University of Southern California
                CA, 90089, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (317.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xiao Han, Y. X. Rachel Wang, Qing Yang, Xin Tong.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0005.html.[0m



=== Processing ../JMLR 2024/Infeasible Deterministic  Stochastic  and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Infeasible Deterministic  Stochastic  and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 4/23; Revised 10/24; Published 12/24[0m

Box rectangle:  [32m(91.5, 101.6) -> (520.7, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInfeasible Deterministic, Stochastic, and Variance-Reduction
                Algorithms for Optimization under Orthogonality
                Constraints[0m

Box rectangle:  [32m(90.0, 169.8) -> (522.0, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPierre Ablin
                pierre.ablin@apple.com
                Apple[0m

Box rectangle:  [32m(90.0, 199.5) -> (522.0, 223.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSimon Vary
                simon.vary@stats.ox.ac.uk
                Department of Statistics, University of Oxford[0m

Box rectangle:  [32m(90.0, 229.1) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBin Gao
                gaobin@lsec.cc.ac.cn
                ICMSEC/LSEC, AMSS, Chinese Academy of Sciences[0m

Box rectangle:  [32m(90.0, 260.4) -> (522.0, 286.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mP.-A. Absil
                pa.absil@uclouvain.be
                ICTEAM/INMA, UCLouvain[0m

Box rectangle:  [32m(90.0, 726.3) -> (345.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Pierre Ablin, Simon Vary, Bin Gao and Pierre-Antoine Absil.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0451.html.[0m



=== Processing ../JMLR 2024/Inference on High-dimensional Single-index Models with Streaming Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Inference on High-dimensional Single-index Models with Streaming Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-68
                Submitted 10/22; Revised 6/24; Published 10/24[0m

Box rectangle:  [32m(106.0, 101.6) -> (506.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInference on High-dimensional Single-index Models with
                Streaming Data[0m

Box rectangle:  [32m(90.0, 151.5) -> (521.9, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDongxiao Han∗
                handongxiao@nankai.edu.cn
                School of Statistics and Data Science, KLMDASR, LEBPS, and LPMC
                Nankai University
                Tianjin, 300071,China[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 289.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJinhan Xie∗
                jinhanxie@163.com
                Yunnan Key Laboratory of Statistical Modeling and Data Analysis
                Yunnan University
                Kunming, 650091, China;
                Department of Mathematical and Statistical Sciences
                University of Alberta
                Edmonton, AB, T6G 2G1, Canada[0m

Box rectangle:  [32m(90.0, 294.6) -> (521.9, 342.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJin Liu†
                liujin@nankai.edu.cn
                School of Statistics and Data Science, KLMDASR, LEBPS, and LPMC
                Nankai University
                Tianjin, 300071,China[0m

Box rectangle:  [32m(90.0, 348.5) -> (522.0, 408.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLiuquan Sun
                slq@amt.ac.cn
                Academy of Mathematics and Systems Science, Chinese Academy of Sciences, and School of Math-
                ematical Sciences
                University of Chinese Academy of Sciences
                Beijing 100190, China[0m

Box rectangle:  [32m(90.0, 414.0) -> (522.0, 462.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJian Huang
                j.huang@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Hong Kong, China[0m

Box rectangle:  [32m(90.0, 467.6) -> (522.0, 515.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBei Jiang
                bei1@ualberta.ca
                Department of Mathematical and Statistical Sciences
                University of Alberta
                Edmonton, AB, T6G 2G1, Canada[0m

Box rectangle:  [32m(90.0, 522.8) -> (522.0, 575.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLinglong Kong
                lkong@ualberta.ca
                Department of Mathematical and Statistical Sciences
                University of Alberta
                Edmonton, AB, T6G 2G1, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (465.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Dongxiao Han, Jinhan Xie, Jin Liu, Liuquan Sun, Jian Huang, Bei Jiang and Linglong Kong.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1124.html.[0m



=== Processing ../JMLR 2024/Infinite-Dimensional Diffusion Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Infinite-Dimensional Diffusion Models.pdf') ---[0m

Box rectangle:  [32m(171.9, 101.6) -> (440.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInfinite-Dimensional Diffusion Models[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJakiw Pidstrigach
                jakiw.pidstrigach@stats.ox.ac.uk
                Institut f ̈ur Mathematik
                Universit ̈at Potsdam
                Karl-Liebknecht-Str. 24/25
                14476 Potsdam[0m

Box rectangle:  [32m(90.0, 199.5) -> (522.0, 259.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoussef Marzouk
                ymarz@mit.edu
                Statistics and Data Science Center
                Massachusetts Institute of Technology
                77 Massachusetts Ave
                Cambridge, MA 02139 USA[0m

Box rectangle:  [32m(90.0, 265.0) -> (522.0, 325.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSebastian Reich
                sereich@uni-potsdam.de
                Institut f ̈ur Mathematik
                Universit ̈at Potsdam
                Karl-Liebknecht-Str. 24/25
                14476 Potsdam[0m

Box rectangle:  [32m(90.0, 332.1) -> (522.0, 398.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSven Wang
                svenwang@mit.edu
                Statistics and Data Science Center
                Massachusetts Institute of Technology
                77 Massachusetts Ave
                Cambridge, MA 02139 USA[0m

Box rectangle:  [32m(280.3, 424.0) -> (331.7, 436.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 442.9) -> (502.2, 608.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDiffusion models have had a profound impact on many application areas, including those
                where data are intrinsically infinite-dimensional, such as images or time series. The stand-
                ard approach is first to discretize and then to apply diffusion models to the discretized
                data. While such approaches are practically appealing, the performance of the resulting
                algorithms typically deteriorates as discretization parameters are refined. In this paper,
                we instead directly formulate diffusion-based generative models in infinite dimensions and
                apply them to the generative modelling of functions.
                We prove that our formulations
                are well posed in the infinite-dimensional setting and provide dimension-independent dis-
                tance bounds from the sample to the target measure. Using our theory, we also develop
                guidelines for the design of infinite-dimensional diffusion models. For image distributions,
                these guidelines are in line with current canonical choices. For other distributions, however,
                we can improve upon these canonical choices. We demonstrate these results both theoret-
                ically and empirically, by applying the algorithms to data distributions on manifolds and
                to distributions arising in Bayesian inverse problems or simulation-based inference.[0m

Box rectangle:  [32m(109.9, 613.5) -> (502.1, 635.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: diffusion models, score-based generative models, infinite-dimensional analysis,
                hilbert spaces, bayesian inverse problems, function space[0m

Box rectangle:  [32m(90.0, 657.2) -> (180.3, 669.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 680.8) -> (522.0, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDiffusion models (also score-based generative models or SGMs) (Sohl-Dickstein et al., 2015;
                Song et al., 2021) have recently shown great empirical success across a variety of domains.[0m

Box rectangle:  [32m(90.0, 726.3) -> (365.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, Sven Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (371.6, 749.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.[0m



=== Processing ../JMLR 2024/Information Capacity Regret Bounds for Bandits with Mediator Feedback.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Information Capacity Regret Bounds for Bandits with Mediator Feedback.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 2/24; Published 10/24[0m

Box rectangle:  [32m(162.2, 101.6) -> (449.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInformation Capacity Regret Bounds for
                Bandits with Mediator Feedback[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKhaled Eldowa
                khaled.eldowa@unimi.it
                Universit`a degli Studi di Milano
                Milano, 20133, Italy[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicol`o Cesa-Bianchi
                nicolo.cesa-bianchi@unimi.it
                Universit`a degli Studi di Milano and Politecnico di Milano
                Milano, 20133, Italy[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlberto Maria Metelli
                albertomaria.metelli@polimi.it
                Politecnico di Milano
                Milano, 20133, Italy[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarcello Restelli
                marcello.restelli@polimi.it
                Politecnico di Milano
                Milano, 20133, Italy[0m

Box rectangle:  [32m(90.0, 726.3) -> (426.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Khaled Eldowa, Nicol`o Cesa-Bianchi, Alberto Maria Metelli, and Marcello Restelli.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0227.html.[0m



=== Processing ../JMLR 2024/Information Processing Equalities and the Information–Risk Bridge.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Information Processing Equalities and the Information–Risk Bridge.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 9/22; Revised 3/24; Published 3/24[0m

Box rectangle:  [32m(189.2, 101.5) -> (422.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInformation Processing Equalities
                and the Information–Risk Bridge[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobert C. Williamson
                Bob.Williamson@uni-tuebingen.de
                University of Tübingen and Tübingen AI Center,
                Germany[0m

Box rectangle:  [32m(89.5, 195.0) -> (522.0, 220.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZac Cranko
                Zac.Cranko@gmail.com
                Sydney, Australia[0m

Box rectangle:  [32m(90.0, 245.9) -> (211.5, 255.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Aryeh Kontorovich[0m

Box rectangle:  [32m(280.3, 281.4) -> (331.7, 293.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 298.3) -> (503.4, 440.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe introduce two new classes of measures of information for statistical experiments which
                generalise and subsume φ-divergences, integral probability metrics, N-distances (MMD),
                and (f, Γ) divergences between two or more distributions. This enables us to derive a simple
                geometrical relationship between measures of information and the Bayes risk of a statistical
                decision problem, thus extending the variational φ-divergence representation to multiple
                distributions in an entirely symmetric manner. The new families of divergence are closed
                under the action of Markov operators which yields an information processing equality which
                is a refinement and generalisation of the classical information processing inequality. This
                equality gives insight into the significance of the choice of the hypothesis class in classical
                risk minimization.
                Keywords:
                information processing inequality, φ-divergence, MMD, Bayes risk, loss
                functions, Markov kernels, regularisation via noise.[0m

Box rectangle:  [32m(90.0, 460.6) -> (180.2, 472.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(305.2, 471.6) -> (523.8, 513.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA key word in statistics is information. . . But what is
                information? No other concept in statistics is more
                elusive in its meaning and less amenable to a generally
                agreed definition. — Debabrata Basu (1975, p. 1).[0m

Box rectangle:  [32m(90.0, 525.5) -> (522.3, 699.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMachine learning is information processing. But what “information” is meant? Choosing
                exactly how to measure information has become topical of late in machine learning, with
                methods such as GANs predicated on the notion of being unable to compute a likelihood
                function, but being able to measure an information distance between a target and synthesised
                distribution (Bińkowski et al., 2018). Commonly used measures include the the Shannon
                information/entropy of a single distribution and the Kullback-Leibler divergence or Variational
                divergence between two different distributions. Csizár’s φ-entropies (Csiszár, 1967) and
                φ-divergences (Csiszár, 1963, 1967) subsume these and many other divergences, and satisfy
                the famous information processing inequality (Ziv and Zakai, 1973) which states that the
                amount of information can only decrease (or stay constant) as a result of “information
                processing.”
                The present paper presents a new and general definition of information that subsumes
                many in the literature. The key novelty of the paper is the redefinition of classical measures[0m

Box rectangle:  [32m(89.1, 726.4) -> (252.9, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Robert Williamson and Zac Cranko.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0988.html.[0m



=== Processing ../JMLR 2024/Information-Theoretic Generalization Bounds for Transductive Learning and its Applications.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Information-Theoretic Generalization Bounds for Transductive Learning and its Applications.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-69
                Submitted 10/23; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(132.2, 101.6) -> (480.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInformation-Theoretic Generalization Bounds for
                Transductive Learning and its Applications[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHuayi Tang
                huayitang@ruc.edu.cn
                Gaoling School of Artificial Intelligence
                Renmin University of China
                Beijing, 100872, China[0m

Box rectangle:  [32m(90.0, 206.7) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYong Liu∗
                liuyonggsai@ruc.edu.cn
                Gaoling School of Artificial Intelligence
                Renmin University of China
                Beijing, 100872, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (217.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Huayi Tang and Yong Liu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1368.html.[0m



=== Processing ../JMLR 2024/Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 8/22; Revised 9/24; Published 10/24[0m

Box rectangle:  [32m(107.8, 82.7) -> (504.3, 115.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInstrumental Variable Value Iteration for Causal Offline
                Reinforcement Learning[0m

Box rectangle:  [32m(70.8, 132.7) -> (540.0, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLuofeng Liao∗
                ll3530@columbia.edu
                Department of Industrial Engineering and Operations Research
                Columbia University
                New York, NY 10027, USA[0m

Box rectangle:  [32m(71.4, 186.3) -> (540.0, 234.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZuyue Fu∗
                zuyuefu2022@u.northwestern.edu
                Department of Industrial Engineering and Management Sciences
                Northwestern University
                Evanston, IL 60208, USA[0m

Box rectangle:  [32m(70.1, 240.2) -> (540.0, 288.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhuoran Yang
                zhuoran.yang@yale.edu
                Department of Statistics and Data Science
                Yale University
                New Haven, CT 06520, USA[0m

Box rectangle:  [32m(70.3, 293.7) -> (540.0, 341.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYixin Wang
                yixinw@umich.edu
                Department of Statistics
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(70.3, 347.3) -> (540.0, 395.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDingli Ma
                dingli98@uw.edu
                Department of Information Systems and Operations Management, Michael G. Foster School of Business
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(70.3, 400.9) -> (540.0, 448.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMladen Kolar
                mkolar@marshall.usc.edu
                Department of Data Sciences and Operations, Marshall School of Business
                University of Southern California
                Los Angeles, CA 90089, USA[0m

Box rectangle:  [32m(71.4, 456.1) -> (540.0, 508.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhaoran Wang
                zhaoranwang@gmail.com
                Department of Industrial Engineering and Management Sciences
                Northwestern University
                Evanston, IL 60208, USA[0m

Box rectangle:  [32m(72.0, 743.4) -> (456.7, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Luofeng Liao, Zuyue Fu, Zhuoran Yang, Yixin Wang, Dingli Ma, Mladen Kolar, Zhaoran Wang.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0965.html.[0m



=== Processing ../JMLR 2024/Interpretable algorithmic fairness in structured and unstructured data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Interpretable algorithmic fairness in structured and unstructured data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 6/23; Revised 5/24; Published 7/24[0m

Box rectangle:  [32m(123.5, 101.6) -> (488.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInterpretable algorithmic fairness in structured and
                unstructured data[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHari Bandi
                hbandi@mit.edu
                Massachusetts Institute of Technology
                77 Massachusetts Avenue, Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDimitris Bertsimas
                dbertsim@mit.edu
                Sloan School of Management, Massachusetts Institute of Technology
                77 Massachusetts Avenue, Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThodoris Koukouvinos
                tkoukouv@mit.edu
                Operations Research Center, Massachusetts Institute of Technology
                77 Massachusetts Avenue, Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSofie Kupiec
                skupiec@mit.edu
                Massachusetts Institute of Technology
                77 Massachusetts Avenue, Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (393.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hari Bandi, Dimitris Bertsimas, Thodoris Koukouvinos, and Sofie Kupiec.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0816.html.[0m



=== Processing ../JMLR 2024/Invariant and Equivariant Reynolds Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Invariant and Equivariant Reynolds Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 8/22; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(163.9, 101.7) -> (448.1, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInvariant and Equivariant Reynolds Networks[0m

Box rectangle:  [32m(90.0, 135.4) -> (521.8, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAkiyoshi Sannai
                SANNAI.AKIYOSHI.7Z@KYOTO-U.AC.JP
                Department of Physics
                Kyoto University, RIKEN
                Kitashirakawa, Sakyo, Kyoto 606-8502 Japan[0m

Box rectangle:  [32m(90.0, 189.0) -> (521.8, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMakoto Kawano
                KAWANO@WEBLAB.T.U-TOKYO.AC.JP
                Graduate School of Engineering
                The University of Tokyo
                7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8654 Japan[0m

Box rectangle:  [32m(90.0, 244.2) -> (521.8, 295.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWataru Kumagai
                KUMAGAI@WEBLAB.T.U-TOKYO.AC.JP
                Graduate School of Engineering
                The University of Tokyo, RIKEN
                7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8654 Japan[0m

Box rectangle:  [32m(90.0, 726.4) -> (297.7, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Akiyoshi Sannai, Makoto Kawano and Wataru Kumagai.[0m

Box rectangle:  [32m(90.0, 741.2) -> (447.5, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0891.html.[0m



=== Processing ../JMLR 2024/Invariant Physics-Informed Neural Networks for Ordinary Differential Equations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Invariant Physics-Informed Neural Networks for Ordinary Differential Equations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-24
                Submitted 11/23; Revised 3/24; Published 7/24[0m

Box rectangle:  [32m(100.5, 101.6) -> (511.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mInvariant Physics-Informed Neural Networks for Ordinary
                Differential Equations[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShivam Arora and Alex Bihlo
                sarora17@mun.ca, abihlo@mun.ca
                Department of Mathematics and Statistics
                Memorial University of Newfoundland
                St. John’s, NL, A1C 5S7, Canada[0m

Box rectangle:  [32m(90.0, 219.0) -> (522.0, 271.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrancis Valiquette
                fvalique@monmouth.edu
                Department of Mathematics
                Monmouth University
                West Long Branch, NJ, 07764, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (288.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shivam Arora, Alex Bihlo, Francis Valiquette.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1511.html.[0m



=== Processing ../JMLR 2024/Iterate Averaging in the Quest for Best Test Error.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Iterate Averaging in the Quest for Best Test Error.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 9/21; Revised 5/23; Published 1/24[0m

Box rectangle:  [32m(131.6, 101.5) -> (480.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIterate Averaging in the Quest for Best Test Error[0m

Box rectangle:  [32m(88.3, 133.5) -> (522.0, 169.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDiego Granziol∗
                diego@purestrength.ai
                Machine Learning Research Group
                University of Oxford, Oxford, UK[0m

Box rectangle:  [32m(88.3, 175.2) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicholas P. Baskerville∗
                n.p.baskerville@bristol.ac.uk
                School of Mathematics
                University of Bristol, Bristol, UK[0m

Box rectangle:  [32m(88.3, 216.8) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXingchen Wan∗
                xwan@robots.ox.ac.uk
                Machine Learning Research Group
                University of Oxford, Oxford, UK[0m

Box rectangle:  [32m(88.3, 258.7) -> (522.0, 294.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSamuel Albanie
                samuel.albanie.academic@gmail.com
                Department of Engineering
                University of Cambridge, Cambridge, UK[0m

Box rectangle:  [32m(88.3, 302.0) -> (522.0, 341.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStephen Roberts
                sjrob@robots.ox.ac.uk
                Machine Learning Research Group
                University of Oxford, Oxford, UK[0m

Box rectangle:  [32m(90.0, 366.4) -> (221.7, 376.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Simon Lacoste-Julien[0m

Box rectangle:  [32m(280.3, 400.0) -> (331.7, 411.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 424.9) -> (502.5, 578.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe analyse and explain the increased generalisation performance of iterate averaging using
                a Gaussian process perturbation model between the true and batch risk surface on the high
                dimensional quadratic. We derive three phenomena from our theoretical results: (1) The
                importance of combining iterate averaging (IA) with large learning rates and regularisation
                for improved generalisation. (2) Justification for less frequent averaging. (3) That we
                expect adaptive gradient methods to work equally well, or better, with iterate averaging
                than their non-adaptive counterparts. Inspired by these results, together with empirical
                investigations of the importance of appropriate regularisation for the solution diversity of the
                iterates, we propose two adaptive algorithms with iterate averaging. These give significantly
                better results compared to stochastic gradient descent (SGD), require less tuning and do
                not require early stopping or validation set monitoring. We showcase the efficacy of our
                approach on the CIFAR-10/100, ImageNet and Penn Treebank datasets on a variety of
                modern and classical network architectures.[0m

Box rectangle:  [32m(109.9, 589.4) -> (503.5, 611.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                iterate averaging, generalisation, deep learning theory, deep learning limit,
                adaptive gradient methods[0m

Box rectangle:  [32m(93.7, 652.0) -> (522.3, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m*. These authors contributed equally to this work. XW discovered the AdamW + IA combination and
                led many experiments. DG developed the true gradient perturbation framework using i.i.d gaussian
                assumptions and conducted ImageNet experiments and paper writing. NPB expanded the i.i.d framework
                to a general GP and did the AdamW convergence proofs. SA helped in ImageNet experiments and SR
                helped with foundation concepts and did extensive proof reading; both provided overall supervision.[0m

Box rectangle:  [32m(89.1, 726.4) -> (454.4, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Diego Granziol, Nicholas P. Baskerville, Xingchen Wan, Samuel Albanie, Stephen Roberts.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-1125.html.[0m



=== Processing ../JMLR 2024/Just Wing It  Near-Optimal Estimation of Missing Mass in a Markovian Sequence.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Just Wing It  Near-Optimal Estimation of Missing Mass in a Markovian Sequence.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-43
                Submitted 4/24; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJust Wing It: Near-Optimal Estimation of Missing Mass in a
                Markovian Sequence[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAshwin Pananjady
                ashwinpm@gatech.edu
                Schools of Industrial and Systems Engineering
                and Electrical and Computer Engineering
                Georgia Institute of Technology
                Atlanta, USA[0m

Box rectangle:  [32m(90.0, 217.4) -> (522.0, 277.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVidya Muthukumar
                vmuthukumar8@gatech.edu
                Schools of Electrical and Computer Engineering
                and Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, USA[0m

Box rectangle:  [32m(90.0, 284.5) -> (522.0, 337.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrew Thangaraj
                andrew@ee.iitm.ac.in
                Department of Electrical Engineering
                Indian Institute of Technology Madras
                Chennai, India[0m

Box rectangle:  [32m(90.0, 362.5) -> (205.5, 372.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Christian Shelton[0m

Box rectangle:  [32m(280.3, 396.1) -> (331.7, 408.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 415.6) -> (502.1, 606.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe study the problem of estimating the stationary mass—also called the unigram mass—
                that is missing from a single trajectory of a discrete-time, ergodic Markov chain. This
                problem has several applications—for example, estimating the stationary missing mass
                is critical for accurately smoothing probability estimates in sequence models. While the
                classical Good–Turing estimator from the 1950s has appealing properties for i.i.d. data,
                it is known to be biased in the Markovian setting, and other heuristic estimators do not
                come equipped with guarantees. Operating in the general setting in which the size of the
                state space may be much larger than the length n of the trajectory, we develop a linear-
                runtime estimator called Windowed Good–Turing (WingIt) and show that its risk decays
                as e
                O(Tmix/n), where Tmix denotes the mixing time of the chain in total variation distance.
                Notably, this rate is independent of the size of the state space and minimax-optimal up to a
                logarithmic factor in n/Tmix. We also present an upper bound on the variance of the missing
                mass random variable, which may be of independent interest. We extend our estimator to
                approximate the stationary mass placed on elements occurring with small frequency in the
                trajectory. Finally, we demonstrate the efficacy of our estimators both in simulations on
                canonical chains and on sequences constructed from natural language text.[0m

Box rectangle:  [32m(109.9, 611.3) -> (442.1, 621.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                missing mass, Good–Turing, Markov chains, minimax optimal[0m

Box rectangle:  [32m(90.0, 643.3) -> (180.3, 655.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 667.6) -> (522.0, 706.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTwo classical problems in statistical analysis—relevant to both design of experiments and
                inference—are those of assessing sample coverage and discovery probability. Given a “train-
                ing” sequence Xn = (X1, X2, . . . , Xn) of random examples in some unknown sample space,[0m

Box rectangle:  [32m(90.0, 726.4) -> (344.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Ashwin Pananjady, Vidya Muthukumar, Andrew Thangaraj.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0511.html.[0m



=== Processing ../JMLR 2024/KerasCV and KerasNLP  Multi-framework Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/KerasCV and KerasNLP  Multi-framework Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-10
                Submitted 3/24; Revised 9/24; Published 11/24[0m

Box rectangle:  [32m(125.5, 101.6) -> (486.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKerasCV and KerasNLP: Multi-framework Models[0m

Box rectangle:  [32m(90.0, 134.6) -> (499.0, 202.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLead Authors
                Matthew Watson, Divyashree Shivakumar Sreepathihalli, Fran ̧cois Chollet
                Martin G ̈orner, Kiranbir Sodhia, Ramesh Sampath, Tirth Patel
                Haifeng Jin, Neel Kovelamudi, Gabriel Rasskin, Samaneh Saadat
                Luke Wood, Chen Qian, Jonathan Bischof, Ian Stenbit[0m

Box rectangle:  [32m(90.0, 207.4) -> (475.9, 243.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m{mattdangerw, divyasreepat, fchollet, mgorner, ksodhia}@google.com
                {rameshsampath, tirthp, haifengj, nkovela, grasskin, ssaadat}@google.com
                {lukewoodcs, qianchen94era, jbischof1}@gmail.com, ian@stenbit.com[0m

Box rectangle:  [32m(90.0, 251.2) -> (204.6, 261.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeras Team, Google, USA[0m

Box rectangle:  [32m(90.0, 286.7) -> (296.3, 313.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCommunity Contributors
                Abheesht Sharma, Anshuman Mishra[0m

Box rectangle:  [32m(90.0, 726.3) -> (520.4, 753.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Matthew Watson, Divyashree Shivakumar Sreepathihalli, Fran ̧cois Chollet, Martin G ̈orner, Kiranbir Sodhia,
                Ramesh Sampath, Tirth Patel, Haifeng Jin, Neel Kovelamudi, Gabriel Rasskin, Samaneh Saadat, Luke Wood, Chen
                Qian, Jonathan Bischof, Ian Stenbit, Abheesht Sharma, Anshuman Mishra.[0m

Box rectangle:  [32m(90.0, 759.9) -> (517.0, 777.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0404.html.[0m



=== Processing ../JMLR 2024/Kernel Thinning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Kernel Thinning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-77
                Submitted 11/21; Revised 3/24; Published 4/24[0m

Box rectangle:  [32m(247.5, 101.6) -> (364.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKernel Thinning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRaaz Dwivedi
                dwivedi@cornell.edu
                Cornell Tech[0m

Box rectangle:  [32m(90.0, 165.2) -> (522.0, 190.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLester Mackey
                lmackey@microsoft.com
                Microsoft Research New England[0m

Box rectangle:  [32m(90.0, 216.1) -> (192.9, 226.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ingo Steinwart[0m

Box rectangle:  [32m(280.3, 251.6) -> (331.7, 263.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 266.3) -> (502.1, 346.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe introduce kernel thinning, a new procedure for compressing a distribution P more effec-
                tively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel k⋆and
                O(n2) time, kernel thinning compresses an n-point approximation to P into a √n-point ap-
                proximation with comparable worst-case integration error across the associated reproducing
                kernel Hilbert space. The maximum discrepancy in integration error is Od(n−1/2√log n)
                in probability for compactly supported P and Od(n−1[0m

Box rectangle:  [32m(109.9, 321.9) -> (502.2, 460.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2 (log n)(d+1)/2√log log n) for sub-
                exponential P on Rd. In contrast, an equal-sized i.i.d. sample from P suffers Ω(n−1/4) in-
                tegration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo
                error rates for uniform P on [0, 1]d but apply to general distributions on Rd and a wide
                range of common kernels. Moreover, the same construction delivers near-optimal L∞core-
                sets in O(n2) time. We use our results to derive explicit non-asymptotic maximum mean
                discrepancy bounds for Gaussian, Mat ́ern, and B-spline kernels and present two vignettes il-
                lustrating the practical benefits of kernel thinning over i.i.d. sampling and standard Markov
                chain Monte Carlo thinning, in dimensions d = 2 through 100.
                Keywords: coresets, distribution compression, Markov chain Monte Carlo, maximum
                mean discrepancy, reproducing kernel Hilbert space, thinning[0m

Box rectangle:  [32m(90.0, 479.9) -> (180.3, 491.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 501.6) -> (522.0, 540.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMonte Carlo and Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are
                commonly used to approximate intractable target expectations Pf ≜EX∼P[f(X)] of P-
                integrable functions f with asymptotically exact averages Pnf ≜1[0m

Box rectangle:  [32m(90.0, 526.0) -> (522.1, 671.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mn
                Pn
                i=1 f(xi) based on
                points (xi)n
                i=1 generated from a Markov chain. A standard practice, to minimize the ex-
                pense of downstream function evaluation, is to thin the Markov chain output down to a
                smaller size nout by keeping only every (n/nout)-th sample point (Owen, 2017). We call this
                approach standard thinning, and such sample compression is critical in fields like computa-
                tional cardiology in which each function evaluation triggers an organ or tissue simulation
                consuming thousands of CPU hours (Niederer et al., 2011; Augustin et al., 2016; Stroc-
                chi et al., 2020). Unfortunately, standard thinning also leads to a significant reduction in
                accuracy. For example, thinning one’s chain down to nout = √n sample points increases
                integration error from O(n−1[0m

Box rectangle:  [32m(225.8, 648.4) -> (351.1, 663.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2 ) in probability to Ω(n−1[0m

Box rectangle:  [32m(90.0, 650.9) -> (522.1, 692.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m4 ) by the Markov chain central limit
                theorem (Roberts and Rosenthal, 2004, Prop. 29). Our primary contribution is a more
                effective thinning strategy, which provides op(n−1[0m

Box rectangle:  [32m(90.0, 677.1) -> (522.0, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m4 )-integration error when n
                1
                2 points are
                returned.[0m

Box rectangle:  [32m(90.0, 726.4) -> (244.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Raaz Dwivedi and Lester Mackey.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1334.html.[0m



=== Processing ../JMLR 2024/Label Alignment Regularization for Distribution Shift.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Label Alignment Regularization for Distribution Shift.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 7/23; Published 8/24[0m

Box rectangle:  [32m(115.2, 101.6) -> (497.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLabel Alignment Regularization for Distribution Shift[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEhsan Imani∗
                imani@ualberta.ca
                University of Alberta, Alberta Machine Intelligence Institute[0m

Box rectangle:  [32m(90.0, 163.6) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuojun Zhang
                guojun.zhang@huawei.com
                Huawei Noah’s Ark Lab[0m

Box rectangle:  [32m(90.0, 193.3) -> (522.0, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRunjia Li
                runjia@robots.ox.ac.uk
                Department of Engineering Science, University of Oxford[0m

Box rectangle:  [32m(90.0, 222.9) -> (522.0, 247.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun Luo
                jun.luo1@huawei.com
                Huawei Noah’s Ark Lab[0m

Box rectangle:  [32m(90.0, 252.6) -> (522.0, 276.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPascal Poupart
                ppoupart@uwaterloo.ca
                School of Computer Science, University of Waterloo[0m

Box rectangle:  [32m(90.0, 283.9) -> (522.0, 323.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPhilip H.S. Torr
                philip.torr@eng.ox.ac.uk
                Yangchen Pan∗
                yangchen.pan@eng.ox.ac.uk
                Department of Engineering Science, University of Oxford[0m

Box rectangle:  [32m(90.0, 726.3) -> (503.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip H.S. Torr, and Yangchen Pan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0899.html.[0m



=== Processing ../JMLR 2024/Label Noise Robustness of Conformal Prediction.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Label Noise Robustness of Conformal Prediction.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 11/23; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(133.6, 107.0) -> (478.5, 121.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLabel Noise Robustness of Conformal Prediction[0m

Box rectangle:  [32m(90.0, 144.5) -> (522.0, 180.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBat-Sheva Einbinder∗
                bat-shevab@campus.technion.ac.il
                Department of Electrical and Computer Engineering
                Technion - Israel Institute of Technology[0m

Box rectangle:  [32m(90.0, 191.6) -> (522.0, 228.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShai Feldman∗
                shai.feldman@cs.technion.ac.il
                Department of Computer Science
                Technion - Israel Institute of Technology[0m

Box rectangle:  [32m(90.0, 239.0) -> (522.0, 275.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStephen Bates
                stephenbates@mit.edu
                Department of Electrical Engineering and Computer Science
                Massachusetts Institute of Technology[0m

Box rectangle:  [32m(90.0, 286.1) -> (522.0, 322.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnastasios N. Angelopoulos
                angelopoulos@berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley[0m

Box rectangle:  [32m(90.0, 333.2) -> (522.0, 369.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAsaf Gendler
                asafgendler@campus.technion.ac.il
                Department of Electrical and Computer Engineering
                Technion—Israel Institute of Technology[0m

Box rectangle:  [32m(90.0, 381.8) -> (522.0, 421.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYaniv Romano
                yromano@technion.ac.il
                Departments of Electrical and Computer Engineering and of Computer Science
                Technion—Israel Institute of Technology[0m

Box rectangle:  [32m(90.0, 726.3) -> (497.4, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bat-Sheva Einbinder, Shai Feldman, Stephen Bates, Anastasios N. Angelopoulos, Asaf Gendler, Yaniv
                Romano.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1549.html.[0m



=== Processing ../JMLR 2024/Law of Large Numbers and Central Limit Theorem for Wide Two-layer Neural Networks  The Mini-Batch and Noisy Case.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Law of Large Numbers and Central Limit Theorem for Wide Two-layer Neural Networks  The Mini-Batch and Noisy Case.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-76
                Submitted 8/22; Revised 1/24; Published 7/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLaw of Large Numbers and Central Limit Theorem for Wide
                Two-layer Neural Networks: The Mini-Batch and Noisy Case[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArnaud Descours
                arnaud.descours@uca.fr
                Laboratoire de Math ́ematiques Blaise Pascal UMR 6620
                Universit ́e Clermont Auvergne
                Aubi`ere, France[0m

Box rectangle:  [32m(90.0, 205.4) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArnaud Guillin
                arnaud.guillin@uca.fr
                Laboratoire de Math ́ematiques Blaise Pascal UMR 6620 and Institut Universaire de France
                Universit ́e Clermont Auvergne
                Aubi`ere, France[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mManon Michel
                manon.michel@uca.fr
                CNRS, Laboratoire de Math ́ematiques Blaise Pascal UMR 6620
                Universit ́e Clermont Auvergne
                Aubi`ere, France[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoris Nectoux
                boris.nectoux@uca.fr
                Laboratoire de Math ́ematiques Blaise Pascal UMR 6620
                Universit ́e Clermont Auvergne
                Aubi`ere, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (375.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Arnaud Descours, Arnaud Guillin, Manon Michel and Boris Nectoux.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0952.html.[0m



=== Processing ../JMLR 2024/Learnability of Linear Port-Hamiltonian Systems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learnability of Linear Port-Hamiltonian Systems.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 4/23; Published 2/24[0m

Box rectangle:  [32m(132.9, 102.0) -> (479.1, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearnability of Linear Port-Hamiltonian Systems[0m

Box rectangle:  [32m(90.0, 135.7) -> (522.0, 170.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJuan-Pablo Ortega
                Juan-Pablo.Ortega@ntu.edu.sg
                Division of Mathematical Sciences
                Nanyang Technological University, Singapore[0m

Box rectangle:  [32m(90.0, 178.9) -> (522.0, 216.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaiying Yin
                yind0004@e.ntu.edu.sg
                Division of Mathematical Sciences
                Nanyang Technological University, Singapore[0m

Box rectangle:  [32m(90.0, 241.9) -> (202.8, 251.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Maxim Raginsky[0m

Box rectangle:  [32m(280.3, 277.5) -> (331.7, 289.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 296.0) -> (502.1, 485.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA complete structure-preserving learning scheme for single-input/single-output (SISO) lin-
                ear port-Hamiltonian systems is proposed.
                The construction is based on the solution,
                when possible, of the unique identification problem for these systems, in ways that reveal
                fundamental relationships between classical notions in control theory and crucial proper-
                ties in the machine learning context, like structure-preservation and expressive power. In
                the canonical case, it is shown that, up to initializations, the set of uniquely identified
                systems can be explicitly characterized as a smooth manifold endowed with global Eu-
                clidean coordinates, which allows concluding that the parameter complexity necessary for
                the replication of the dynamics is only O(n) and not O(n2), as suggested by the standard
                parametrization of these systems. Furthermore, it is shown that linear port-Hamiltonian
                systems can be learned while remaining agnostic about the dimension of the underlying
                data-generating system. Numerical experiments show that this methodology can be used
                to efficiently estimate linear port-Hamiltonian systems out of input-output realizations,
                making the contributions in this paper the first example of a structure-preserving machine
                learning paradigm for linear port-Hamiltonian systems based on explicit representations of
                this model category.[0m

Box rectangle:  [32m(109.9, 502.3) -> (502.1, 536.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Linear port-Hamiltonian system, machine learning, structure-preserving al-
                gorithm, systems theory, physics-informed machine learning, unique identification problem,
                controllable representation, observable representation, canonical representation.[0m

Box rectangle:  [32m(90.0, 558.9) -> (142.5, 570.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mContents[0m

Box rectangle:  [32m(90.0, 587.8) -> (522.0, 598.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1
                Introduction
                3[0m

Box rectangle:  [32m(90.0, 616.0) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2
                Preliminaries
                6
                2.1
                State-space systems and morphisms
                . . . . . . . . . . . . . . . . . . . . . .
                6
                2.2
                Hamiltonian and port-Hamiltonian systems . . . . . . . . . . . . . . . . . .
                7
                2.3
                Controllability and observability
                . . . . . . . . . . . . . . . . . . . . . . . .
                9
                2.4
                The symplectic Lie group and its Lie algebra
                . . . . . . . . . . . . . . . . .
                10
                2.5
                Williamson’s normal form . . . . . . . . . . . . . . . . . . . . . . . . . . . .
                10[0m

Box rectangle:  [32m(90.0, 726.5) -> (255.0, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Juan-Pablo Ortega and Daiying Yin.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0450.html.[0m



=== Processing ../JMLR 2024/Learning and scoring Gaussian latent variable causal models with unknown additive interventions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning and scoring Gaussian latent variable causal models with unknown additive interventions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-68
                Submitted 8/22; Published 9/24[0m

Box rectangle:  [32m(92.8, 96.4) -> (519.4, 128.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning and scoring Gaussian latent variable causal models
                with unknown additive interventions[0m

Box rectangle:  [32m(90.0, 141.5) -> (522.0, 165.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArmeen Taeb
                ataeb@uw.edu
                Department of Statistics, University of Washington[0m

Box rectangle:  [32m(90.0, 177.9) -> (521.9, 202.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJuan L. Gamella
                juan.gamella@stat.math.ethz.ch
                Seminar for Statistics, ETH Z ̈urich[0m

Box rectangle:  [32m(90.0, 214.4) -> (522.0, 238.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristina Heinze-Deml
                heinzedeml@stat.math.ethz.ch
                Seminar for Statistics, ETH Z ̈urich[0m

Box rectangle:  [32m(90.0, 252.4) -> (522.0, 278.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter B ̈uhlmann
                peter.buehlmann@stat.math.ethz.ch
                Seminar for Statistics, ETH Z ̈urich[0m

Box rectangle:  [32m(90.0, 726.3) -> (407.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Armeen Taeb, Juan L. Gamella, Christina Heinze-Deml and Peter B ̈uhlmann.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0979.html.[0m



=== Processing ../JMLR 2024/Learning Discretized Neural Networks under Ricci Flow.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Discretized Neural Networks under Ricci Flow.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 4/22; Revised 9/24; Published 11/24[0m

Box rectangle:  [32m(107.3, 101.6) -> (504.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Discretized Neural Networks under Ricci Flow[0m

Box rectangle:  [32m(90.0, 133.7) -> (518.2, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun Chen1,2
                junc@zju.edu.cn[0m

Box rectangle:  [32m(90.0, 151.4) -> (518.2, 164.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanwen Chen1
                chenhanwen@zju.edu.cn[0m

Box rectangle:  [32m(90.0, 169.1) -> (518.2, 181.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMengmeng Wang1
                mengmengwang@zju.edu.cn[0m

Box rectangle:  [32m(90.0, 186.8) -> (518.2, 199.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuang Dai3
                guang.gdai@gmail.com[0m

Box rectangle:  [32m(90.0, 204.6) -> (518.2, 217.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIvor W. Tsang4,5,6
                ivor.tsang@gmail.com[0m

Box rectangle:  [32m(90.0, 222.2) -> (518.2, 234.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYong Liu1∗
                yongliu@iipc.zju.edu.cn[0m

Box rectangle:  [32m(90.0, 241.6) -> (523.4, 348.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1Institute of Cyber-Systems and Control, Zhejiang University, China
                2School of Computer Science and Technology, Zhejiang Normal University, China
                3SGIT AI Lab, State Grid Corporation of China, China
                4Centre for Frontier Artificial Intelligence Research, Agency for Science, Technology and Research
                (A*STAR), Singapore
                5Institute of High Performance Computing, Agency for Science, Technology and Research (A*STAR),
                Singapore
                6College of Computing and Data Science, Nanyang Technological University, Singapore[0m

Box rectangle:  [32m(90.0, 726.3) -> (440.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang and Yong Liu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0444.html.[0m



=== Processing ../JMLR 2024/Learning Dynamic Mechanisms in Unknown Environments  A Reinforcement Learning Approach.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Dynamic Mechanisms in Unknown Environments  A Reinforcement Learning Approach.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-73
                Submitted 2/23; Revised 12/23; Published 8/24[0m

Box rectangle:  [32m(95.9, 101.6) -> (516.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Dynamic Mechanisms in Unknown Environments:
                A Reinforcement Learning Approach[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShuang Qiu∗
                masqiu@ust.hk
                The Hong Kong University of Science and Technology
                Hong Kong, China[0m

Box rectangle:  [32m(90.0, 193.2) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoxiang Lyu∗
                blyu@chicagobooth.edu
                The University of Chicago
                Chicago, IL, USA[0m

Box rectangle:  [32m(90.0, 234.8) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQinglin Meng∗
                meng160@purdue.edu
                Purdue University
                West Lafayette, IN, USA[0m

Box rectangle:  [32m(90.0, 276.7) -> (522.0, 312.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhaoran Wang
                zhaoranwang@gmail.com
                Northwestern University
                Evanston, IL, USA[0m

Box rectangle:  [32m(90.0, 318.4) -> (522.0, 354.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhuoran Yang
                zhuoran.yang@yale.edu
                Yale University
                New Haven, CT, USA[0m

Box rectangle:  [32m(90.0, 361.6) -> (522.0, 400.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael I. Jordan
                jordan@cs.berkeley.edu
                University of California
                Berkeley, CA, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (476.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shuang Qiu, Boxiang Lyu, Qinglin Meng, Zhaoran Wang, Zhuoran Yang, and Michael I. Jordan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0159.html.[0m



=== Processing ../JMLR 2024/Learning from many trajectories.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning from many trajectories.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-109
                Submitted 9/23; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(191.1, 101.6) -> (421.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning from many trajectories[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStephen Tu
                stephen.tu@usc.edu
                University of Southern California∗[0m

Box rectangle:  [32m(90.0, 160.0) -> (371.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMing Hsieh Department of Electrical and Computer Engineering
                Los Angeles, CA 90089, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 223.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRoy Frostig
                frostig@google.com
                Google DeepMind
                San Francisco, CA 94105, USA[0m

Box rectangle:  [32m(90.0, 230.7) -> (522.0, 283.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMahdi Soltanolkotabi
                soltanol@usc.edu
                University of Southern California
                Ming Hsieh Department of Electrical and Computer Engineering
                Los Angeles, CA 90089, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (312.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Stephen Tu, Roy Frostig, and Mahdi Soltanolkotabi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1145.html.[0m



=== Processing ../JMLR 2024/Learning Gaussian DAGs from Network Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Gaussian DAGs from Network Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 7/21; Revised 8/23; Published 12/24[0m

Box rectangle:  [32m(144.6, 101.6) -> (467.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Gaussian DAGs from Network Data[0m

Box rectangle:  [32m(88.3, 135.5) -> (522.0, 215.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHangjian Li
                lihangjian123@ucla.edu
                Oscar Hernan Madrid Padilla
                oscar.madrid@stat.ucla.edu
                Qing Zhou
                zhou@stat.ucla.edu
                Department of Statistics and Data Science
                University of California, Los Angeles
                Los Angeles, CA 90095, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (324.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hangjian Li, Oscar Hernan Madrid Padilla, Qing Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0846.html.[0m



=== Processing ../JMLR 2024/Learning Non-Gaussian Graphical Models via Hessian Scores and Triangular Transport.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Non-Gaussian Graphical Models via Hessian Scores and Triangular Transport.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 1/21; Revised 2/23; Published 3/24[0m

Box rectangle:  [32m(151.1, 101.5) -> (460.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Non-Gaussian Graphical Models
                via Hessian Scores and Triangular Transport[0m

Box rectangle:  [32m(88.8, 151.5) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRicardo Baptista∗
                rsb@caltech.edu
                Department of Computing and Mathematical Sciences
                California Institute of Technology
                Pasadena, CA 91125-2100, USA[0m

Box rectangle:  [32m(88.3, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRebecca Morrison∗
                rebeccam@colorado.edu
                Department of Computer Science
                University of Colorado Boulder
                Boulder, CO 80309-0430, USA[0m

Box rectangle:  [32m(88.3, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOlivier Zahm
                olivier.zahm@inria.fr
                Université Grenoble Alpes
                Inria, CNRS, Grenoble INP, LJK
                38000 Grenoble, France[0m

Box rectangle:  [32m(88.8, 314.1) -> (522.0, 366.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoussef Marzouk
                ymarz@mit.edu
                Center for Computational Science and Engineering
                Massachusetts Institute of Technology
                Cambridge, MA 02139-4301, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (394.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ricardo Baptista, Rebecca Morrison, Olivier Zahm, and Youssef Marzouk.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0022.html.[0m



=== Processing ../JMLR 2024/Learning Optimal Dynamic Treatment Regimens Subject to Stagewise Risk Controls.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Optimal Dynamic Treatment Regimens Subject to Stagewise Risk Controls.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-64
                Submitted 1/23; Published 4/24[0m

Box rectangle:  [32m(93.9, 101.6) -> (518.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Optimal Dynamic Treatment Regimens Subject to
                Stagewise Risk Controls[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMochuan Liu
                mochuan@live.unc.edu
                Department of Biostatistics
                University of North Carolina at Chapel Hill
                Chapel Hill, NC 27599, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanjia Wang
                yw2016@cumc.columbia.edu
                Department of Biostatistics
                Columbia University
                New York, NY 10032, USA[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHaoda Fu
                fu haoda@lilly.com
                Eli Lilly and Company
                Indianapolis, IN 46285, USA[0m

Box rectangle:  [32m(90.0, 302.2) -> (522.0, 355.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDonglin Zeng
                dzeng@umich.edu
                Department of Biostatistics
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (337.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mochuan Liu, Yuanjia Wang, Haoda Fu and Donglin Zeng.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0072.html.[0m



=== Processing ../JMLR 2024/Learning Regularized Graphon Mean-Field Games with Unknown Graphons.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning Regularized Graphon Mean-Field Games with Unknown Graphons.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-95
                Submitted 10/23; Revised 5/24; Published 11/24[0m

Box rectangle:  [32m(109.1, 101.6) -> (502.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning Regularized Graphon Mean-Field Games with
                Unknown Graphons[0m

Box rectangle:  [32m(89.4, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFengzhuo Zhang
                fzzhang@u.nus.edu
                Department of Electrical and Computer Engineering
                National University of Singapore
                Singapore 117583[0m

Box rectangle:  [32m(89.4, 205.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVincent Y. F. Tan
                vtan@nus.edu.sg
                Department of Mathematics
                Department of Electrical and Computer Engineering
                National University of Singapore
                Singapore 119076[0m

Box rectangle:  [32m(89.4, 271.0) -> (522.0, 319.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhaoran Wang
                zhaoranwang@gmail.com
                Department of Industrial Engineering and Management Sciences
                Northwestern University
                Evanston, IL 60208-3109, USA[0m

Box rectangle:  [32m(88.1, 326.2) -> (522.0, 378.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhuoran Yang
                zhuoranyang.work@gmail.com
                Department of Statistics and Data Science
                Yale University
                New Haven, CT 06511, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (388.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Fengzhuo Zhang, Vincent Y. F. Tan, Zhaoran Wang, and Zhuoran Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1409.html.[0m



=== Processing ../JMLR 2024/Learning to Warm-Start Fixed-Point Optimization Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning to Warm-Start Fixed-Point Optimization Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 9/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(127.1, 101.6) -> (485.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning to Warm-Start Fixed-Point Optimization
                Algorithms[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRajiv Sambharya
                rajivs@princeton.edu
                Operations Research and Financial Engineering, Princeton University, Princeton, NJ, USA[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorgina Hall
                georgina.hall@insead.edu
                Decision Sciences, INSEAD, Fontainebleau, France[0m

Box rectangle:  [32m(90.0, 211.2) -> (522.0, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBrandon Amos
                bda@meta.com
                Meta AI, New York City, NY, USA[0m

Box rectangle:  [32m(90.0, 242.5) -> (522.0, 268.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBartolomeo Stellato
                bstellato@princeton.edu
                Operations Research and Financial Engineering, Princeton University, Princeton, NJ, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (423.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rajiv Sambharya and Georgina Hall and Brandon Amos and Bartolomeo Stellato.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1174.html.[0m



=== Processing ../JMLR 2024/Learning with a linear loss function  excess risk and estimation bounds for ERM  minmax MOM and their regularized versions with applications to robustness in sparse PCA.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning with a linear loss function  excess risk and estimation bounds for ERM  minmax MOM and their regularized versions with applications to robustness in sparse PCA.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-90
                Submitted 10/23; Revised 12/24; Published 12/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 169.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning with a linear loss function: excess risk and
                estimation bounds for ERM, minmax MOM and their
                regularized versions with applications to robustness in sparse
                PCA.[0m

Box rectangle:  [32m(90.0, 187.7) -> (522.0, 235.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuillaume Lecu ́e
                lecue@essec.edu
                IDS Department
                ESSEC, Business school
                3 Av. Bernard Hirsch, 95000 Cergy, France.[0m

Box rectangle:  [32m(90.0, 242.9) -> (522.0, 295.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLucie Neirac
                lucie.neirac@ensae.fr
                Statistics Department
                CREST, ENSAE, IPParis
                5 Av. Henry Le Chatelier, Palaiseau, France.[0m

Box rectangle:  [32m(90.0, 726.3) -> (248.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Guillaume Lecu ́e and Lucie Neirac.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1405.html.[0m



=== Processing ../JMLR 2024/Learning with Norm Constrained  Over-parameterized  Two-layer Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Learning with Norm Constrained  Over-parameterized  Two-layer Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 11/22; Revised 10/23; Published 4/24[0m

Box rectangle:  [32m(103.1, 101.7) -> (509.2, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning with Norm Constrained, Over-parameterized, Two-layer
                Neural Networks[0m

Box rectangle:  [32m(89.6, 153.0) -> (522.0, 233.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFanghui Liu∗
                FANGHUI.LIU@WARWICK.AC.UK
                Department of Computer Science, University of Warwick, Coventry, UK
                Leello Dadi
                LEELLO.DADI@EPFL.CH
                Lab for Information and Inference Systems,  ́Ecole Polytechnique F ́ed ́erale de Lausanne (EPFL), Switzerland
                Volkan Cevher
                VOLKAN.CEVHER@EPFL.CH
                Lab for Information and Inference Systems,  ́Ecole Polytechnique F ́ed ́erale de Lausanne (EPFL), Switzerland[0m

Box rectangle:  [32m(90.0, 726.4) -> (250.1, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Fanghui Liu, Leello Dadi, Volkan Cevher.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1250.html.[0m



=== Processing ../JMLR 2024/Linear Distance Metric Learning with Noisy Labels.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Linear Distance Metric Learning with Noisy Labels.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 6/23; Revised 4/24; Published 4/24[0m

Box rectangle:  [32m(148.3, 101.7) -> (463.7, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLinear Distance Metric Learning with Noisy Labels[0m

Box rectangle:  [32m(90.0, 135.4) -> (521.8, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMeysam Alishahi
                ALISHAHI@CS.UTAH.EDU
                Kahlert School of Computing
                University of Utah
                Salt Lake City, UT 84112, USA[0m

Box rectangle:  [32m(90.0, 189.0) -> (521.8, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnna Little
                LITTLE@MATH.UTAH.EDU
                Department of Mathematics, Utah Center For Data Science
                University of Utah
                Salt Lake City, UT 84112, USA[0m

Box rectangle:  [32m(90.0, 244.2) -> (521.8, 309.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeff M. Phillips
                JEFFP@CS.UTAH.EDU
                Kahlert School of Computing, Utah Center for Data Science
                University of Utah
                Salt Lake City, UT 84112, USA
                and visiting ScaDS.AI, University of Leipzig, and MPI for Math in the Sciences[0m

Box rectangle:  [32m(90.0, 726.4) -> (281.9, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Meysam Alishahi, Anna Little, and Jeff M. Phillips.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0791.html.[0m



=== Processing ../JMLR 2024/Linear Regression With Unmatched Data  A Deconvolution Perspective.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Linear Regression With Unmatched Data  A Deconvolution Perspective.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 8/22; Revised 3/24; Published 7/24[0m

Box rectangle:  [32m(94.0, 101.6) -> (518.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLinear Regression With Unmatched Data: A Deconvolution
                Perspective[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMona Azadkia
                m.azadkia@lse.ac.uk
                Department of Statistics
                London School of Economics and Political Science
                London, United Kingdom[0m

Box rectangle:  [32m(90.0, 207.0) -> (521.9, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFadoua Balabdaoui
                fadoua.balabdaoui@stat.math.ethz.ch
                Department of Mathematics
                ETH Z ̈urich
                Z ̈urich, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (267.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mona Azadkia, and Fadoua Balabdaoui.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0930.html.[0m



=== Processing ../JMLR 2024/Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 12/23; Revised 1/24; Published 12/24[0m

Box rectangle:  [32m(128.7, 101.6) -> (483.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLine Graph Vietoris-Rips Persistence Diagram for
                Topological Graph Representation Learning[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJaesun Shin
                j1991.shin@samsung.com
                Samsung SDS[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEunjoo Jeon
                ej85.jeon@samsung.com
                Samsung SDS[0m

Box rectangle:  [32m(90.0, 211.2) -> (522.0, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTaewon Cho
                taewon08.cho@samsung.com
                Samsung SDS[0m

Box rectangle:  [32m(90.0, 240.6) -> (522.0, 276.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNamkyeong Cho1
                namkyeong.cho@gmail.com
                Center for Mathematical Machine Learning and its Applications(CM2LA), Department of Mathe-
                matics POSTECH[0m

Box rectangle:  [32m(90.0, 284.1) -> (522.0, 309.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoungjune Gwon
                gyj.gwon@samsung.com
                Samsung SDS[0m

Box rectangle:  [32m(90.0, 726.3) -> (403.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jaesun Shin, Eunjoo Jeon, Taewon Cho, Namkyeong Cho, Youngjune Gwon.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1610.html.[0m



=== Processing ../JMLR 2024/Localisation of Regularised and Multiview Support Vector Machine Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Localisation of Regularised and Multiview Support Vector Machine Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 4/23; Revised 12/24; Published 12/24[0m

Box rectangle:  [32m(99.5, 101.6) -> (512.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLocalisation of Regularised and Multiview Support Vector
                Machine Learning[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 247.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAurelian Gheondea
                a.gheondea@imar.ro
                Institute of Mathematics of the Romanian Academy
                21 Calea Grivit ̧ei
                010702 Bucharest, Romania
                and
                Department of Mathematics
                Bilkent University
                06800 Bilkent, Ankara, Turkey
                aurelian@fen.bilkent.edu.tr[0m

Box rectangle:  [32m(90.0, 266.8) -> (522.0, 333.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCankat Tilki
                cankat@vt.edu
                Department of Mathematics and
                Division of Computational Modeling and Data Analytics
                Virginia Polytechnic Institute and State University
                Blacksburg Virginia, 24061 U.S.A.[0m

Box rectangle:  [32m(90.0, 726.3) -> (258.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Aurelian Gheondea and Cankat Tilki.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0522.html.[0m



=== Processing ../JMLR 2024/Localized Debiased Machine Learning  Efficient Inference on Quantile Treatment Effects and Beyond.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Localized Debiased Machine Learning  Efficient Inference on Quantile Treatment Effects and Beyond.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 5/23; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(91.4, 96.4) -> (520.7, 128.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLocalized Debiased Machine Learning: Efficient Inference on
                Quantile Treatment Effects and Beyond[0m

Box rectangle:  [32m(90.0, 141.5) -> (522.0, 189.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNathan Kallus
                kallus@cornell.edu
                Cornell Tech
                Cornell University
                2 West Loop Rd, NY 10044, USA[0m

Box rectangle:  [32m(90.0, 201.9) -> (522.0, 249.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaojie Mao
                maoxj@sem.tsinghua.edu.cn
                School of Economics and Management
                Tsinghua University
                Beijing, 100084, China[0m

Box rectangle:  [32m(90.0, 263.8) -> (522.0, 316.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMasatoshi Uehara
                mu223@cornell.edu
                Cornell Tech
                Cornell University
                2 West Loop Rd, NY 10044, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (197.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kallus, Mao, Uehara.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0661.html.[0m



=== Processing ../JMLR 2024/Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 8/22; Revised 12/23; Published 3/24[0m

Box rectangle:  [32m(90.1, 101.6) -> (522.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLog Barriers for Safe Black-box Optimization with Application
                to Safe Reinforcement Learning[0m

Box rectangle:  [32m(89.4, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIlnura Usmanova
                ilnura.usmanova@psi.ch
                Swiss Data Science Center,
                Paul Scherrer Institute, 5232 Villigen, Switzerland[0m

Box rectangle:  [32m(89.5, 193.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYarden As
                yarden.as@inf.ethz.ch
                Institute for Machine Learning, D-INFK,
                ETH Zürich, 8092 Zurich, Switzerland[0m

Box rectangle:  [32m(89.5, 235.0) -> (522.0, 259.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaryam Kamgarpour*
                maryam.kamgarpour@epfl.ch
                STI-IGM-Sycamore, EPFL, 1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(89.5, 266.3) -> (522.0, 305.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndreas Krause*
                krausea@ethz.ch
                Institute for Machine Learning, D-INFK,
                ETH Zürich, 8092 Zurich, Switzerland[0m

Box rectangle:  [32m(90.0, 330.7) -> (203.8, 340.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Csaba Szepesvari[0m

Box rectangle:  [32m(280.3, 366.3) -> (331.7, 378.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(108.8, 384.1) -> (504.0, 587.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimizing noisy functions online, when evaluating the objective requires experiments on
                a deployed system, is a crucial task arising in manufacturing, robotics and various other
                domains. Often, constraints on safe inputs are unknown ahead of time, and we only obtain
                noisy information, indicating how close we are to violating the constraints. Yet, safety must
                be guaranteed at all times, not only for the final output of the algorithm.
                We introduce a general approach for seeking a stationary point in high dimensional
                non-linear stochastic optimization problems in which maintaining safety during learning
                is crucial. Our approach called LB-SGD, is based on applying stochastic gradient descent
                (SGD) with a carefully chosen adaptive step size to a logarithmic barrier approximation of
                the original problem. We provide a complete convergence analysis of non-convex, convex, and
                strongly-convex smooth constrained problems, with first-order and zeroth-order feedback.
                Our approach yields efficient updates and scales better with dimensionality compared to
                existing approaches.
                We empirically compare the sample complexity and the computational cost of our method
                with existing safe learning approaches. Beyond synthetic benchmarks, we demonstrate the
                effectiveness of our approach on minimizing constraint violation in policy search tasks in
                safe reinforcement learning (RL). 1[0m

Box rectangle:  [32m(109.9, 591.6) -> (502.1, 613.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Stochastic optimization, safe learning, black-box optimization, smooth
                constrained optimization, reinforcement learning[0m

Box rectangle:  [32m(90.0, 634.3) -> (180.2, 646.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 656.7) -> (521.9, 681.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMany optimization tasks in robotics, manufacturing, health sciences, and finance require
                minimizing a loss function under constraints and uncertainties. In several applications, these[0m

Box rectangle:  [32m(93.7, 694.5) -> (182.9, 705.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. * Equal supervision[0m

Box rectangle:  [32m(89.1, 726.4) -> (393.8, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Ilnura Usmanova, Yarden As, Maryam Kamgarpour, and Andreas Krause.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-0878.html.[0m



=== Processing ../JMLR 2024/Logistic Regression Under Network Dependence.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Logistic Regression Under Network Dependence.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-62
                Submitted 9/22; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(114.7, 101.6) -> (497.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHigh Dimensional Logistic Regression Under Network
                Dependence[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSomabha Mukherjee
                somabha@nus.edu.sg
                Department of Statistics and Data Science
                National University of Singapore, Singapore[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZiang Niu
                ziangniu@wharton.upenn.edu
                Department of Statistics and Data Science
                University of Pennsylvania, Philadelphia, USA[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSagnik Halder
                shalder@ufl.edu
                Department of Statistics
                University of Florida, Gainesville, USA[0m

Box rectangle:  [32m(90.0, 276.7) -> (522.0, 312.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBhaswar B. Bhattacharya
                bhaswar@wharton.upenn.edu
                Department of Statistics and Data Science
                University of Pennsylvania, Philadelphia, USA[0m

Box rectangle:  [32m(90.0, 320.0) -> (522.0, 359.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge Michailidis
                gmichail@ufl.edu
                Department of Statistics and Data Science
                University of California, Los Angeles, Los Angeles, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (486.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Somabha Mukherjee, Ziang Niu, Sagnik Halder, Bhaswar B. Bhattacharya, and George Michailidis.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1040.html.[0m



=== Processing ../JMLR 2024/Lower Bounds on the Bayesian Risk via Information Measures.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Lower Bounds on the Bayesian Risk via Information Measures.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 3/23; Revised 2/24; Published 11/24[0m

Box rectangle:  [32m(178.4, 101.6) -> (433.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLower Bounds on the Bayesian Risk
                via Information Measures[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmedeo Roberto Esposito
                amedeo.esposito@oist.jp
                Okinawa Institute of Science and Technology, Okinawa[0m

Box rectangle:  [32m(90.0, 181.5) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdrien Vandenbroucque
                adrien.vandenbroucque@epfl.ch
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 212.8) -> (522.0, 238.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael Gastpar
                michael.gastpar@epfl.ch
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 263.7) -> (177.7, 273.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Daniel Roy[0m

Box rectangle:  [32m(280.3, 299.2) -> (331.7, 311.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 317.1) -> (502.2, 520.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThis paper focuses on parameter estimation and introduces a new method for lower bound-
                ing the Bayesian risk. The method allows for the use of virtually any information measure,
                including R ́enyi’s α, φ-divergences, and Sibson’s α-Mutual Information.
                The approach
                considers divergences as functionals of measures and exploits the duality between spaces
                of measures and spaces of functions. In particular, we show that one can lower bound the
                risk with any information measure by upper bounding its dual via Markov’s inequality. We
                are thus able to provide estimator-independent impossibility results thanks to the Data-
                Processing Inequalities that divergences satisfy. The results are then applied to settings of
                interest involving both discrete and continuous parameters, including the “Hide-and-Seek”
                problem, and compared to the state-of-the-art techniques. An important observation is
                that the behaviour of the lower bound in the number of samples is influenced by the choice
                of the information measure. We leverage this by introducing a new divergence inspired by
                the “Hockey-Stick” divergence, which is demonstrated empirically to provide the largest
                lower bound across all considered settings. If the observations are subject to privatisation,
                stronger impossibility results can be obtained via Strong Data-Processing Inequalities. The
                paper also discusses some generalisations and alternative directions.
                Keywords:
                Bayesian Risk, Estimation, Divergences, Duality, H ̈older’s Inequality[0m

Box rectangle:  [32m(90.0, 541.5) -> (176.6, 553.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 560.3) -> (522.1, 670.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this work,1 we consider the problem of parameter estimation in a Bayesian setting. In
                this problem, an underlying parameter is modelled as a random variable. Noisy observations
                are made according to a given conditional probability distribution, conditioned on the reali-
                sation of the underlying parameter. Based on these observations, the parameter needs to be
                estimated. Estimation quality is assessed through a fidelity criterion, expressed in terms of
                a loss function. The average incurred loss is referred to as the Bayesian risk. This problem
                has a rich history, dating back to Bayes (1764). Given the characteristics of the observation
                process, it is of interest to characterise the performance of the optimal estimator. This is[0m

Box rectangle:  [32m(93.7, 684.9) -> (522.1, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. This article was presented in part at the 2021 and 2022 IEEE International Symposia on Information
                Theory[0m

Box rectangle:  [32m(90.0, 726.4) -> (379.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Amedeo Roberto Esposito, Adrien Vandenbroucque, Michael Gastpar.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0361.html.[0m



=== Processing ../JMLR 2024/Lower Complexity Adaptation for Empirical Entropic Optimal Transport.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Lower Complexity Adaptation for Empirical Entropic Optimal Transport.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 7/23; Revised 5/24; Published 10/24[0m

Box rectangle:  [32m(160.6, 101.5) -> (451.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLower Complexity Adaptation
                for Empirical Entropic Optimal Transport[0m

Box rectangle:  [32m(88.3, 153.4) -> (522.0, 219.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichel Groppe
                michel.groppe@uni-goettingen.de
                Shayan Hundrieser
                s.hundrieser@math.uni-goettingen.de
                Institute for Mathematical Stochastics
                University of Göttingen
                Goldschmidtstraße 7, 37077 Göttingen, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (264.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Michel Groppe and Shayan Hundrieser.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0856.html.[0m



=== Processing ../JMLR 2024/Lower Complexity Bounds of Finite-Sum Optimization Problems  The Results and Construction.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Lower Complexity Bounds of Finite-Sum Optimization Problems  The Results and Construction.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-86
                Submitted 3/21; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(112.2, 101.6) -> (500.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLower Complexity Bounds of Finite-Sum Optimization
                Problems: The Results and Construction[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuze Han
                hanyuze97@pku.edu.cn
                School of Mathematical Sciences
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuangzeng Xie
                smsxgz@pku.edu.cn
                Academy for Advanced Interdisciplinary Studies
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhihua Zhang
                zhzhang@math.pku.edu.cn
                School of Mathematical Sciences
                Peking University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (274.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuze Han, Guangzeng Xie, Zhihua Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0264.html.[0m



=== Processing ../JMLR 2024/Low-Rank Matrix Estimation in the Presence of Change-Points.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Low-Rank Matrix Estimation in the Presence of Change-Points.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-71
                Submitted 7/22; Published 7/24[0m

Box rectangle:  [32m(135.4, 101.6) -> (476.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLow-Rank Matrix Estimation in the Presence of
                Change-Points[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLei Shi
                leishi@berkeley.edu
                Department of Biostatistics
                University of California, Berkeley
                California 94704, United States[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuanghui Wang
                ghwang.nk@gmail.com
                School of Statistics and Data Science, LPMC, KLMDASR, and LEBPS
                Nankai University
                Tianjin 300071, China[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChangliang Zou
                nk.chlzou@gmail.com
                NITFID, School of Statistics and Data Science, LPMC, KLMDASR, and LEBPS
                Nankai University
                Tianjin 300071, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (275.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lei Shi, Guanghui Wang, Changliang Zou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0852.html.[0m



=== Processing ../JMLR 2024/Low-rank Variational Bayes correction to the Laplace method.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Low-rank Variational Bayes correction to the Laplace method.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-25
                Submitted 11/21; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(117.0, 101.6) -> (495.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLow-rank Variational Bayes correction to the Laplace
                method[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJanet van Niekerk
                janet.vanNiekerk@kaust.edu.sa
                Statistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King
                Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Kingdom of Saudi
                Arabia[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mH ̊avard Rue
                haavard.rue@kaust.edu.sa
                Statistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King
                Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Kingdom of Saudi
                Arabia[0m

Box rectangle:  [32m(90.0, 726.3) -> (251.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Janet van Niekerk and H ̊avard Rue.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1405.html.[0m



=== Processing ../JMLR 2024/Manifold Learning by Mixture Models of VAEs for Inverse Problems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Manifold Learning by Mixture Models of VAEs for Inverse Problems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 3/23; Revised 2/24; Published 6/24[0m

Box rectangle:  [32m(138.3, 101.6) -> (473.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mManifold Learning by Mixture Models of VAEs
                for Inverse Problems[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGiovanni S. Alberti
                giovanni.alberti@unige.it
                MaLGa Center
                Department of Mathematics, Department of Excellence 2023–2027
                University of Genoa, Italy[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohannes Hertrich
                j.hertrich@ucl.ac.uk
                Department of Computer Science
                University College London,
                London, United Kingdom[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatteo Santacesaria
                matteo.santacesaria@unige.it
                MaLGa Center
                Department of Mathematics, Department of Excellence 2023–2027
                University of Genoa, Italy[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSilvia Sciutto
                silvia.sciutto@edu.unige.it
                MaLGa Center
                Department of Mathematics, Department of Excellence 2023–2027
                University of Genoa, Italy[0m

Box rectangle:  [32m(90.0, 726.3) -> (414.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Giovanni S. Alberti, Johannes Hertrich, Matteo Santacesaria and Silvia Sciutto.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0396.html.[0m



=== Processing ../JMLR 2024/MAP- and MLE-Based Teaching.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/MAP- and MLE-Based Teaching.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-34
                Submitted 8/23; Revised 3/24; Published 3/24[0m

Box rectangle:  [32m(190.4, 99.7) -> (426.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMAP- and MLE-Based Teaching∗[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHans Ulrich Simon
                hsimon@mpi-inf.mpg.de
                Max-Planck Institute for Informatics, Germany
                and Ruhr-University Bochum, Department of Mathematics, Germany[0m

Box rectangle:  [32m(90.0, 177.1) -> (522.0, 202.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJan Arne Telle
                telle@ii.uib.no
                Department of Informatics, University of Bergen, Norway[0m

Box rectangle:  [32m(90.0, 726.3) -> (239.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hans Simon and Jan Arne Telle.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1086.html.[0m



=== Processing ../JMLR 2024/Margin-Based Active Learning of Classifiers.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Margin-Based Active Learning of Classifiers.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 10/22; Revised 11/23; Published 4/24[0m

Box rectangle:  [32m(113.3, 99.6) -> (503.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMargin-Based Active Learning of Multiclass Classifiers ∗[0m

Box rectangle:  [32m(89.4, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarco Bressan
                marco.bressan@unimi.it
                Dept. of Computer Science, Università degli Studi di Milano, Italy[0m

Box rectangle:  [32m(89.0, 163.5) -> (522.0, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicolò Cesa-Bianchi
                nicolo.cesa-bianchi@unimi.it
                Dept. of Computer Science, Università degli Studi di Milano, Italy
                & Politecnico di Milano, Italy[0m

Box rectangle:  [32m(88.8, 205.2) -> (522.0, 229.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSilvio Lattanzi
                silviol@google.com
                Google Research[0m

Box rectangle:  [32m(89.4, 236.4) -> (522.0, 262.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrea Paudice
                andrea.paudice@unimi.it
                Dept. of Computer Science, Università degli Studi di Milano, Italy[0m

Box rectangle:  [32m(90.0, 726.3) -> (378.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Marco Bressan, Nicolò Cesa-Bianchi, Silvio Lattanzi, Andrea Paudice.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1127.html.[0m



=== Processing ../JMLR 2024/Materials Discovery using Max K-Armed Bandit.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Materials Discovery using Max K-Armed Bandit.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 2/22; Revised 10/23; Published 3/24[0m

Box rectangle:  [32m(133.3, 101.6) -> (478.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaterials Discovery using Max K-Armed Bandit[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNobuaki Kikkawa
                kikkawa@mosk.tytlabs.co.jp
                Toyota Central R&D Labs., Inc.
                41-1, Yokomichi, Nagakute, Aichi 480-1192, Japan[0m

Box rectangle:  [32m(90.0, 177.1) -> (522.0, 216.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHiroshi Ohno
                oono-h@mosk.tytlabs.co.jp
                Toyota Central R&D Labs., Inc.
                41-1, Yokomichi, Nagakute, Aichi 480-1192, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (254.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nobuaki Kikkawa and Hiroshi Ohno.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0186.html.[0m



=== Processing ../JMLR 2024/Mathematical Framework for Online Social Media Auditing.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Mathematical Framework for Online Social Media Auditing.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 10/22; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(95.1, 101.6) -> (517.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMathematical Framework for Online Social Media Auditing[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWasim Huleihel
                wasimh@tauex.tau.ac.il
                Department of Electrical Engineering-Systems
                Tel Aviv University
                Tel Aviv 6997801, Israel[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYehonathan Refael
                refaelkalim@mail.tau.ac.il
                Department of Electrical Engineering-Systems
                Tel Aviv University
                Tel Aviv 6997801, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (253.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Wasim Huleihel, Yehonathan Refael.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1112.html.[0m



=== Processing ../JMLR 2024/Matryoshka Policy Gradient for Entropy-Regularized RL  Convergence and Global Optimality.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Matryoshka Policy Gradient for Entropy-Regularized RL  Convergence and Global Optimality.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 7/23; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(102.2, 101.6) -> (509.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatryoshka Policy Gradient for Entropy-Regularized RL:
                Convergence and Global Optimality[0m

Box rectangle:  [32m(90.0, 151.6) -> (522.0, 235.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFran ̧cois G. Ged1,2
                fged.math@gmail.com
                1Chair of Statistical Field Theory
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne
                Lausanne, Switzerland
                2Dynamical Systems in Biomathematics
                University of Vienna
                Vienna, Austria[0m

Box rectangle:  [32m(90.0, 242.9) -> (522.0, 295.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaria Han Veiga
                hanveiga.1@osu.edu
                Department of Mathematics
                The Ohio State University
                Columbus, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (263.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Fran ̧cois G. Ged and Maria Han Veiga.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0879.html.[0m



=== Processing ../JMLR 2024/Mean-Field Approximation of Cooperative Constrained Multi-Agent Reinforcement Learning (CMARL).pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Mean-Field Approximation of Cooperative Constrained Multi-Agent Reinforcement Learning (CMARL).pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 8/22; Revised 7/23; Published 3/24[0m

Box rectangle:  [32m(113.9, 101.5) -> (498.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMean-Field Approximation of Cooperative Constrained
                Multi-Agent Reinforcement Learning (CMARL)[0m

Box rectangle:  [32m(88.2, 151.8) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWashim Uddin Mondal
                wmondal@purdue.edu
                Lyles School of Civil Engineering,
                School of Industrial Engineering,
                Purdue University,
                West Lafayette, IN, 47907, USA[0m

Box rectangle:  [32m(88.2, 217.3) -> (522.0, 277.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVaneet Aggarwal
                vaneet@purdue.edu
                School of Industrial Engineering,
                School of Electrical and Computer Engineering,
                Purdue University,
                West Lafayette, IN, 47907, USA[0m

Box rectangle:  [32m(88.2, 284.5) -> (522.0, 337.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSatish V. Ukkusuri
                sukkusur@purdue.edu
                Lyles School of Civil Engineering,
                Purdue University,
                West Lafayette, IN, 47907, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (350.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Washim Uddin Mondal, Vaneet Aggarwal, Satish V. Ukkusuri.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0956.html.[0m



=== Processing ../JMLR 2024/Mean-Field Games With Finitely Many Players  Independent Learning and Subjectivity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Mean-Field Games With Finitely Many Players  Independent Learning and Subjectivity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-69
                Submitted 10/22; Revised 11/23; Published 10/24[0m

Box rectangle:  [32m(135.0, 101.6) -> (477.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMean-Field Games With Finitely Many Players:
                Independent Learning and Subjectivity[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBora Yongacoglu
                bora.yongacoglu@utoronto.ca
                Department of Electrical and Computer Engineering
                University of Toronto[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mG ̈urdal Arslan
                gurdal@hawaii.edu
                Department of Electrical Engineering
                University of Hawaii at Manoa[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSerdar Y ̈uksel
                yuksel@queensu.ca
                Department of Mathematics and Statistics
                Queen’s University[0m

Box rectangle:  [32m(90.0, 726.3) -> (315.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bora Yongacoglu, G ̈urdal Arslan, and Serdar Y ̈uksel.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1207.html.[0m



=== Processing ../JMLR 2024/Mean-Square Analysis of Discretized Itô Diffusions for Heavy-tailed Sampling.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Mean-Square Analysis of Discretized Itô Diffusions for Heavy-tailed Sampling.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 10/22; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(125.7, 101.6) -> (486.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMean-Square Analysis of Discretized Itˆo Diffusions
                for Heavy-tailed Sampling[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYe He
                leohe@ucdavis.edu
                Department of Mathematics
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTyler Farghly
                farghly@stats.ox.ac.uk
                Department of Statistics
                University of Oxford
                Oxford, OX1 2JD, United Kingdom[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKrishnakumar Balasubramanian
                kbala@ucdavis.edu
                Department of Statistics
                University of California
                Davis, CA 95616, USA[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMurat A. Erdogdu
                erdogdu@cs.toronto.edu
                Department of Computer Science & Department of Statistics
                University of Toronto
                Toronto, ON M5S 3G4, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (397.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ye He, Tyler Farghly, Krishnakumar Balasubramanian, Murat A. Erdogdu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1198.html.[0m



=== Processing ../JMLR 2024/Measuring Sample Quality in Algorithms for Intractable Normalizing Function Problems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Measuring Sample Quality in Algorithms for Intractable Normalizing Function Problems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 6/23; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(92.4, 101.7) -> (519.6, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMeasuring Sample Quality in Algorithms for Intractable Normalizing
                Function Problems[0m

Box rectangle:  [32m(90.0, 153.4) -> (521.8, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBokgyeong Kang
                BOKGYEONG.KANG@DUKE.EDU
                Department of Statistical Science
                Duke University
                Durham, NC 27708, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (521.8, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohn Hughes
                JOH621@LEHIGH.EDU
                College of Health
                Lehigh University
                Bethlehem, PA 18015, USA[0m

Box rectangle:  [32m(90.0, 262.1) -> (521.8, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMurali Haran
                MURALI.HARAN@PSU.EDU
                Department of Statistics
                The Pennsylvania State University
                University Park, PA 16802, USA[0m

Box rectangle:  [32m(90.0, 726.4) -> (280.4, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bokgyeong Kang, John Hughes, and Murali Haran.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0810.html.[0m



=== Processing ../JMLR 2024/Memorization With Neural Nets  Going Beyond the Worst Case.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Memorization With Neural Nets  Going Beyond the Worst Case.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 10/23; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(188.2, 101.6) -> (423.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMemorization With Neural Nets:
                Going Beyond the Worst Case[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSjoerd Dirksen
                s.dirksen@uu.nl
                Mathematical Institute
                Utrecht University
                3584 CD Utrecht, Netherlands[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPatrick Finke
                p.g.finke@uu.nl
                Mathematical Institute
                Utrecht University
                3584 CD Utrecht, Netherlands[0m

Box rectangle:  [32m(90.0, 260.4) -> (522.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartin Genzel∗
                martin.genzel@merantix-momentum.com
                Merantix Momentum GmbH
                13355 Berlin, Germany[0m

Box rectangle:  [32m(90.0, 726.5) -> (303.3, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sjoerd Dirksen, Patrick Finke and Martin Genzel.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1376.html.[0m



=== Processing ../JMLR 2024/Memory-Efficient Sequential Pattern Mining with Hybrid Tries.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Memory-Efficient Sequential Pattern Mining with Hybrid Tries.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-29
                Submitted 2/22; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(106.5, 101.5) -> (505.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMemory-Efficient Sequential Pattern Mining with Hybrid
                Tries[0m

Box rectangle:  [32m(88.2, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmin Hosseininasab
                amin.hosseininasab@warrington.ufl.edu
                Warrington College of Business
                University of Florida
                Gainesville, FL, USA[0m

Box rectangle:  [32m(88.4, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWillem-Jan van Hoeve
                vanhoeve@andrew.cmu.edu
                Tepper School of Business
                Carnegie Mellon University
                Pittsburgh, PA, USA[0m

Box rectangle:  [32m(88.3, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndre A. Cire
                andre.cire@rotman.utoronto.ca
                Rotman School of Management
                University of Toronto
                Toronto, ON, Canada[0m

Box rectangle:  [32m(90.0, 726.3) -> (358.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Amin Hosseininasab, Willem-Jan van Hoeve, and Andre A. Cire.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0125.html.[0m



=== Processing ../JMLR 2024/Memory of recurrent networks  Do we compute it right.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Memory of recurrent networks  Do we compute it right.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 5/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(92.9, 101.6) -> (519.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMemory of Recurrent Networks: Do We Compute It Right?[0m

Box rectangle:  [32m(90.0, 133.9) -> (521.9, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGiovanni Ballarin
                Giovanni.Ballarin@gess.uni-mannheim.de
                Department of Economics
                University of Mannheim
                Germany[0m

Box rectangle:  [32m(90.0, 187.5) -> (521.9, 271.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLyudmila Grigoryeva
                Lyudmila.Grigoryeva@unisg.ch
                Faculty of Mathematics and Statistics
                Universit ̈at Sankt Gallen
                Switzerland
                Department of Statistics (Honorary Assoc. Prof.)
                University of Warwick
                United Kingdom[0m

Box rectangle:  [32m(90.0, 278.5) -> (522.0, 344.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJuan-Pablo Ortega
                Juan-Pablo.Ortega@ntu.edu.sg
                Division of Mathematical Sciences
                School of Physical and Mathematical Sciences
                Nanyang Technological University
                Singapore[0m

Box rectangle:  [32m(90.0, 726.3) -> (361.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Giovanni Ballarin, Lyudmila Grigoryeva, and Juan-Pablo Ortega.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0568.html.[0m



=== Processing ../JMLR 2024/Mentored Learning  Improving Generalization and Convergence of Student Learner.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Mentored Learning  Improving Generalization and Convergence of Student Learner.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 9/23; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(110.2, 96.4) -> (502.2, 128.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMentored Learning: Improving Generalization and
                Convergence of Student Learner via Teaching Feedback[0m

Box rectangle:  [32m(88.3, 141.7) -> (522.0, 466.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaofeng Cao( )∗
                xiaofeng.cao.uts@gmail.com
                School of Artificial Intelligence
                Jilin University, Changchun 130012, China
                and
                Australian Artifcial Intelligence Institute (AAII)
                University of Technology Sydney (UTS), NSW 2007, Australia
                Yaming Guo
                guoym21@mails.jlu.edu.cn
                School of Artificial Intelligence
                Jilin University, Changchun 130012, China
                Heng Tao Shen
                shenhengtao@hotmail.com
                School of Computer Science and Technology
                Tongji University, Shanghai 201804, China
                and
                School of Computer Science and Engineering
                University of Electronic Science and Technology of China, Chengdu 611731, China
                Ivor W. Tsang
                ivor tsang@ihpc.a-star.edu.sg
                Centre for Frontier AI Research and Institute of High Performance Computing
                Agency for Science, Technology, and Research(A*STAR), Singapore 138632, Singapore
                and
                College of Computing and Data Science
                Nanyang Technological University, Singapore 639798, Singapore
                James T. Kwok
                jamesk@cse.ust.hk
                Department of Computer Science and Engineering
                The Hong Kong University of Science and Technology, Hong Kong SAR 999077, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (420.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xiaofeng Cao, Yaming Guo, Heng Tao Shen, Ivor W. Tsang and James T. Kwok.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1213.html.[0m



=== Processing ../JMLR 2024/Minimax Rates for High-Dimensional Random Tessellation Forests.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Minimax Rates for High-Dimensional Random Tessellation Forests.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 5/22; Revised 4/23; Published 3/24[0m

Box rectangle:  [32m(97.5, 101.6) -> (514.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMinimax Rates for High-Dimensional Random Tessellation
                Forests[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEliza O’Reilly
                eoreill2@jh.edu
                Applied Mathematics and Statistics Department
                Johns Hopkins University
                Baltimore, MD 21218, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNgoc Mai Tran
                ntran@math.utexas.edu
                Department of Mathematics
                University of Texas at Austin
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (248.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Eliza O’Reilly and Ngoc Mai Tran.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0673.html.[0m



=== Processing ../JMLR 2024/MLRegTest  A Benchmark for the Machine Learning of Regular Languages.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/MLRegTest  A Benchmark for the Machine Learning of Regular Languages.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 4/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(112.7, 101.5) -> (499.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMLRegTest: A Benchmark for the Machine Learning of
                Regular Languages[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSam van der Poel
                samvanderpoel@gatech.edu
                School of Mathematics
                Georgia Institute of Technology[0m

Box rectangle:  [32m(90.0, 193.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDakotah Lambert
                dakotahlambert@acm.org
                Department of Computer Science
                Haverford College[0m

Box rectangle:  [32m(90.0, 235.0) -> (521.9, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKalina Kostyszyn
                kalina.kostyszyn@stonybrook.edu
                Department of Linguistics &
                Institute of Advanced Computational Science
                Stony Brook University[0m

Box rectangle:  [32m(90.0, 288.6) -> (522.0, 336.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTiantian Gao
                tiagao@cs.stonybrook.edu
                Rahul Verma
                rxverma1@gmail.com
                Department of Computer Science
                Stony Brook University[0m

Box rectangle:  [32m(90.0, 342.2) -> (522.0, 414.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDerek Andersen
                derek.andersen@alumni.stonybrook.edu
                Joanne Chau
                choryan.chau@alumni.stonybrook.edu
                Emily Peterson
                emily.peterson@alumni.stonybrook.edu
                Cody St. Clair
                cody.stclair@alumni.stonybrook.edu
                Department of Linguistics
                Stony Brook University[0m

Box rectangle:  [32m(90.0, 419.7) -> (522.0, 455.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPaul Fodor
                pfodor@cs.stonybrook.edu
                Department of Computer Science
                Stony Brook University[0m

Box rectangle:  [32m(90.0, 461.3) -> (522.0, 509.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChihiro Shibata
                chihiro@hosei.ac.jp
                Department of Advanced Sciences
                Graduate School of Science and Engineering
                Hosei University[0m

Box rectangle:  [32m(90.0, 516.5) -> (522.0, 569.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeffrey Heinz
                jeffrey.heinz@stonybrook.edu
                Department of Linguistics &
                Institute of Advanced Computational Science
                Stony Brook University[0m

Box rectangle:  [32m(90.0, 726.3) -> (500.2, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sam van der Poel, Dakotah Lambert, Kalina Kostyszyn, Tiantian Gao, Rahul Verma, Derek Andersen,
                Joanne Chau, Emily Peterson, Cody St. Clair, Paul Fodor, Chihiro Shibata, and Jeffrey Heinz.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0518.html.[0m



=== Processing ../JMLR 2024/Model-Free Representation Learning and Exploration in Low-Rank MDPs.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Model-Free Representation Learning and Exploration in Low-Rank MDPs.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-76
                Submitted 6/22; Revised 4/23; Published 1/24[0m

Box rectangle:  [32m(111.4, 101.5) -> (500.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mModel-Free Representation Learning and Exploration in
                Low-Rank MDPs[0m

Box rectangle:  [32m(89.4, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAditya Modi ∗
                admodi@umich.edu
                Microsoft
                Mountain View, CA 94043, USA[0m

Box rectangle:  [32m(88.3, 193.1) -> (522.0, 241.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJinglin Chen ∗
                jinglinc@illinois.edu
                Department of Computer Science
                University of Illinois Urbana-Champaign
                Urbana, IL 61801, USA[0m

Box rectangle:  [32m(89.4, 247.0) -> (522.0, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAkshay Krishnamurthy
                akshaykr@microsoft.com
                Microsoft Research
                New York, NY 10011, USA[0m

Box rectangle:  [32m(88.3, 288.6) -> (522.0, 336.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNan Jiang
                nanjiang@illinois.edu
                Department of Computer Science
                University of Illinois Urbana-Champaign
                Urbana, IL 61801, USA[0m

Box rectangle:  [32m(88.8, 343.8) -> (522.0, 369.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlekh Agarwal
                alekhagarwal@google.com
                Google Research[0m

Box rectangle:  [32m(90.0, 726.3) -> (422.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Aditya Modi*, Jinglin Chen*, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0687.html.[0m



=== Processing ../JMLR 2024/Modeling Random Networks with Heterogeneous Reciprocity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Modeling Random Networks with Heterogeneous Reciprocity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 11/22; Revised 8/23; Published 1/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mModeling Random Networks with Heterogeneous Reciprocity[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel Cirkovic
                cirkovd@stat.tamu.edu
                Department of Statistics
                Texas A&M University
                College Station, TX 77843, USA[0m

Box rectangle:  [32m(90.0, 188.8) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTiandong Wang∗
                td wang@fudan.edu.cn
                Shanghai Center for Mathematical Sciences
                Fudan University
                Shanghai 200438, China[0m

Box rectangle:  [32m(90.0, 267.1) -> (201.9, 277.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Tina Eliassi-Rad[0m

Box rectangle:  [32m(280.3, 300.7) -> (331.7, 312.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 317.9) -> (504.6, 508.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mReciprocity, or the tendency of individuals to mirror behavior, is a key measure that de-
                scribes information exchange in a social network. Users in social networks tend to engage
                in different levels of reciprocal behavior. Differences in such behavior may indicate the
                existence of communities that reciprocate links at varying rates. In this paper, we de-
                velop methodology to model the diverse reciprocal behavior in growing social networks. In
                particular, we present a preferential attachment model with heterogeneous reciprocity that
                imitates the attraction users have for popular users, plus the heterogeneous nature by which
                they reciprocate links. We compare Bayesian and frequentist model fitting techniques for
                large networks, as well as computationally efficient variational alternatives. Cases where
                the number of communities is known and unknown are both considered. We apply the
                presented methods to the analysis of Facebook and Reddit networks where users have non-
                uniform reciprocal behavior patterns. The fitted model captures the heavy-tailed nature
                of the empirical degree distributions in the datasets and identifies multiple groups of users
                that differ in their tendency to reply to and receive responses to wallposts and comments.
                Keywords: Variational inference, community detection, preferential attachment, Bayesian
                methods[0m

Box rectangle:  [32m(90.0, 528.9) -> (176.6, 540.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 550.9) -> (522.1, 683.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mA frequent goal in the statistical inference of social networks is to develop models that
                adequately capture and quantify common types of user interaction. One such feature is the
                propensity of users to generate links with other users that already have attracted a large
                number of links (Newman, 2001; Jeong et al., 2003). To model this “rich get richer” self-
                organizing feature of nodes in a growing network, Barab ́asi and Albert (1999) developed
                the preferential attachment (PA) model. The classical preferential attachment model posits
                that as users enter a growing network, they connect with other users with probability pro-
                portional to their degree. This simple mechanism produces power-law degree distributions,
                yet another feature of many real-world networks (Mislove et al., 2007). Since its inception,
                many generalizations of the preferential attachment model have been developed to capture[0m

Box rectangle:  [32m(93.7, 695.7) -> (256.9, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. T. Wang is the corresponding author.[0m

Box rectangle:  [32m(90.0, 726.4) -> (257.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Daniel Cirkovic and Tiandong Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1317.html.[0m



=== Processing ../JMLR 2024/Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 10/22; Revised 7/23; Published 1/24[0m

Box rectangle:  [32m(108.2, 102.0) -> (503.7, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMonotonic Risk Relationships under Distribution Shifts[0m

Box rectangle:  [32m(183.8, 120.0) -> (428.2, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mfor Regularized Risk Minimization[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel LeJeune
                daniel@dlej.net
                Department of Statistics
                Stanford University
                Stanford, CA 94305-4020, USA[0m

Box rectangle:  [32m(90.0, 208.8) -> (522.0, 273.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiayu Liu
                jiayu.liu@tum.de
                Reinhard Heckel
                reinhard.heckel@tum.de
                Department of Electrical and Computer Engineering
                Technical University of Munich
                80333 Munich, DE[0m

Box rectangle:  [32m(90.0, 298.9) -> (177.3, 308.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Daniel Hsu[0m

Box rectangle:  [32m(280.3, 332.5) -> (331.7, 344.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 349.4) -> (502.1, 504.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMachine learning systems are often applied to data that is drawn from a different distribu-
                tion than the training distribution. Recent work has shown that for a variety of classification
                and signal reconstruction problems, the out-of-distribution performance is strongly linearly
                correlated with the in-distribution performance. If this relationship or more generally a
                monotonic one holds, it has important consequences. For example, it allows to optimize
                performance on one distribution as a proxy for performance on the other. In this paper,
                we study conditions under which a monotonic relationship between the performances of
                a model on two distributions is expected. We prove an exact asymptotic linear relation
                for squared error and a monotonic relation for misclassification error for ridge-regularized
                general linear models under covariate shift, as well as an approximate linear relation for
                linear inverse problems.
                Keywords:
                distribution shifts, asymptotics, empirical risk minimization, general linear
                models, inverse problems[0m

Box rectangle:  [32m(90.0, 523.9) -> (180.3, 535.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 545.5) -> (522.0, 651.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMachine learning models are typically evaluated by shuffling a set of labeled data, splitting
                it into training and test sets, and evaluating the model trained on the training set on
                the test set. This measures how well the model performs on the distribution the model
                was trained on.
                However, in practice a model is most commonly not applied to such
                in-distribution data, but rather to out-of-distribution data that is almost always at least
                slightly different. In order to understand the performance of machine learning methods
                in practice, it is therefore important to understand how out-of-distribution performance
                relates to in-distribution performance.[0m

Box rectangle:  [32m(90.0, 654.0) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhile there are settings in which models with similar in-distribution performance have
                different out-of-distribution performance (McCoy et al., 2020), a series of recent empirical
                studies have shown that often, the in-distribution and out-of-distribution performances of
                models are strongly correlated:[0m

Box rectangle:  [32m(90.0, 726.5) -> (300.3, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Daniel LeJeune, Jiayu Liu, and Reinhard Heckel.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1197.html.[0m



=== Processing ../JMLR 2024/More Efficient Estimation of Multivariate Additive Models Based on Tensor Decomposition and Penalization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/More Efficient Estimation of Multivariate Additive Models Based on Tensor Decomposition and Penalization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.5) -> (522.0, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 5/22; Revised 4/23; Published 5/24[0m

Box rectangle:  [32m(97.8, 101.3) -> (514.2, 133.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMore Efficient Estimation of Multivariate Additive Models
                Based on Tensor Decomposition and Penalization[0m

Box rectangle:  [32m(90.0, 153.1) -> (522.0, 223.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXu Liu
                liu.xu@sufe.edu.cn
                School of Statistics and Management
                Shanghai University of Finance and Economics, Shanghai, China
                and
                Yunnan Key Laboratory of Statistical Modeling and Data Analysis
                Yunnan University, Kunming, China[0m

Box rectangle:  [32m(90.0, 242.6) -> (522.0, 313.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHeng Lian
                henglian@cityu.edu.hk
                City University of Hong Kong Shenzhen Research Institute
                Shenzhen, China
                and
                Department of Mathematics
                City University of Hong Kong, Kowloon Tong, Hong Kong, China[0m

Box rectangle:  [32m(90.0, 321.6) -> (522.0, 359.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJian Huang
                j.huang@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University, Hong Kong, China[0m

Box rectangle:  [32m(90.0, 384.7) -> (240.6, 394.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Animashree Anandkumar[0m

Box rectangle:  [32m(280.3, 418.3) -> (331.7, 430.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 438.2) -> (502.1, 591.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe consider parsimonious modeling of high-dimensional multivariate additive models using
                regression splines, with or without sparsity assumptions. The approach is based on treating
                the coefficients in the spline expansions as a third-order tensor. Note the data does not have
                tensor predictors or tensor responses, which distinguishes our study from the existing ones.
                A Tucker decomposition is used to reduce the number of parameters in the tensor. We also
                combined the Tucker decomposition with penalization to enable variable selection. The
                proposed method can avoid the statistical inefficiency caused by estimating a large number
                of nonparametric functions. We provide sufficient conditions under which the proposed
                tensor-based estimators achieve the optimal rate of convergence for the nonparametric
                regression components. We conduct simulation studies to demonstrate the effectiveness of
                the proposed novel approach in fitting high-dimensional multivariate additive models and
                illustrate its application on a breast cancer copy number variation and gene expression data
                set.[0m

Box rectangle:  [32m(109.9, 597.8) -> (502.1, 619.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                High dimensionality; Sparse models; Splines; Tensor estimation; Tucker
                decomposition.[0m

Box rectangle:  [32m(90.0, 642.4) -> (176.5, 654.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 667.0) -> (522.0, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLinear regression is one of the oldest and the most popular statistical tools used for relating
                predictors to a continuous response. It imposes the strict assumption that the effect between
                any predictor and the response is linear, which may not be satisfied in some applications[0m

Box rectangle:  [32m(90.0, 726.3) -> (249.3, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Xu Liu, Heng Lian and Jian Huang.[0m

Box rectangle:  [32m(90.0, 740.8) -> (516.9, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0578.html.[0m



=== Processing ../JMLR 2024/More PAC-Bayes bounds  From bounded losses  to losses with general tail behaviors  to anytime validity.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/More PAC-Bayes bounds  From bounded losses  to losses with general tail behaviors  to anytime validity.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-43
                Submitted 10/23; Revised 3/24; Published 3/24[0m

Box rectangle:  [32m(90.0, 101.5) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMore PAC-Bayes bounds: From bounded losses, to losses with
                general tail behaviors, to anytime validity[0m

Box rectangle:  [32m(89.4, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBorja Rodríguez-Gálvez
                borjarg@kth.se
                Division of Information Science and Engineering (ISE)
                KTH Royal Institute of Technology
                Stockholm, Sweden[0m

Box rectangle:  [32m(89.4, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRagnar Thobaben
                ragnart@kth.se
                Division of Information Science and Engineering (ISE)
                KTH Royal Institute of Technology
                Stockholm, Sweden[0m

Box rectangle:  [32m(89.4, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMikael Skoglund
                skoglund@kth.se
                Division of Information Science and Engineering (ISE)
                KTH Royal Institute of Technology
                Stockholm, Sweden[0m

Box rectangle:  [32m(90.0, 726.3) -> (364.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Borja Rodríguez-Gálvez, Ragnar Thobaben, and Mikael Skoglund.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1360.html.[0m



=== Processing ../JMLR 2024/Multi-class Probabilistic Bounds for Majority Vote Classifiers with Partially Labeled Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Multi-class Probabilistic Bounds for Majority Vote Classifiers with Partially Labeled Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 1/23; Revised 8/23; Published 3/24[0m

Box rectangle:  [32m(125.7, 101.6) -> (486.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMulti-class Probabilistic Bounds for Majority Vote
                Classifiers with Partially Labeled Data[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVasilii Feofanov
                vasilii.feofanov@huawei.com
                Univ. Grenoble Alpes / Huawei Noah’s Ark Lab
                92100 Boulogne-Billancourt, France[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmilie Devijver
                emilie.devijver@univ-grenoble-alpes.fr
                Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
                38000 Grenoble, France[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 275.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMassih-Reza Amini
                massih-reza.amini@univ-grenoble-alpes.fr
                Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
                38000 Grenoble, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (332.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Vasilii Feofanov, Emilie Devijver and Massih-Reza Amini.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0121.html.[0m



=== Processing ../JMLR 2024/Multi-Objective Neural Architecture Search by Learning Search Space Partitions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Multi-Objective Neural Architecture Search by Learning Search Space Partitions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 8/23; Revised 5/24; Published 6/24[0m

Box rectangle:  [32m(170.9, 101.7) -> (441.1, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMulti-Objective Neural Architecture Search
                by Learning Search Space Partitions[0m

Box rectangle:  [32m(89.4, 153.4) -> (521.8, 176.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYiyang Zhao
                YZHAO10@WPI.EDU
                WORCESTER POLYTECHNIC INSTITUTE[0m

Box rectangle:  [32m(90.0, 195.0) -> (521.7, 217.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLinnan Wang
                WANGNAN318@GMAIL.COM
                BROWN UNIVERSITY[0m

Box rectangle:  [32m(89.6, 238.2) -> (521.8, 262.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTian Guo
                TIAN@WPI.EDU
                WORCESTER POLYTECHNIC INSTITUTE[0m

Box rectangle:  [32m(90.0, 726.4) -> (254.3, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yiyang Zhao, Linnan Wang, and Tian Guo.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1013.html.[0m



=== Processing ../JMLR 2024/Multiple Descent in the Multiple Random Feature Model.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Multiple Descent in the Multiple Random Feature Model.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 12/22; Revised 9/23; Published 1/24[0m

Box rectangle:  [32m(102.5, 101.6) -> (509.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMultiple Descent in the Multiple Random Feature Model[0m

Box rectangle:  [32m(90.0, 134.5) -> (522.0, 171.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuran Meng
                xuranmeng@connect.hku.hk
                Department of Statistics and Actuarial Science
                The University of Hong Kong[0m

Box rectangle:  [32m(90.0, 177.9) -> (522.0, 215.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJianfeng Yao
                jeffyao@cuhk.edu.cn
                School of Data Science
                The Chinese University of Hong Kong (Shenzhen)[0m

Box rectangle:  [32m(90.0, 223.0) -> (522.0, 263.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuan Cao
                yuancao@hku.hk
                Department of Statistics and Actuarial Science
                The University of Hong Kong[0m

Box rectangle:  [32m(90.0, 726.3) -> (276.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xuran Meng, Jianfeng Yao and Yuan Cao.[0m

Box rectangle:  [32m(90.0, 741.4) -> (517.0, 759.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1389.html.[0m



=== Processing ../JMLR 2024/Multi-Response Linear Discriminant Analysis in High Dimensions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Multi-Response Linear Discriminant Analysis in High Dimensions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 7/23; Revised 8/24; Published 11/24[0m

Box rectangle:  [32m(115.7, 101.6) -> (496.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMulti-Response Linear Discriminant Analysis in High
                Dimensions[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKai Deng
                kdeng@fsu.edu
                Xin Zhang
                henry@stat.fsu.edu
                Department of Statistics
                Florida State University
                Tallahassee, Florida, USA[0m

Box rectangle:  [32m(90.0, 219.0) -> (522.0, 271.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAaron J. Molstad
                amolstad@umn.edu
                School of Statistics
                University of Minnesota
                Minneapolis, Minnesota, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (286.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kai Deng, Xin Zhang, and Aaron J. Molstad.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0961.html.[0m



=== Processing ../JMLR 2024/Nearest Neighbor Sampling for Covariate Shift Adaptation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nearest Neighbor Sampling for Covariate Shift Adaptation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 6/24; Revised 11/24; Published 1/25[0m

Box rectangle:  [32m(101.6, 101.5) -> (510.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNearest Neighbor Sampling for Covariate Shift Adaptation[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 169.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrançois Portier
                francois.portier@ensai.fr
                Department of Statistics,
                Univ Rennes, Ensai, CNRS, CREST—UMR 9194, F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 175.5) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLionel Truquet
                lionel.truquet@ensai.fr
                Department of Statistics,
                Univ Rennes, Ensai, CNRS, CREST—UMR 9194, F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 218.7) -> (522.0, 258.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIkko Yamane
                ikko.yamane@ensai.fr
                Department of Computer Science,
                Univ Rennes, Ensai, CNRS, CREST—UMR 9194, F-35000 Rennes, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (310.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 François Portier, Lionel Truquet, and Ikko Yamane.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0890.html.[0m



=== Processing ../JMLR 2024/Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Near-Optimal Algorithms for Making the Gradient Small in Stochastic Minimax Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 10/22; Published 12/24[0m

Box rectangle:  [32m(93.6, 101.6) -> (518.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNear-Optimal Algorithms for Making the Gradient Small in
                Stochastic Minimax Optimization[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLesi Chen
                chenlc23@mails.edu.cn
                Institute for Interdisciplinary Information Sciences, Tsinghua University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 194.8) -> (522.0, 275.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLuo Luo∗
                luoluo@fudan.edu.cn
                School of Data Science, Fudan University
                Shanghai, China
                and
                Shanghai Key Laboratory for Contemporary Applied Mathematics
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (208.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lesi Chen and Luo Luo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1126.html.[0m



=== Processing ../JMLR 2024/Neural Bayes estimators for censored inference with peaks-over-threshold models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Neural Bayes estimators for censored inference with peaks-over-threshold models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 9/23; Revised 6/24; Published 12/24[0m

Box rectangle:  [32m(121.5, 101.6) -> (490.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Bayes estimators for censored inference with
                peaks-over-threshold models[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJordan Richards
                jordan.richards@ed.ac.uk
                School of Mathematics and Maxwell Institute for Mathematical Sciences,
                University of Edinburgh,
                Edinburgh, United Kingdom.[0m

Box rectangle:  [32m(90.0, 207.6) -> (504.2, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStatistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division,
                King Abdullah University of Science and Technology (KAUST),
                Thuwal, 23955-6900, Saudi Arabia.[0m

Box rectangle:  [32m(90.0, 247.1) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew Sainsbury-Dale
                matthew.sainsburydale@kaust.edu.sa
                Statistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division,
                King Abdullah University of Science and Technology (KAUST),
                Thuwal, 23955-6900, Saudi Arabia.[0m

Box rectangle:  [32m(90.0, 302.8) -> (289.4, 336.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSchool of Mathematics and Applied Statistics,
                University of Wollongong,
                Wollongong, Australia.[0m

Box rectangle:  [32m(90.0, 342.3) -> (522.0, 390.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrew Zammit-Mangion
                azm@uow.edu.au
                School of Mathematics and Applied Statistics,
                University of Wollongong,
                Wollongong, Australia.[0m

Box rectangle:  [32m(90.0, 397.4) -> (522.0, 450.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRapha ̈el Huser
                raphael.huser@kaust.edu.sa
                Statistics Program, Computer, Electrical and Mathematical Sciences and Engineering Division,
                King Abdullah University of Science and Technology (KAUST),
                Thuwal, 23955-6900, Saudi Arabia.[0m

Box rectangle:  [32m(90.0, 726.3) -> (381.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Richards, J., Sainsbury-Dale, M., Zammit-Mangion, A., and Huser, R..[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1134.html.[0m



=== Processing ../JMLR 2024/Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.5) -> (522.0, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 9/23; Published 5/24[0m

Box rectangle:  [32m(105.9, 101.3) -> (506.1, 133.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Collapse for Unconstrained Feature Model under
                Cross-entropy Loss with Imbalanced Data[0m

Box rectangle:  [32m(90.0, 153.0) -> (522.0, 201.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWanli Hong ∗†
                wh992@nyu.edu
                Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
                New York University Shanghai
                Shanghai, 200124, China[0m

Box rectangle:  [32m(90.0, 215.1) -> (210.2, 248.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCenter for Data Science
                New York University
                New York, NY 10011, USA[0m

Box rectangle:  [32m(90.0, 256.0) -> (522.0, 308.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShuyang Ling∗
                sl3635@nyu.edu
                Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning
                New York University Shanghai
                Shanghai, 200124, China[0m

Box rectangle:  [32m(90.0, 347.6) -> (202.8, 357.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Maxim Raginsky[0m

Box rectangle:  [32m(280.3, 383.2) -> (331.7, 395.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 401.4) -> (502.1, 605.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Collapse (NC) is a fascinating phenomenon that arises during the terminal phase of
                training (TPT) of deep neural networks (DNNs). Specifically, for balanced training datasets
                (each class shares the same number of samples), it is observed that the feature vectors of
                samples from the same class converge to their corresponding in-class mean features and their
                pairwise angles are the same. In this paper, we study the extension of NC phenomenon to
                imbalanced datasets under cross-entropy loss function in the context of the unconstrained
                feature model (UFM). Our contribution is multi-fold compared with the state-of-the-art
                results: (a) we show that the feature vectors within the same class still collapse to the
                same mean vector; (b) the mean feature vectors no longer share the same pairwise angle.
                Instead, those angles depend on sample sizes; (c) we also characterize the sharp threshold
                on which the minority collapse (the feature vectors of the minority groups collapse to one
                single vector) will happen; (d) finally, we argue that the effect of the imbalance in datasets
                diminishes as the sample size grows. Our results provide a complete picture of the NC
                under the cross-entropy loss for imbalanced datasets. Numerical experiments confirm our
                theories.
                Keywords:
                Neural Collapse, Minority Collapse, Unconstrained Feature Model, Singular
                Value Thresholding[0m

Box rectangle:  [32m(93.7, 618.9) -> (522.0, 704.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning, New York University
                Shanghai, China. S.L. and W.H. are (partially) financially supported by the National Key R&D Program
                of China, Project Number 2021YFA1002800, National Natural Science Foundation of China (NSFC)
                No.12001372, Shanghai Municipal Education Commission (SMEC) via Grant 0920000112, and NYU
                Shanghai Boost Fund. W.H. is also supported by NYU Shanghai Ph.D. fellowship and acknowledges the
                NSF/NRT support.
                †. Center for Data Science, New York University.
                Correspondence to: Shuyang Ling (sl3635@nyu.edu)[0m

Box rectangle:  [32m(90.0, 726.3) -> (231.4, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Wanli Hong and Shuyang Ling.[0m

Box rectangle:  [32m(90.0, 740.8) -> (516.9, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1215.html.[0m



=== Processing ../JMLR 2024/Neural Feature Learning in Function Space.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Neural Feature Learning in Function Space.pdf') ---[0m

Box rectangle:  [32m(72.0, 23.1) -> (539.9, 31.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-76
                Submitted 9/23; Revised 1/24; Published 5/24[0m

Box rectangle:  [32m(153.3, 81.0) -> (463.7, 98.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Feature Learning in Function Space∗[0m

Box rectangle:  [32m(72.0, 115.1) -> (536.2, 127.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiangxiang Xu
                xuxx@mit.edu[0m

Box rectangle:  [32m(72.0, 134.4) -> (540.0, 146.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLizhong Zheng
                lizhong@mit.edu[0m

Box rectangle:  [32m(72.0, 161.1) -> (335.2, 198.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDepartment of Electrical Engineering and Computer Science
                Massachusetts Institute of Technology
                Cambridge, MA 02139-4307, USA[0m

Box rectangle:  [32m(72.0, 223.4) -> (190.2, 233.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Kilian Weinberger[0m

Box rectangle:  [32m(280.3, 258.9) -> (331.7, 270.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(91.9, 276.1) -> (520.2, 418.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe present a novel framework for learning system design with neural feature extractors. First, we
                introduce the feature geometry, which unifies statistical dependence and feature representations in
                a function space equipped with inner products. This connection defines function-space concepts on
                statistical dependence, such as norms, orthogonal projection, and spectral decomposition, exhibiting
                clear operational meanings. In particular, we associate each learning setting with a dependence
                component and formulate learning tasks as finding corresponding feature approximations.
                We
                propose a nesting technique, which provides systematic algorithm designs for learning the optimal
                features from data samples with off-the-shelf network architectures and optimizers. We further
                demonstrate multivariate learning applications, including conditional inference and multimodal
                learning, where we present the optimal features and reveal their connections to classical approaches.
                Keywords:
                Feature geometry, information processing, neural feature learning, nesting technique,
                multivariate dependence decomposition[0m

Box rectangle:  [32m(72.0, 438.2) -> (162.3, 450.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(72.0, 460.1) -> (540.1, 620.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning useful feature representations from data observations is a fundamental task in machine
                learning. Early developments of such algorithms focused on learning optimal linear features, e.g.,
                linear regression, PCA (Principal Component Analysis) (Pearson, 1901), CCA (Canonical Corre-
                lation Analysis) (Hotelling, 1936), and LDA (Linear Discriminant Analysis) (Fisher, 1936). The
                resulting algorithms admit straightforward implementations, with well-established connections be-
                tween learned features and statistical behaviors of data samples. However, practical learning appli-
                cations often involve data with complex structures which linear features fail to capture. To address
                such problems, practitioners employ more complicated feature designs and build inference models
                based on these features, e.g., kernel methods (Cortes and Vapnik, 1995; Hofmann et al., 2008) and
                deep neural networks (LeCun et al., 2015). The feature representations serve as the information
                carrier, capturing useful information from data for subsequent processing. An illustration of such
                feature-centric learning systems is shown in Figure 1, which consists of two parts:[0m

Box rectangle:  [32m(85.3, 631.6) -> (540.1, 669.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. A learning module which generates a collection of features from the data. Data can take
                different forms, for example, input-output pairs1 or some tuples. The features can be either
                specified implicitly, e.g., by a kernel function in kernel methods, or explicitly parameterized[0m

Box rectangle:  [32m(75.7, 679.9) -> (540.1, 722.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. This work was presented in part at 2022 58th Annual Allerton Conference on Communication, Control, and
                Computing (Allerton), Monticello, IL, USA, Sep. 2022 (Xu and Zheng, 2022).
                1. In literature, the input variables are sometimes referred to as independent/predictor variables, and the output
                variables are also referred to as dependent/response/target variables.[0m

Box rectangle:  [32m(72.0, 743.5) -> (233.2, 751.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Xiangxiang Xu and Lizhong Zheng.[0m

Box rectangle:  [32m(72.0, 758.4) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1202.html.[0m



=== Processing ../JMLR 2024/Neural Hilbert Ladders  Multi-Layer Neural Networks in Function Space.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Neural Hilbert Ladders  Multi-Layer Neural Networks in Function Space.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-65
                Submitted 9/23; Revised 3/24; Published 3/24[0m

Box rectangle:  [32m(107.6, 101.5) -> (504.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Hilbert Ladders: Multi-Layer Neural Networks in
                Function Space[0m

Box rectangle:  [32m(88.8, 153.4) -> (522.0, 192.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhengdao Chen
                zhengdao.c3@gmail.com
                Google Research
                Mountain View, CA 94043[0m

Box rectangle:  [32m(90.0, 726.3) -> (178.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zhengdao Chen.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1225.html.[0m



=== Processing ../JMLR 2024/Neural Networks with Sparse Activation Induced by Large Bias  Tighter Analysis with Bias-Generalized NTK.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Neural Networks with Sparse Activation Induced by Large Bias  Tighter Analysis with Bias-Generalized NTK.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 6/23; Revised 4/24; Published 8/24[0m

Box rectangle:  [32m(98.0, 101.6) -> (514.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural Networks with Sparse Activation Induced by Large
                Bias: Tighter Analysis with Bias-Generalized NTK[0m

Box rectangle:  [32m(88.4, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHongru Yang
                hy6385@utexas.edu
                Department of Computer Science
                The University of Texas at Austin
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(89.4, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZiyu Jiang
                jiangziyu@tamu.edu
                NEC Labs America
                San Jose, CA 95110, USA[0m

Box rectangle:  [32m(88.3, 247.1) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuizhe Zhang
                rzzhang@berkeley.edu
                Simons Institute for the Theory of Computing
                University of California, Berkeley
                Berkeley, CA 94720, USA[0m

Box rectangle:  [32m(88.4, 300.6) -> (522.0, 348.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYingbin Liang
                liang.889@osu.edu
                Department of Electrical and Computer Engineering
                The Ohio State University
                Columbus, OH 43210, USA[0m

Box rectangle:  [32m(88.4, 355.8) -> (522.0, 408.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhangyang Wang
                atlaswang@utexas.edu
                Department of Electrical and Computer Engineering
                The University of Texas at Austin
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (415.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hongru Yang, Ziyu Jiang, Ruizhe Zhang, Yingbin Liang, and Zhangyang Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0831.html.[0m



=== Processing ../JMLR 2024/Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-34
                Submitted 11/21; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(94.2, 101.6) -> (518.0, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonasymptotic analysis of Stochastic Gradient Hamiltonian
                Monte Carlo under local conditions for nonconvex
                optimization[0m

Box rectangle:  [32m(90.0, 170.6) -> (521.9, 195.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m̈O. Deniz Akyildiz
                deniz.akyildiz@imperial.ac.uk
                Department of Mathematics, Imperial College London[0m

Box rectangle:  [32m(90.0, 203.1) -> (522.0, 255.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSotirios Sabanis
                s.sabanis@ed.ac.uk
                School of Mathematics, University of Edinburgh
                The Alan Turing Institute
                National Technical University of Athens[0m

Box rectangle:  [32m(90.0, 724.5) -> (276.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024  ̈Omer Deniz Akyildiz and Sotirios Sabanis.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1423.html.[0m



=== Processing ../JMLR 2024/Non-Euclidean Monotone Operator Theory and Applications.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Non-Euclidean Monotone Operator Theory and Applications.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 6/23; Revised 4/24; Published 10/24[0m

Box rectangle:  [32m(90.5, 101.6) -> (521.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNon-Euclidean Monotone Operator Theory and Applications[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Davydov ∗
                davydov@ucsb.edu
                Center for Control, Dynamical Systems, and Computation
                University of California, Santa Barbara
                Santa Barbara, CA 93106-5070, USA[0m

Box rectangle:  [32m(90.0, 187.2) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSaber Jafarpour∗
                saber.jafarpour@colorado.edu
                Department of Electrical, Computer, and Energy Engineering
                University of Colorado, Boulder
                Boulder, CO 80309-0020, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnton V. Proskurnikov
                anton.p.1982@ieee.org
                Department of Electronics and Telecommunications
                Politecnico di Torino
                Turin, Italy[0m

Box rectangle:  [32m(90.0, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrancesco Bullo
                bullo@ucsb.edu
                Center for Control, Dynamical Systems, and Computation
                University of California, Santa Barbara
                Santa Barbara, CA 93106-5070, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (429.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alexander Davydov, Saber Jafarpour, Anton V. Proskurnikov, and Francesco Bullo.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0805.html.[0m



=== Processing ../JMLR 2024/Nonparametric Copula Models for Multivariate  Mixed  and Missing Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonparametric Copula Models for Multivariate  Mixed  and Missing Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 4/23; Revised 2/24; Published 5/24[0m

Box rectangle:  [32m(93.0, 101.6) -> (519.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonparametric Copula Models for Multivariate, Mixed, and
                Missing Data[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoseph Feldman
                joseph.feldman@duke.edu
                Department of Statistical Science
                Duke University, Durham, NC 27708-0251, USA[0m

Box rectangle:  [32m(90.0, 195.1) -> (522.0, 261.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel R. Kowal
                dan.kowal@cornell.edu
                Department of Statistics and Data Science
                Cornell University, Ithaca, NY 14853-2601, USA
                Department of Statistics
                Rice University, Houston, TX 77251-1892, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (260.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Joseph Feldman and Daniel R. Kowal.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0495.html.[0m



=== Processing ../JMLR 2024/Nonparametric Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonparametric Estimation of Non-Crossing Quantile Regression Process with Deep ReQU Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-75
                Submitted 05/22; Revised 12/23; Published 03/24[0m

Box rectangle:  [32m(112.4, 101.6) -> (499.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonparametric Estimation of Non-Crossing Quantile
                Regression Process with Deep ReQU Neural Networks[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuohao Shen∗
                guohao.shen@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuling Jiao∗
                yulingjiaomath@whu.edu.cn
                School of Mathematics and Statistics
                and Hubei Key Laboratory of Computational Science
                Wuhan University
                Wuhan 430072, China[0m

Box rectangle:  [32m(90.0, 271.0) -> (522.0, 319.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanyuan Lin
                ylin@sta.cuhk.edu.hk
                Department of Statistics
                The Chinese University of Hong Kong
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 324.6) -> (522.0, 372.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoel L. Horowitz
                joel-horowitz@northwestern.edu
                Department of Economics
                Northwestern University
                Evanston, IL 60208, USA[0m

Box rectangle:  [32m(90.0, 379.7) -> (522.0, 432.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJian Huang
                j.huang@polyu.edu.hk
                Department of Applied Mathematics
                The Hong Kong Polytechnic University
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (395.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Guohao Shen, Yuling Jiao, Yuanyuan Lin, Joel Horowitz and Jian Huang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0488.html.[0m



=== Processing ../JMLR 2024/Nonparametric Inference under B-bits Quantization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonparametric Inference under B-bits Quantization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-68
                Submitted 1/20; Revised 4/23; Published 1/24[0m

Box rectangle:  [32m(122.3, 101.6) -> (489.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonparametric Inference under B-bits Quantization[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKexuan Li
                kexuan.li.77@gmail.com
                Global Biometrics and Data Sciences
                Bristol Myers Squibb
                Princeton Pike, NJ 08648, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuiqi Liu
                ruiqliu@ttu.edu
                Department of Mathematics and Statistics
                Texas Tech University
                Lubbock, TX 79409, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGanggang Xu
                gangxu@bus.miami.edu
                Department of Management Science
                University of Miami
                Coral Gables, FL 33146, USA[0m

Box rectangle:  [32m(90.0, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZuofeng Shang
                zshang@njit.edu
                Department of Mathematical Sciences
                New Jersey Institute of Technology
                Newark, NJ 07102, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (520.2, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kexuan Li, Ruiqi Liu, Ganggang Xu and Zuofeng Shang. For correspondence, please contact Zuofeng Shang
                and Ganggang Xu..[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/20-075.html.[0m



=== Processing ../JMLR 2024/Nonparametric Regression for 3D Point Cloud Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonparametric Regression for 3D Point Cloud Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 6/22; Revised 2/23; Published 1/24[0m

Box rectangle:  [32m(107.9, 101.6) -> (504.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonparametric Regression for 3D Point Cloud Learning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXinyi Li
                lixinyi@clemson.edu
                School of Mathematical and Statistical Sciences, Clemson University
                Clemson, SC 29634, USA[0m

Box rectangle:  [32m(90.0, 175.6) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShan Yu
                sy5jx@virginia.edu
                Department of Statistics, University of Virginia
                Charlottesville, VA 22904, USA[0m

Box rectangle:  [32m(90.0, 217.0) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYueying Wang1
                yueyingw@amazon.com
                Amazon.com, Inc.
                Seattle, WA 98121, USA[0m

Box rectangle:  [32m(90.0, 258.8) -> (522.0, 294.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuannan Wang
                gwang01@wm.edu
                Department of Mathematics, College of William & Mary
                Williamsburg, VA 23185, USA[0m

Box rectangle:  [32m(90.0, 300.4) -> (522.0, 336.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLi Wang
                lwang41@gmu.edu
                Department of Statistics, George Mason University
                Fairfax, VA 22030, USA[0m

Box rectangle:  [32m(90.0, 342.1) -> (522.0, 378.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMing-Jun Lai
                mjlai@uga.edu
                Department of Mathematics, University of Georgia
                Athens, GA 30602, USA[0m

Box rectangle:  [32m(90.0, 385.1) -> (380.2, 397.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mfor the Alzheimer’s Disease Neuroimaging Initiative2[0m

Box rectangle:  [32m(90.0, 422.6) -> (192.9, 432.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Xiaotong Shen[0m

Box rectangle:  [32m(280.3, 458.1) -> (331.7, 470.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 477.8) -> (502.2, 619.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn recent years, there has been an exponentially increased amount of point clouds collected
                with irregular shapes in various areas. Motivated by the importance of solid modeling for
                point clouds, we develop a novel and efficient smoothing tool based on multivariate splines
                over the triangulation to extract the underlying signal and build up a 3D solid model from
                the point cloud. The proposed method can denoise or deblur the point cloud effectively,
                provide a multi-resolution reconstruction of the actual signal, and handle sparse and ir-
                regularly distributed point clouds to recover the underlying trajectory. In addition, our
                method provides a natural way of numerosity data reduction. We establish the theoretical
                guarantees of the proposed method, including the convergence rate and asymptotic normal-
                ity of the estimator, and show that the convergence rate achieves optimal nonparametric
                convergence. We also introduce a bootstrap method to quantify the uncertainty of the es-
                timators. Through extensive simulation studies and a real data example, we demonstrate[0m

Box rectangle:  [32m(93.7, 641.1) -> (522.1, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Yueying Wang’s work in this paper was completed prior to joining Amazon.
                2. Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging
                Initiative (ADNI) database (adni.loni.usc.edu).
                As such, the investigators within the ADNI con-
                tributed to the design and implementation of ADNI and/or provided data but did not participate
                in analysis or writing of this report.
                A complete listing of ADNI investigators can be found at:
                http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.[0m

Box rectangle:  [32m(90.0, 726.4) -> (417.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Xinyi Li, Shan Yu, Yueying Wang, Guannan Wang, Li Wang and Ming-Jun Lai.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0735.html.[0m



=== Processing ../JMLR 2024/Nonparametric Regression Using Over-parameterized Shallow ReLU Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Nonparametric Regression Using Over-parameterized Shallow ReLU Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 7/23; Revised 5/24; Published 5/24[0m

Box rectangle:  [32m(116.9, 101.6) -> (495.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNonparametric Regression Using Over-parameterized
                Shallow ReLU Neural Networks[0m

Box rectangle:  [32m(89.4, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunfei Yang ∗
                yunfyang@cityu.edu.hk
                Department of Mathematics, City University of Hong Kong
                Kowloon, Hong Kong, China[0m

Box rectangle:  [32m(89.5, 195.1) -> (521.9, 234.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDing-Xuan Zhou
                dingxuan.zhou@sydney.edu.au
                School of Mathematics and Statistics, University of Sydney
                Sydney, NSW 2006, Australia[0m

Box rectangle:  [32m(90.0, 726.3) -> (247.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yunfei Yang and Ding-Xuan Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0918.html.[0m



=== Processing ../JMLR 2024/Non-splitting Neyman-Pearson Classifiers.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Non-splitting Neyman-Pearson Classifiers.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-61
                Submitted 7/22; Revised 4/24; Published 7/24[0m

Box rectangle:  [32m(158.7, 101.6) -> (453.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNon-splitting Neyman-Pearson Classifiers[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJingming Wang ∗
                pdw9qv@virginia.edu
                Department of Statistics
                University of Virginia[0m

Box rectangle:  [32m(90.0, 175.2) -> (522.0, 223.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLucy Xia ∗
                lucyxia@ust.hk
                Department of ISOM
                School of Business and Management
                Hong Kong University of Science and Technology[0m

Box rectangle:  [32m(90.0, 229.1) -> (522.0, 265.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhigang Bao
                zgbao@hku.hk
                Department of Mathematics
                The University of Hong Kong[0m

Box rectangle:  [32m(90.0, 272.4) -> (522.0, 325.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXin Tong
                xint@marshall.usc.edu
                Department of Data Sciences and Operations
                Marshall Business School
                University of Southern California[0m

Box rectangle:  [32m(90.0, 726.3) -> (323.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jingming Wang, Lucy Xia, Xin Tong and Zhigang Bao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0795.html.[0m



=== Processing ../JMLR 2024/Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.5) -> (522.0, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 10/22; Revised 9/23; Published 1/24[0m

Box rectangle:  [32m(145.2, 99.5) -> (466.9, 131.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNumerically Stable Sparse Gaussian Processes
                via Minimum Separation using Cover Trees[0m

Box rectangle:  [32m(89.6, 147.6) -> (187.0, 159.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Terenin∗[0m

Box rectangle:  [32m(88.4, 160.7) -> (306.3, 169.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUniversity of Cambridge and Imperial College London[0m

Box rectangle:  [32m(90.0, 175.3) -> (166.8, 186.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid R. Burt∗[0m

Box rectangle:  [32m(88.4, 188.4) -> (226.3, 197.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUniversity of Cambridge and MIT[0m

Box rectangle:  [32m(89.6, 203.0) -> (174.0, 214.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArtem Artemev∗[0m

Box rectangle:  [32m(89.5, 216.1) -> (256.5, 225.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImperial College London and Secondmind[0m

Box rectangle:  [32m(88.4, 232.0) -> (170.8, 252.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSeth Flaxman
                University of Oxford[0m

Box rectangle:  [32m(89.5, 259.7) -> (291.4, 280.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMark van der Wilk
                Imperial College London and University of Oxford[0m

Box rectangle:  [32m(88.4, 287.4) -> (255.5, 308.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCarl Edward Rasmussen
                University of Cambridge and Secondmind[0m

Box rectangle:  [32m(88.4, 316.1) -> (186.8, 337.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHong Ge
                University of Cambridge[0m

Box rectangle:  [32m(90.0, 362.4) -> (235.2, 371.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Mohammad Emtiyaz Khan[0m

Box rectangle:  [32m(280.3, 395.3) -> (331.7, 407.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.5, 411.7) -> (503.8, 574.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGaussian processes are frequently deployed as part of larger machine learning and decision-making
                systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models.
                Within a system, the Gaussian process model needs to perform in a stable and reliable manner to
                ensure it interacts correctly with other parts of the system. In this work, we study the numerical
                stability of scalable sparse approximations based on inducing points. To do so, we first review
                numerical stability, and illustrate typical situations in which Gaussian process models can be
                unstable. Building on stability theory originally developed in the interpolation literature, we derive
                sufficient and in certain cases necessary conditions on the inducing points for the computations
                performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we
                propose an automated method for computing inducing points satisfying these conditions. This
                is done via a modification of the cover tree data structure, which is of independent interest. We
                additionally propose an alternative sparse approximation for regression with a Gaussian likelihood
                which trades offa small amount of performance to further improve stability. We provide illustrative
                examples showing the relationship between stability of calculations and predictive performance of
                inducing point methods on spatial tasks.[0m

Box rectangle:  [32m(90.0, 593.9) -> (180.2, 605.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.6, 614.6) -> (522.3, 660.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGaussian processes are a flexible framework and model class for learning unknown functions. By
                way of being constructed in the language of Bayesian learning, Gaussian process models provide an
                ability to incorporate prior information into the model, and assess and propagate uncertainty in a
                principled manner. This makes them well-suited for a wide variety of areas where these capabilities[0m

Box rectangle:  [32m(90.0, 671.8) -> (399.8, 690.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗Equal contribution.
                Code available at: https://github.com/awav/conjugate-gradient-sparse-gp.[0m

Box rectangle:  [32m(90.0, 727.1) -> (512.6, 742.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc
                ⃝2024 Alexander Terenin, David R. Burt, Artem Artemev, Seth Flaxman, Mark van der Wilk, Carl Edward Rasmussen,
                and Hong Ge.[0m

Box rectangle:  [32m(90.0, 746.7) -> (504.3, 763.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1170.html.[0m



=== Processing ../JMLR 2024/Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-31
                Submitted 4/23; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(142.3, 101.6) -> (469.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOff-Policy Action Anticipation in Multi-Agent
                Reinforcement Learning[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 246.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAriyan Bighashdel
                a.bighashdel@tue.nl
                Daan de Geus
                d.c.d.geus@tue.nl
                Pavol Jancura
                p.jancura@tue.nl
                Gijs Dubbelman
                g.dubbelman@tue.nl
                Department of Electrical Engineering
                Eindhoven University of Technology
                Eindhoven, 5612 AZ, The Netherlands[0m

Box rectangle:  [32m(90.0, 272.1) -> (211.5, 282.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Alessandro Lazaric[0m

Box rectangle:  [32m(280.3, 305.7) -> (331.7, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 326.2) -> (502.2, 527.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLearning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning
                paradigm where agents anticipate the learning steps of other agents to improve cooperation
                among themselves. As MARL uses gradient-based optimization, learning anticipation re-
                quires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG
                methods are based on policy parameter anticipation, i.e., agents anticipate the changes in
                policy parameters of other agents. Currently, however, these existing HOG methods have
                only been developed for differentiable games or games with small state spaces. In this
                work, we demonstrate that in the case of non-differentiable games with large state spaces,
                existing HOG methods do not perform well and are inefficient due to their inherent limita-
                tions related to policy parameter anticipation and multiple sampling stages. To overcome
                these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework
                that approaches learning anticipation through action anticipation, i.e., agents anticipate
                the changes in actions of other agents, via off-policy sampling. We theoretically analyze
                our proposed OffPA2 and employ it to develop multiple HOG methods that are applicable
                to non-differentiable games with large state spaces. We conduct a large set of experiments
                and illustrate that our proposed HOG methods outperform the existing ones regarding
                efficiency and performance.[0m

Box rectangle:  [32m(109.9, 532.5) -> (502.1, 554.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Multi-agent reinforcement learning, Reasoning, Learning anticipation, oppo-
                nent shaping[0m

Box rectangle:  [32m(90.0, 576.0) -> (180.3, 588.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 599.5) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn multi-agent systems, the paradigm of agents’ reasoning about other agents has been ex-
                plored and researched extensively (Goodie et al., 2012; Liu and Lakemeyer, 2021). Recently,
                this paradigm is also being studied in the subfield of Multi-Agent Reinforcement Learning
                (MARL) (Wen et al., 2019, 2020; Konan et al., 2022). Generally speaking, MARL deals with
                several agents simultaneously learning and interacting in an environment. In the context
                of MARL, one reasoning strategy is anticipating the learning steps of other agents (Zhang
                and Lesser, 2010), i.e., learning anticipation. As MARL uses gradient-based optimization,
                learning anticipation naturally leads to the usage of Higher-Order Gradients (HOG), with[0m

Box rectangle:  [32m(90.0, 726.4) -> (387.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Ariyan Bighashdel, Daan de Geus, Pavol Jancura, and Gijs Dubbelman.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0413.html.[0m



=== Processing ../JMLR 2024/OmniSafe  An Infrastructure for Accelerating Safe Reinforcement Learning Research.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/OmniSafe  An Infrastructure for Accelerating Safe Reinforcement Learning Research.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-6
                Submitted 5/23; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(144.2, 101.6) -> (467.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOmniSafe: An Infrastructure for Accelerating
                Safe Reinforcement Learning Research[0m

Box rectangle:  [32m(90.0, 153.1) -> (522.0, 301.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiaming Ji∗
                jiamg.ji@gmail.com
                Jiayi Zhou∗
                gaiejj@outlook.com
                Borong Zhang∗
                borongzh@gmail.com
                Juntao Dai
                jtd.acad@gmail.com
                Xuehai Pan
                xuehaipan@pku.edu.cn
                Ruiyang Sun
                sun ruiyang@stu.pku.edu.cn
                Weidong Huang
                bigeasthuang@gmail.com
                Yiran Geng
                gyr@stu.pku.edu.cn
                Mickel Liu
                mickelliu7@gmail.com
                Yaodong Yang†
                yaodong.yang@pku.edu.cn
                Institute for Artificial Intelligence, Peking University, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (521.5, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng,
                Mickel Liu, Yaodong Yang.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0681.html.[0m



=== Processing ../JMLR 2024/On Causality in Domain Adaptation and Semi-Supervised Learning  an Information-Theoretic Analysis for Parametric Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Causality in Domain Adaptation and Semi-Supervised Learning  an Information-Theoretic Analysis for Parametric Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 9/22; Revised 6/24; Published 9/24[0m

Box rectangle:  [32m(93.2, 101.6) -> (519.0, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Causality in Domain Adaptation and Semi-Supervised
                Learning: an Information-Theoretic Analysis for Parametric
                Models[0m

Box rectangle:  [32m(89.4, 169.8) -> (522.0, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuetong Wu
                xuetongw1@student.unimelb.edu.au
                Department of Electrical and Electronic Engineering[0m

Box rectangle:  [32m(89.5, 211.4) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMingming Gong
                mingming.gong@unimelb.edu.au
                School of Mathematics and Statistics[0m

Box rectangle:  [32m(89.4, 253.0) -> (522.0, 277.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonathan H. Manton
                jmanton@unimelb.edu.au
                Department of Electrical and Electronic Engineering[0m

Box rectangle:  [32m(89.4, 294.7) -> (522.0, 318.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUwe Aickelin
                uwe.aickelin@unimelb.edu.au
                Department of Computing and Information Systems[0m

Box rectangle:  [32m(88.3, 337.9) -> (521.9, 390.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJingge Zhu
                jingge.zhu@unimelb.edu.au
                Department of Electrical and Electronic Engineering
                University of Melbourne
                Parkville, 3010, Australia[0m

Box rectangle:  [32m(90.0, 726.3) -> (414.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xuetong Wu, Mingming Gong, Jonathan H. Manton, Uwe Aickelin, Jingge Zhu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1024.html.[0m



=== Processing ../JMLR 2024/On Doubly Robust Inference for Double Machine Learning in Semiparametric Regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Doubly Robust Inference for Double Machine Learning in Semiparametric Regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 10/22; Revised 6/24; Published 6/24[0m

Box rectangle:  [32m(97.5, 101.6) -> (514.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Doubly Robust Inference for Double Machine Learning
                in Semiparametric Regression[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOliver Dukes
                oliver.dukes@ugent.be
                Department of Applied Mathematics, Computer Science and Statistics
                Ghent University
                9000 Ghent, Belgium[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStijn Vansteelandt
                stijn.vansteelandt@ugent.be
                Department of Applied Mathematics, Computer Science and Statistics
                Ghent University
                9000 Ghent, Belgium[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Whitney
                david.e.whitney@gsk.com
                GSK
                Gunnels Wood Road, Stevenage, SG1 2NY, U.K.[0m

Box rectangle:  [32m(90.0, 726.3) -> (317.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Oliver Dukes, Stijn Vansteelandt and David Whitney.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1233.html.[0m



=== Processing ../JMLR 2024/On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood Estimator in Mixture Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 10/22; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Efficient and Scalable Computation of the Nonparametric
                Maximum Likelihood Estimator in Mixture Models[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYangjing Zhang
                yangjing.zhang@amss.ac.cn
                Institute of Applied Mathematics
                Academy of Mathematics and Systems Science
                Chinese Academy of Sciences[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYing Cui
                yingcui@berkeley.edu
                Department of Industrial Engineering and Operations Research
                University of California, Berkeley[0m

Box rectangle:  [32m(90.0, 247.1) -> (522.0, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBodhisattva Sen
                bodhi@stat.columbia.edu
                Department of Statistics
                Columbia University[0m

Box rectangle:  [32m(90.0, 290.3) -> (522.0, 343.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKim-Chuan Toh
                mattohkc@nus.edu.sg
                Department of Mathematics
                Institute of Operations Research and Analytics
                National University of Singapore[0m

Box rectangle:  [32m(90.0, 726.3) -> (365.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yangjing Zhang, Ying Cui, Bodhisattva Sen, and Kim-Chuan Toh.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1120.html.[0m



=== Processing ../JMLR 2024/On Regularized Radon-Nikodym Differentiation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Regularized Radon-Nikodym Differentiation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-24
                Submitted 5/23; Revised 4/24; Published 9/24[0m

Box rectangle:  [32m(135.2, 101.6) -> (476.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Regularized Radon–Nikodym Differentiation[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDuc Hoan Nguyen
                duc.nguyen@ricam.oeaw.ac.at
                Johann Radon Institute for Computational and Applied Mathematics
                Austrian Academy of Sciences
                Altenberger Straße 69, 4040 Linz, Austria[0m

Box rectangle:  [32m(90.0, 195.9) -> (264.9, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUniversity of Lorraine,
                CNRS, CRAN, Nancy, F-54000, France[0m

Box rectangle:  [32m(90.0, 225.0) -> (522.0, 291.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWerner Zellinger
                werner.zellinger@ricam.oeaw.ac.at
                Sergei Pereverzyev
                sergei.pereverzyev@oeaw.ac.at
                Johann Radon Institute for Computational and Applied Mathematics
                Austrian Academy of Sciences
                Altenberger Straße 69, 4040 Linz, Austria[0m

Box rectangle:  [32m(90.0, 726.3) -> (343.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Duc Hoan Nguyen, Werner Zellinger and Sergei Pereverzyev.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0567.html.[0m



=== Processing ../JMLR 2024/On Sufficient Graphical Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Sufficient Graphical Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-64
                Submitted 7/23; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(208.6, 101.7) -> (403.4, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Sufficient Graphical Models[0m

Box rectangle:  [32m(90.0, 135.4) -> (521.8, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBing Li
                BXL9@PSU.EDU
                Department of Statistics, Pennsylvania State University
                326 Thomas Building, University Park, PA 16802[0m

Box rectangle:  [32m(90.0, 178.7) -> (521.8, 216.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKyongwon Kim
                KIMK@EWHA.AC.KR
                Department of Statistics, Ewha Womans University
                52 Ewhayeodae-gil, Seodaemun-gu, Seoul, Republic of Korea, 03760[0m

Box rectangle:  [32m(90.0, 726.4) -> (209.8, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bing Li and Kyongwon Kim.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0893.html.[0m



=== Processing ../JMLR 2024/On Tail Decay Rate Estimation of Loss Function Distributions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Tail Decay Rate Estimation of Loss Function Distributions.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 7/22; Revised 7/23; Published 1/24[0m

Box rectangle:  [32m(134.1, 102.0) -> (477.9, 116.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Tail Decay Rate Estimation of Loss Function[0m

Box rectangle:  [32m(259.7, 120.0) -> (352.3, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDistributions[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 200.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEtrit Haxholli
                etrit.haxholli@inria.fr
                Epione Research Group
                Inria, Univesity Cte d’Azur
                2004 Rte des Lucioles, 06902 Valbonne, France[0m

Box rectangle:  [32m(90.0, 208.8) -> (522.0, 260.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarco Lorenzi
                marco.lorenzi@inria.fr
                Epione Research Group
                Inria, Univesity Cte d’Azur
                2004 Rte des Lucioles, 06902 Valbonne, France[0m

Box rectangle:  [32m(90.0, 285.3) -> (201.3, 295.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Lorenzo Rosasco[0m

Box rectangle:  [32m(280.3, 319.0) -> (331.7, 330.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 336.4) -> (502.1, 394.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe study of loss-function distributions is critical to characterize a model’s behaviour
                on a given machine-learning problem. While model quality is commonly measured by the
                average loss assessed on a testing set, this quantity does not ascertain the existence of
                the mean of the loss distribution. Conversely, the existence of a distribution’s statistical
                moments can be verified by examining the thickness of its tails.[0m

Box rectangle:  [32m(109.9, 396.6) -> (502.1, 514.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCross-validation schemes determine a family of testing loss distributions conditioned
                on the training sets.
                By marginalizing across training sets, we can recover the overall
                (marginal) loss distribution, whose tail-shape we aim to estimate. Small sample-sizes di-
                minish the reliability and efficiency of classical tail-estimation methods like Peaks-Over-
                Threshold, and we demonstrate that this effect is notably significant when estimating
                tails of marginal distributions composed of conditional distributions with substantial tail-
                location variability. We mitigate this problem by utilizing a result we prove: under certain
                conditions, the marginal-distribution’s tail-shape parameter is the maximum tail-shape pa-
                rameter across the conditional distributions underlying the marginal. We label the resulting
                approach as ‘cross-tail estimation (CTE)’.[0m

Box rectangle:  [32m(109.9, 515.1) -> (502.1, 562.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe test CTE in a series of experiments on simulated and real data1, showing the
                improved robustness and quality of tail estimation as compared to classical approaches.
                Keywords:
                Extreme Value Theory, Tail Modelling, Peaks-Over-Threshold, Cross-Tail-
                Estimation, Model Ranking[0m

Box rectangle:  [32m(90.0, 583.9) -> (180.3, 595.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 605.7) -> (522.0, 684.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLoss function distributions form critical subjects of analysis, serving as barometers for ma-
                chine learning model performance. In the context of a particular model and associated
                machine learning task, the true distribution of the loss function is typically elusive; we
                predominantly have access to a finite sample set, born from diverse choices of training and
                testing sets. To facilitate performance comparisons across different models based on the
                underlying loss function distributions, a spectrum of methodologies has been established.[0m

Box rectangle:  [32m(93.7, 696.1) -> (345.8, 705.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. The code is available at https://github.com/ehaxholli/CTE[0m

Box rectangle:  [32m(90.0, 726.5) -> (231.1, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Etrit Haxholli, Marco Lorenzi.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0846.html.[0m



=== Processing ../JMLR 2024/On the Computational and Statistical Complexity of Over-parameterized Matrix Sensing.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Computational and Statistical Complexity of Over-parameterized Matrix Sensing.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 12/21; Revised 10/23; Published 2/24[0m

Box rectangle:  [32m(120.0, 101.6) -> (492.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Computational and Statistical Complexity of
                Over-parameterized Matrix Sensing[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiacheng Zhuo∗
                jzhuo@utexas.edu
                Department of Computer Science
                University of Texas
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeongyeol Kwon
                jeongyeol.kwon@wisc.edu
                Wisconsin Institute for Discovery
                University of Wisconsin-Madison
                Madison, WI 53705, USA[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNhat Ho
                minhnhat@utexas.edu
                Department of Statistics and Data Sciences
                University of Texas
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConstantine Caramanis
                constantine@utexas.edu
                Department of Electrical and Computer Engineering
                University of Texas
                Austin, TX 78712, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (368.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, Constantine Caramanis.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1437.html.[0m



=== Processing ../JMLR 2024/On the Computational Complexity of Metropolis-Adjusted Langevin Algorithms for Bayesian Posterior Sampling.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Computational Complexity of Metropolis-Adjusted Langevin Algorithms for Bayesian Posterior Sampling.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-79
                Submitted 6/23; Revised 4/24; Published 4/24[0m

Box rectangle:  [32m(97.9, 119.3) -> (514.2, 151.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Computational Complexity of Metropolis-Adjusted
                Langevin Algorithms for Bayesian Posterior Sampling[0m

Box rectangle:  [32m(90.0, 169.6) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRong Tang
                martang@ust.hk
                Department of Mathematics
                The Hong Kong University of Science and Technology[0m

Box rectangle:  [32m(90.0, 212.8) -> (522.0, 252.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYun Yang
                yy84@illinois.edu
                Department of Statistics
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 726.3) -> (217.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rong Tang and Yun Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0783.html.[0m



=== Processing ../JMLR 2024/On the Concentration of the Minimizers of Empirical Risks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Concentration of the Minimizers of Empirical Risks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 9/23; Revised 8/24; Published 8/24[0m

Box rectangle:  [32m(95.9, 101.6) -> (516.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Concentration of the Minimizers of Empirical Risks[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 188.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPaul Escande
                paul.escande@math.univ-toulouse.fr
                Institut de Math ́ematiques de Toulouse
                UMR 5219, Universit ́e de Toulouse, CNRS
                UPS, F-31062 Toulouse Cedex 9, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (170.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Paul Escande.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1149.html.[0m



=== Processing ../JMLR 2024/On the Connection between Lp- and Risk Consistency and its Implications on Regularized Kernel Methods.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Connection between Lp- and Risk Consistency and its Implications on Regularized Kernel Methods.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 3/23; Revised 1/24; Published 7/24[0m

Box rectangle:  [32m(98.3, 101.6) -> (513.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Connection between Lp- and Risk Consistency and
                its Implications on Regularized Kernel Methods[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHannes K ̈ohler
                hannes.koehler@uni-bayreuth.de
                Department of Mathematics
                University of Bayreuth
                95440 Bayreuth, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (175.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hannes K ̈ohler.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0397.html.[0m



=== Processing ../JMLR 2024/On the Convergence of Projected Alternating Maximization for Equitable and Optimal Transport.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Convergence of Projected Alternating Maximization for Equitable and Optimal Transport.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-33
                Submitted 5/22; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(93.4, 101.6) -> (518.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Convergence of Projected Alternating Maximization
                for Equitable and Optimal Transport[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMinhui Huang
                mhhuang@ucdavis.edu
                Department of Electrical and Computer Engineering
                University of California
                Davis, CA 95616-5270, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShiqian Ma
                sqma@rice.edu
                Department of Computational Applied Mathematics and Operations Research
                Rice University
                Houston, TX 77005-1827, USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLifeng Lai
                lflai@ucdavis.edu
                Department of Electrical and Computer Engineering
                University of California
                Davis, CA 95616-5270, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (292.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Minhui Huang and Shiqian Ma and Lifeng Lai.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0524.html.[0m



=== Processing ../JMLR 2024/On the Effect of Initialization  The Scaling Path of 2-Layer Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Effect of Initialization  The Scaling Path of 2-Layer Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-24
                Submitted 4/23; Revised 8/23; Published 1/24[0m

Box rectangle:  [32m(129.2, 101.5) -> (482.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Effect of Initialization: The Scaling Path of
                2-Layer Neural Networks[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSebastian Neumayer∗
                sebastian.neumayer@epfl.ch
                Biomedical Imaging Group
                École polytechnique fédérale de Lausanne
                Lausane, CH-1015, Switzerland[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLénaïc Chizat
                lenaic.chizat@epfl.ch
                Chair of Dynamics of Learning Algorithms
                École polytechnique fédérale de Lausanne
                Lausane, CH-1015, Switzerland[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael Unser
                michael.unser@epfl.ch
                Biomedical Imaging Group
                École polytechnique fédérale de Lausanne
                Lausane, CH-1015, Switzerland[0m

Box rectangle:  [32m(90.0, 726.3) -> (325.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sebastian Neumayer, Lénaïc Chizat and Michael Unser.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0549.html.[0m



=== Processing ../JMLR 2024/On the Eigenvalue Decay Rates of a Class of Neural-Network Related Kernel Functions Defined on General Domains.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Eigenvalue Decay Rates of a Class of Neural-Network Related Kernel Functions Defined on General Domains.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 7/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Eigenvalue Decay Rates of a Class of Neural-Network
                Related Kernel Functions Defined on General Domains[0m

Box rectangle:  [32m(90.0, 151.9) -> (521.9, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYicheng Li
                liyc22@mails.tsinghua.edu.cn
                Center for Statistical Science, Department of Industrial Engineering
                Tsinghua University
                Beijing, 100084, China[0m

Box rectangle:  [32m(90.0, 205.4) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZixiong Yu
                yuzx19@mails.tsinghua.edu.cn
                Yau Mathematical Sciences Center, Department of Mathematical Sciences
                Tsinghua University
                Beijing, 100084, China[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuhan Chen
                chen-gh23@mails.tsinghua.edu.cn
                Center for Statistical Science, Department of Industrial Engineering
                Tsinghua University
                Beijing, 100084, China[0m

Box rectangle:  [32m(90.0, 313.9) -> (521.9, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQian Lin∗
                qianlin@tsinghua.edu.cn
                Center for Statistical Science, Department of Industrial Engineering
                Tsinghua University
                Beijing, 100084, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (311.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yicheng Li, Zixiong Yu, Guhan Chen and Qian Lin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0866.html.[0m



=== Processing ../JMLR 2024/On the Generalization of Stochastic Gradient Descent with Momentum.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Generalization of Stochastic Gradient Descent with Momentum.pdf') ---[0m

Box rectangle:  [32m(108.0, 58.9) -> (503.9, 66.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 1/22; Revised 10/23; Published 1/24[0m

Box rectangle:  [32m(179.8, 118.6) -> (432.3, 150.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Generalization of Stochastic
                Gradient Descent with Momentum[0m

Box rectangle:  [32m(106.8, 168.8) -> (504.0, 216.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAli Ramezani-Kebrya
                ali@uio.no
                Department of Informatics, University of Oslo and Visual Intelligence Centre
                Integreat, Norwegian Centre for Knowledge-driven Machine Learning
                Gaustadalléen 23B, Ole-Johan Dahls hus, 0373 Oslo, Norway[0m

Box rectangle:  [32m(107.5, 222.4) -> (504.0, 258.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKimon Antonakopoulos
                kimon.antonakopoulos@epfl.ch
                Laboratory for Information and Inference Systems (LIONS), EPFL
                EPFL STI IEL LIONS, Station 11, CH-1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(107.5, 264.1) -> (504.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVolkan Cevher
                volkan.cevher@epfl.ch
                Laboratory for Information and Inference Systems (LIONS), EPFL
                EPFL STI IEL LIONS, Station 11, CH-1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(107.3, 305.7) -> (504.0, 341.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAshish Khisti
                akhisti@ece.utoronto.ca
                Department of Electrical and Computer Engineering, University of Toronto
                40 St. George Street, Toronto, ON M5S 2E4, Canada[0m

Box rectangle:  [32m(107.3, 348.9) -> (504.0, 387.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBen Liang
                liang@ece.utoronto.ca
                Department of Electrical and Computer Engineering, University of Toronto
                40 St. George Street, Toronto, ON M5S 2E4, Canada[0m

Box rectangle:  [32m(108.0, 413.1) -> (230.6, 423.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Francesco Orabona[0m

Box rectangle:  [32m(280.3, 446.9) -> (331.7, 458.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(127.4, 467.6) -> (486.2, 656.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhile momentum-based accelerated variants of stochastic gradient descent (SGD)
                are widely used when training machine learning models, there is little theoretical
                understanding on the generalization error of such methods. In this work, we first
                show that there exists a convex loss function for which the stability gap for multiple
                epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded.
                Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based
                update rule, i.e., SGD with early momentum (SGDEM) under a broad range
                of step-sizes, and show that it can train machine learning models for multiple
                epochs with a guarantee for generalization. Finally, for the special case of strongly
                convex loss functions, we find a range of momentum such that multiple epochs of
                standard SGDM, as a special form of SGDEM, also generalizes. Extending our
                results on generalization, we also develop an upper bound on the expected true
                risk, in terms of the number of training steps, sample size, and momentum. Our
                experimental evaluations verify the consistency between the numerical results and
                our theoretical bounds. SGDEM improves the generalization error of SGDM when
                training ResNet-18 on ImageNet in practical distributed settings.[0m

Box rectangle:  [32m(127.9, 664.1) -> (485.9, 686.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: Uniform stability, generalization error, heavy-ball momentum, stochas-
                tic gradient descent, non-convex[0m

Box rectangle:  [32m(107.5, 707.7) -> (482.1, 715.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Ali Ramezani-Kebrya, Kimon Antonakopoulos, Volkan Cevher, Ashish Khisti, and Ben Liang.[0m

Box rectangle:  [32m(108.0, 722.0) -> (489.3, 739.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/22-0068.html.[0m



=== Processing ../JMLR 2024/On the Hyperparameters in Stochastic Gradient Descent with Momentum.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Hyperparameters in Stochastic Gradient Descent with Momentum.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 10/22; Revised 8/23; Published 8/24[0m

Box rectangle:  [32m(104.8, 101.6) -> (507.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Hyperparameters in Stochastic Gradient Descent
                with Momentum[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBin Shi
                shibin@lsec.cc.ac.cn
                Academy of Mathematics and Systems Science
                Chinese Academy of Sciences
                Beijing, 100190, China[0m

Box rectangle:  [32m(90.0, 221.2) -> (277.8, 258.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSchool of Mathematical Science
                University of Chinese Academy of Sciences
                Beijing 100049, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (148.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Bin Shi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1189.html.[0m



=== Processing ../JMLR 2024/On the Impact of Hard Adversarial Instances on Overfitting in Adversarial Training.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Impact of Hard Adversarial Instances on Overfitting in Adversarial Training.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 8/22; Revised 5/23; Published 12/24[0m

Box rectangle:  [32m(145.6, 107.0) -> (466.5, 139.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Impact of Hard Adversarial Instances
                on Overfitting in Adversarial Training[0m

Box rectangle:  [32m(90.0, 163.8) -> (518.2, 176.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChen Liu1 † ∗
                chen.liu@cityu.edu.hk[0m

Box rectangle:  [32m(90.0, 187.0) -> (518.2, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhichao Huang2
                zhichao.huang@connect.ust.hk[0m

Box rectangle:  [32m(90.0, 210.2) -> (518.2, 222.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMathieu Salzmann3
                mathieu.salzmann@epfl.ch[0m

Box rectangle:  [32m(89.6, 233.3) -> (518.2, 245.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTong Zhang4 ‡
                tongzhang@tongzhang-ml.org[0m

Box rectangle:  [32m(90.0, 256.5) -> (522.0, 269.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSabine S ̈usstrunk3
                sabine.susstrunk@epfl.ch[0m

Box rectangle:  [32m(88.8, 292.1) -> (522.0, 441.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Department of Computer Science, City University of Hong Kong
                83 Tai Chee Ave, Kowloon Tong, Hong Kong, China
                2 Department of Mathematics, Hong Kong University of Science and Technology
                Clear Water Bay, Hong Kong, China
                3 School of Computer and Communication Sciences,  ́Ecole Polytechnique F ́ed ́erale de Lausanne
                Rte Cantonale, 1015 Lausanne, Switzerland
                4 Siebel School of Computing and Data Science, University of Illinois Urbana-Champaign
                201 N Goodwin Ave, Urbana, IL 61801, USA
                † Most of the work was done when Chen Liu was with  ́Ecole Polytechnique F ́ed ́erale de Lausanne.
                ‡ The work was done when Tong Zhang was with Hong Kong University of Science and Technology.
                ∗Corresponding author[0m

Box rectangle:  [32m(90.0, 726.3) -> (409.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, Sabine S ̈usstrunk.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0950.html.[0m



=== Processing ../JMLR 2024/On the Intrinsic Structures of Spiking Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Intrinsic Structures of Spiking Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-74
                Submitted 11/23; Revised 3/24; Published 6/24[0m

Box rectangle:  [32m(135.8, 101.7) -> (476.2, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Intrinsic Structures of Spiking Neural Networks[0m

Box rectangle:  [32m(90.0, 134.7) -> (519.3, 147.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShao-Qun Zhang1,2
                ZHANGSQ@LAMDA.NJU.EDU.CN[0m

Box rectangle:  [32m(89.7, 152.4) -> (519.3, 165.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJia-Yi Chen1,2
                CHENJY@LAMDA.NJU.EDU.CN[0m

Box rectangle:  [32m(89.7, 170.1) -> (519.3, 182.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJin-Hui Wu1,3
                WUJH@LAMDA.NJU.EDU.CN[0m

Box rectangle:  [32m(90.0, 187.8) -> (519.3, 200.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGao Zhang4
                GAOZHANG0810@HOTMAIL.COM[0m

Box rectangle:  [32m(90.0, 205.5) -> (519.3, 218.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHuan Xiong5
                HUAN.XIONG@MBZUAI.AC.AE[0m

Box rectangle:  [32m(90.0, 223.3) -> (519.3, 236.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBin Gu5
                BIN.GU@MBZUAI.AC.AE[0m

Box rectangle:  [32m(90.0, 240.9) -> (519.3, 253.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhi-Hua Zhou1,3,∗
                ZHOUZH@LAMDA.NJU.EDU.CN[0m

Box rectangle:  [32m(89.6, 259.1) -> (483.1, 326.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 National Key Laboratory for Novel Software Technology, Nanjing University, China
                2 School of Intelligent Science and Technology, Nanjing University, China
                3 School of Artificial Intelligence, Nanjing University, China
                4 School of Mathematical Sciences, Jiangsu Second Normal University, China
                5 Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, UAE[0m

Box rectangle:  [32m(90.0, 365.7) -> (196.1, 375.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Michael Mahoney[0m

Box rectangle:  [32m(283.8, 393.0) -> (328.2, 404.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.6, 410.5) -> (503.8, 685.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRecent years have emerged a surge of interest in spiking neural networks (SNNs). The performance
                of SNNs hinges not only on searching apposite architectures and connection weights, similar to
                conventional artificial neural networks, but also on the meticulous configuration of their intrinsic
                structures. However, there has been a dearth of comprehensive studies examining the impact of
                intrinsic structures; thus developers often feel challenging to apply a standardized configuration of
                SNNs across diverse datasets or tasks. This work delves deep into the intrinsic structures of SNNs.
                Initially, we draw two key conclusions: (1) the membrane time hyper-parameter is intimately linked
                to the eigenvalues of the integration operation, dictating the functional topology of spiking dynamics;
                (2) various hyper-parameters of the firing-reset mechanism govern the overall firing capacity of
                an SNN, mitigating the injection ratio or sampling density of input data. These findings elucidate
                why the efficacy of SNNs hinges heavily on the configuration of intrinsic structures and lead to a
                recommendation that enhancing the adaptability of these structures contributes to improving the
                overall performance and applicability of SNNs.
                Inspired by this recognition, we propose two feasible approaches to enhance SNN learning,
                involving developing self-connection architectures and stochastic spiking neurons to augment the
                adaptability of the integration operation and firing-reset mechanism, respectively. We theoretically
                prove that (1) both methods promote the expressive property for universal approximation, (2) the
                incorporation of self-connection architectures fosters ample solutions and structural stability for
                SNNs approximating adaptive dynamical systems, (3) the stochastic spiking neurons maintain gener-
                alization bounds with an exponential reduction in Rademacher complexity. Empirical experiments
                conducted on various real-world datasets affirm the effectiveness of our proposed methods.
                Keywords: Spiking Neural Network, Intrinsic Structures, Integration Operation, Self-connection
                Architecture, Firing-Reset Mechanism, Stochastic Excitation, Rademacher Complexity[0m

Box rectangle:  [32m(95.4, 696.1) -> (258.4, 707.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m*. Zhi-Hua Zhou is the corresponding author.[0m

Box rectangle:  [32m(89.4, 726.7) -> (426.2, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Shao-Qun Zhang, Jia-Yi Chen, Jin-Hui Wu, Gao Zhang, Huan Xiong, Bin Gu, and Zhi-Hua Zhou.[0m

Box rectangle:  [32m(90.0, 741.2) -> (447.5, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1526.html.[0m



=== Processing ../JMLR 2024/On the Learnability of Out-of-distribution Detection.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Learnability of Out-of-distribution Detection.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-83
                Submitted 9/23; Published 4/24[0m

Box rectangle:  [32m(120.5, 101.6) -> (491.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Learnability of Out-of-distribution Detection[0m

Box rectangle:  [32m(88.3, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhen Fang
                zhen.fang@uts.edu.au
                Australian Artificial Intelligence Institute
                University of Technology Sydney
                61 Broadway, Ultimo NSW 2007, Australia[0m

Box rectangle:  [32m(88.4, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYixuan Li
                sharonli@cs.wisc.edu
                Department of Computer Sciences
                The University of Wisconsin Madison
                1210 W Dayton St, Madison, WI 53706, USA[0m

Box rectangle:  [32m(88.4, 239.7) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFeng Liu  
                feng.liu1@unimelb.edu.au
                School of Computing and Information Systems
                The University of Melbourne
                700 Swanston Street, Carlton VIC 3053, Australia[0m

Box rectangle:  [32m(89.4, 294.7) -> (522.0, 342.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBo Han
                bhanml@comp.hkbu.edu.hk
                Department of Computer Science
                Hong Kong Baptist University
                Kowloon Tong, Hong Kong SAR[0m

Box rectangle:  [32m(88.3, 348.5) -> (522.0, 402.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJie Lu  
                jie.lu@uts.edu.au
                Australian Artificial Intelligence Institute
                University of Technology Sydney
                61 Broadway, Ultimo NSW 2007, Australia[0m

Box rectangle:  [32m(90.0, 427.8) -> (190.0, 437.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Amos Storkey[0m

Box rectangle:  [32m(280.3, 463.3) -> (331.7, 475.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(108.8, 497.3) -> (504.1, 674.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSupervised learning aims to train a classifier under the assumption that training and test
                data are from the same distribution. To ease the above assumption, researchers have studied
                a more realistic setting: out-of-distribution (OOD) detection, where test data may come
                from classes that are unknown during training (i.e., OOD data). Due to the unavailability
                and diversity of OOD data, good generalization ability is crucial for effective OOD detection
                algorithms, and corresponding learning theory is still an open problem. To study the
                generalization of OOD detection, this paper investigates the probably approximately correct
                (PAC) learning theory of OOD detection that fits the commonly used evaluation metrics in
                the literature. First, we find a necessary condition for the learnability of OOD detection.
                Then, using this condition, we prove several impossibility theorems for the learnability of
                OOD detection under some scenarios. Although the impossibility theorems are frustrating,
                we find that some conditions of these impossibility theorems may not hold in some practical
                scenarios. Based on this observation, we next give several necessary and sufficient conditions
                to characterize the learnability of OOD detection in some practical scenarios. Lastly, we
                offer theoretical support for representative OOD detection works based on our OOD theory.[0m

Box rectangle:  [32m(109.9, 695.1) -> (473.0, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                out-of-distribution detection, weakly supervised learning, learnability[0m

Box rectangle:  [32m(89.1, 726.4) -> (302.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Zhen Fang, Yixuan Li, Feng Liu, Bo Han, Jie Lu.[0m

Box rectangle:  [32m(90.0, 740.7) -> (506.3, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/23-1257.html.[0m



=== Processing ../JMLR 2024/On the Optimality of Gaussian Kernel Based Nonparametric Tests against Smooth Alternatives.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Optimality of Gaussian Kernel Based Nonparametric Tests against Smooth Alternatives.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-62
                Submitted 10/20; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(91.0, 101.6) -> (521.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Optimality of Gaussian Kernel Based Nonparametric
                Tests against Smooth Alternatives[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTong Li
                tong.li@columbia.edu
                Ming Yuan
                ming.yuan@columbia.edu
                Department of Statistics
                Columbia University
                New York, NY 10027, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (210.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Tong Li and Ming Yuan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/20-1228.html.[0m



=== Processing ../JMLR 2024/On the Optimality of Misspecified Spectral Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Optimality of Misspecified Spectral Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 3/23; Published 6/24[0m

Box rectangle:  [32m(111.6, 101.6) -> (500.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Optimality of Misspecified Spectral Algorithms[0m

Box rectangle:  [32m(90.0, 133.9) -> (521.9, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHaobo Zhang
                zhang-hb21@mails.tsinghua.edu.cn[0m

Box rectangle:  [32m(89.5, 163.3) -> (521.9, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYicheng Li ∗
                liyc22@mails.tsinghua.edu.cn[0m

Box rectangle:  [32m(88.4, 194.5) -> (521.9, 247.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQian Lin †
                qianlin@tsinghua.edu.cn
                Center for Statistical Science, Department of Industrial Engineering
                Tsinghua University
                Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (268.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Haobo Zhang, Yicheng Li and Qian Lin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0383.html.[0m



=== Processing ../JMLR 2024/On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-58
                Submitted 11/21; Published 1/24[0m

Box rectangle:  [32m(90.1, 101.6) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn the Sample Complexity and Metastability of Heavy-tailed
                Policy Search in Continuous Control[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmrit Singh Bedi*
                amritbedi@ucf.edu
                Department of Computer Science,
                University of Central FLorida,
                Orlando, FL, USA[0m

Box rectangle:  [32m(89.4, 205.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnjaly Parayil*
                aparayil@microsoft.com
                Microsoft India, Bengaluru[0m

Box rectangle:  [32m(89.4, 235.1) -> (522.0, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJunyu Zhang
                junyuz@nus.edu.sg
                Department of Industrial Systems Engineering and Management
                National University of Singapore
                Singapore, 119077[0m

Box rectangle:  [32m(88.8, 288.7) -> (522.0, 336.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMengdi Wang
                mengdiw@princeton.edu
                Department of Electrical Engineering
                Center for Statistics and Machine Learning
                Princeton University/Deepmind, Princeton, NJ 08544[0m

Box rectangle:  [32m(89.3, 343.9) -> (522.0, 383.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlec Koppel†
                alec.koppel@jpmchase.com
                JP Morgan AI Research
                383 Madison Ave, New York, NY 10017[0m

Box rectangle:  [32m(90.0, 408.3) -> (217.2, 418.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Alexandre Proutiere[0m

Box rectangle:  [32m(280.3, 441.9) -> (331.7, 453.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(108.8, 503.7) -> (504.0, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mReinforcement learning is a framework for interactive decision-making with incentives
                sequentially revealed across time without a system dynamics model. Due to its scaling to
                continuous spaces, we focus on policy search where one iteratively improves a parameterized
                policy with stochastic policy gradient (PG) updates. In tabular Markov Decision Problems
                (MDPs), under persistent exploration and suitable parameterization, global optimality may
                be obtained. By contrast, in continuous space, the non-convexity poses a pathological
                challenge as evidenced by existing convergence results being mostly limited to stationarity
                or arbitrary local extrema. To close this gap, we step towards persistent exploration in
                continuous space through policy parameterizations defined by distributions of heavier tails
                defined by tail-index parameter α, which increases the likelihood of jumping in state space.
                Doing so invalidates smoothness conditions of the score function common to PG. Thus,
                we establish how the convergence rate to stationarity depends on the policy’s tail index
                α, a H ̈older continuity parameter, integrability conditions, and an exploration tolerance
                parameter introduced here for the first time. Further, we characterize the dependence of the
                set of local maxima on the tail index through an exit and transition time analysis of a suitably
                defined Markov chain, identifying that policies associated with L ́evy Processes of a heavier
                tail converge to wider peaks. This phenomenon yields improved stability to perturbations[0m

Box rectangle:  [32m(89.1, 726.4) -> (462.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Amrit Singh Bedi and Anjaly Parayil and Junyu Zhang and Mengdi Wang and Alec Koppel.[0m

Box rectangle:  [32m(90.0, 741.0) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-1343.html.[0m



=== Processing ../JMLR 2024/On Truthing Issues in Supervised Classification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Truthing Issues in Supervised Classification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-91
                Submitted 4/19; Revised 12/23; Published 01/24[0m

Box rectangle:  [32m(138.5, 99.7) -> (478.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Truthing Issues in Supervised Classification∗[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 188.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonathan K. Su
                su@ll.mit.edu
                MIT Lincoln Laboratory
                244 Wood Street
                Lexington, MA 02421-6426, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (179.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jonathan K. Su.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/19-301.html.[0m



=== Processing ../JMLR 2024/On Unbiased Estimation for Partially Observed Diffusions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/On Unbiased Estimation for Partially Observed Diffusions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 3/23; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(100.5, 101.6) -> (511.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOn Unbiased Estimation for Partially Observed Diffusions[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeremy Heng
                heng@essec.edu
                ESSEC Business School[0m

Box rectangle:  [32m(90.0, 163.6) -> (522.0, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeremie Houssineau
                jeremie.houssineau@ntu.edu.sg
                Division of Mathematical Sciences
                Nanyang Technological University[0m

Box rectangle:  [32m(90.0, 206.8) -> (522.0, 246.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAjay Jasra
                ajayjasra@cuhk.edu.cn
                School of Data Science
                Chinese University of Hong Kong, Shenzhen[0m

Box rectangle:  [32m(90.0, 726.3) -> (308.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jeremy Heng, Jeremie Houssineau, and Ajay Jasra.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0347.html.[0m



=== Processing ../JMLR 2024/OpenBox  A Python Toolkit for Generalized Black-box Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/OpenBox  A Python Toolkit for Generalized Black-box Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-11
                Submitted 4/23; Published 5/24[0m

Box rectangle:  [32m(110.4, 101.6) -> (501.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOpenBox: A Python Toolkit for Generalized Black-box
                Optimization[0m

Box rectangle:  [32m(90.0, 153.1) -> (522.0, 314.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHuaijun Jiang1∗
                jianghuaijun@pku.edu.cn
                Yu Shen1∗
                shenyu@pku.edu.cn
                Yang Li2∗
                thomasyngli@tencent.com
                Beicheng Xu1
                beichengxu@stu.pku.edu.cn
                Sixian Du1
                dusixian@stu.pku.edu.cn
                Wentao Zhang1
                wentao.zhang@pku.edu.cn
                Ce Zhang3
                ce.zhang@ethz.ch
                Bin Cui1
                bin.cui@pku.edu.cn
                1 Key Lab of High Confidence Software Technologies (MOE), School of CS, Peking University, China
                2 Department of Data Platform, TEG, Tencent Inc., China
                3 Department of Computer Science, ETH Z ̈urich, Switzerland
                ∗Equal contribution.[0m

Box rectangle:  [32m(90.0, 726.3) -> (485.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Huaijun Jiang, Yu Shen, Yang Li, Beicheng Xu, Sixian Du, Wentao Zhang, Ce Zhang and Bin Cui.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0537.html.[0m



=== Processing ../JMLR 2024/Open-Source Conversational AI with SpeechBrain 1 0.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Open-Source Conversational AI with SpeechBrain 1 0.pdf') ---[0m

Box rectangle:  [32m(115.9, 101.6) -> (496.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOpen-Source Conversational AI with SpeechBrain 1.0[0m

Box rectangle:  [32m(90.0, 136.4) -> (522.0, 242.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMirco Ravanelli1,2,5, Titouan Parcollet4,6, Adel Moumen3, Sylvain de Langen3, Cem
                Subakan7,2,1, Peter Plantinga2, Yingzhi Wang8, Pooneh Mousavi1,2, Luca Della Lib-
                era1,2, Artem Ploujnikov5,2, Francesco Paissan9,14, Davide Borra10, Salah Zaiem11, Zeyu
                Zhao12, Shucong Zhang4, Georgios Karakasidis12, Sung-Lin Yeh12, Pierre Champion13,
                Aku Rouhe14,18, Rudolf Braun20, Florian Mai19, Juan Zuluaga-Gomez20,21, Seyed Ma-
                hed Mousavi15, Andreas Nautsch3, Ha Nguyen3, Xuechen Liu17, Sangeet Sagar16, Jarod
                Duret3, Salima Mdhaffar3, Ga ̈elle Laperri`ere3, Mickael Rouvier3, Renato De Mori3,22,
                Yannick Est`eve3[0m

Box rectangle:  [32m(90.0, 255.7) -> (522.0, 321.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1Concordia University, 2Mila-Quebec AI Institute, 3Avignon University, 4Samsung AI Center Cam-
                bridge, 5Universit ́e de Montr ́eal, 6University of Cambridge, 7Laval University, 8Zaion, 9Fondazione
                Bruno Kessler, 10University of Bologna, 11Telecom Paris, 12University of Edinburgh, 13Inria, 14Aalto
                University, 15University of Trento, 16Saarland University, 17National Institute of Informatics - Tokyo,
                18Silo AI, 19KU Leuven, 20Idiap, 21EPFL, 22McGill University[0m

Box rectangle:  [32m(280.3, 346.7) -> (331.7, 358.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 360.5) -> (502.1, 507.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpeechBrain1 is an open-source Conversational AI toolkit based on PyTorch, focused partic-
                ularly on speech processing tasks such as speech recognition, speech enhancement, speaker
                recognition, text-to-speech, and much more. It promotes transparency and replicability by
                releasing both the pre-trained models and the complete “recipes” of code and algorithms
                required for training them. This paper presents SpeechBrain 1.0, a significant milestone in
                the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language
                processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0
                introduces new technologies to support diverse learning modalities, Large Language Model
                (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and
                modalities.
                It also includes a new benchmark repository, offering researchers a unified
                platform for evaluating models across diverse tasks.
                Keywords:
                Conversational AI, open-source, speech processing, deep learning.[0m

Box rectangle:  [32m(90.0, 527.6) -> (180.3, 539.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 549.7) -> (522.1, 682.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConversational AI is experiencing extraordinary progress, with Large Language Models
                (LLMs) and speech assistants rapidly evolving and becoming widely adopted in the daily
                lives of millions of users (McTear, 2021). However, this quick evolution poses a challenge to
                a fundamental pillar of science: reproducibility. Replicating recent findings is often difficult
                or impossible for many researchers due to limited access to data, computational resources, or
                code (Kapoor and Narayanan, 2023). The open-source community is making a remarkable
                collective effort to mitigate this “reproducibility crisis”, yet many contributors primarily
                release pre-trained models only, known as open-weight (Liesenfeld and Dingemanse, 2024).
                While this is a step forward, it is still very common for the data and algorithms used to train
                them to remain undisclosed. We helped address this problem by releasing SpeechBrain (Ra-[0m

Box rectangle:  [32m(93.7, 695.9) -> (246.2, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. https://speechbrain.github.io/[0m

Box rectangle:  [32m(90.0, 726.3) -> (273.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mirco Ravanelli, Titouan Parcollet, et al..[0m

Box rectangle:  [32m(90.0, 741.0) -> (371.6, 749.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.[0m



=== Processing ../JMLR 2024/Operator learning without the adjoint.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Operator learning without the adjoint.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 1/24; Published 9/24[0m

Box rectangle:  [32m(171.5, 101.6) -> (440.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOperator learning without the adjoint[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicolas Boull ́e
                n.boulle@imperial.ac.uk
                Department of Mathematics
                Imperial College London
                London, SW7 2AZ, UK[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDiana Halikias
                dh736@cornell.edu
                Department of Mathematics
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSamuel E. Otto
                s.otto@cornell.edu
                Sibley School of Mechanical and Aerospace Engineering
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(90.0, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlex Townsend
                townsend@cornell.edu
                Department of Mathematics
                Cornell University
                Ithaca, NY 14853, USA[0m

Box rectangle:  [32m(90.0, 726.5) -> (259.7, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Boull ́e, Halikias, Otto, and Townsend.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0162.html.[0m



=== Processing ../JMLR 2024/Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Algorithms for Stochastic Bilevel Optimization under Relaxed Smoothness Conditions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 10/23; Revised 04/24; Published 05/24[0m

Box rectangle:  [32m(90.3, 101.5) -> (522.1, 140.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Algorithms for Stochastic Bilevel Optimization under
                Relaxed Smoothness Conditions ∗[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXuxing Chen
                xuxchen@ucdavis.edu
                Department of Mathematics
                University of California, Davis[0m

Box rectangle:  [32m(89.3, 193.2) -> (522.0, 217.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTesi Xiao †
                tesixiao.stats@gmail.com
                Amazon Inc.[0m

Box rectangle:  [32m(88.3, 224.7) -> (522.0, 263.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKrishnakumar Balasubramanian
                kbala@ucdavis.edu
                Department of Statistics
                University of California, Davis[0m

Box rectangle:  [32m(90.0, 726.3) -> (348.9, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xuxing Chen, Tesi Xiao and Krishnakumar Balasubramanian.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1323.html.[0m



=== Processing ../JMLR 2024/Optimal Bump Functions for Shallow ReLU networks  Weight Decay  Depth Separation  Curse of Dimensionality.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Bump Functions for Shallow ReLU networks  Weight Decay  Depth Separation  Curse of Dimensionality.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 11/22; Revised 8/23; Published 1/24[0m

Box rectangle:  [32m(100.1, 102.0) -> (511.8, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Bump Functions for Shallow ReLU networks:
                Weight Decay, Depth Separation, Curse of Dimensionality[0m

Box rectangle:  [32m(90.0, 155.2) -> (522.0, 220.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStephan Wojtowytsch
                s.woj@pitt.edu
                University of Pittsburgh
                Department of Mathematics
                Thackeray Hall
                Pittsburgh, PA 15221, USA[0m

Box rectangle:  [32m(90.0, 245.3) -> (179.9, 255.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Joan Bruna[0m

Box rectangle:  [32m(280.3, 278.9) -> (331.7, 290.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 297.9) -> (502.1, 367.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this note, we study how neural networks with a single hidden layer and ReLU activation
                interpolate data drawn from a radially symmetric distribution with target labels 1 at the
                origin and 0 outside the unit ball, if no labels are known inside the unit ball. With weight
                decay regularization and in the infinite neuron, infinite data limit, we prove that a unique
                radially symmetric minimizer exists, whose average parameters and Lipschitz constant grow
                as d and[0m

Box rectangle:  [32m(150.0, 349.0) -> (158.3, 366.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m√[0m

Box rectangle:  [32m(109.9, 357.7) -> (502.1, 417.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33md respectively.
                We furthermore show that the average weight variable grows exponentially in d if the
                label 1 is imposed on a ball of radius ε rather than just at the origin. By comparison,
                a neural networks with two hidden layers can approximate the target function without
                encountering the curse of dimensionality.[0m

Box rectangle:  [32m(109.9, 423.1) -> (502.1, 457.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Deep learning, depth separation, Barron space, Radon-BV, compact support,
                mollifier, weight decay, minimum norm solution, symmetry learning, explicit regularization,
                curse of dimensionality, radial symmetry.[0m

Box rectangle:  [32m(90.0, 479.0) -> (180.3, 491.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 502.8) -> (522.0, 622.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeural networks have revolutionized fields from computer vision (Krizhevsky et al., 2012) to
                natural language processing (Vaswani et al., 2017). They are the driving force behind AIs
                which play strategy games at superhuman levels of proficiency (Silver et al., 2016, 2017),
                facilitated major advances in scientific problems such as protein folding (Tunyasuvunakool
                et al., 2021; Jumper et al., 2021), and have been used for computer-assisted proofs in
                applied mathematics by Wang et al. (2022). While empirical evidence indicates that they
                often generalize well to previously unseen data when trained appropriately, there is little
                rigorous understanding of how neural networks interpolate a function between known data
                points.[0m

Box rectangle:  [32m(90.0, 626.9) -> (522.0, 705.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this article, we provide insight in the simple setting of infinitely wide ReLU networks
                with a single hidden layer and data which are drawn from a radially symmetric distribution
                on a Euclidean space Rd. The target function f∗satisfies f∗(0) = 1 and f∗(x) = 0 for
                |x| ≥1, where |·| denotes the Euclidean norm on Rd. We consider a loss functional composed
                of an l2-error and a weight decay regularizer. Despite the fact that neural networks with
                a single hidden layer cannot represent compactly supported target functions exactly (He[0m

Box rectangle:  [32m(90.0, 726.5) -> (201.5, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Stephan Wojtowytsch.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1296.html.[0m



=== Processing ../JMLR 2024/Optimal Clustering with Bandit Feedback.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Clustering with Bandit Feedback.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-54
                Submitted 9/22; Revised 3/24; Published 7/24[0m

Box rectangle:  [32m(157.8, 101.6) -> (454.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Clustering with Bandit Feedback[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJunwen Yang
                junwen yang@u.nus.edu
                Institute of Operations Research and Analytics
                National University of Singapore
                117602, Singapore[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZixin Zhong
                zixinzhong@hkust-gz.edu.cn
                Thrust of Data Science and Analytics
                Hong Kong University of Science and Technology (Guangzhou)
                511453, Guangzhou, Guangdong, China[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 322.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVincent Y. F. Tan
                vtan@nus.edu.sg
                Department of Mathematics
                Department of Electrical and Computer Engineering
                Institute of Operations Research and Analytics
                National University of Singapore
                119076, Singapore[0m

Box rectangle:  [32m(90.0, 726.3) -> (307.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Junwen Yang, Zixin Zhong and Vincent Y. F. Tan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1088.html.[0m



=== Processing ../JMLR 2024/Optimal Decision Tree and Adaptive Submodular Ranking with Noisy Outcomes.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Decision Tree and Adaptive Submodular Ranking with Noisy Outcomes.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research volume (2024) 1-42
                Submitted 11/23; Revised 6/24; Published 12/24[0m

Box rectangle:  [32m(98.5, 101.6) -> (513.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Decision Tree and Adaptive Submodular Ranking
                with Noisy Outcomes[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSu Jia
                sj693@cornell.edu
                Center of Data Science for Enterprise and Society
                Cornell University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 217.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFatemeh Navidi
                navidi@umich.edu
                Midpoint Markets[0m

Box rectangle:  [32m(90.0, 223.2) -> (522.0, 259.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mViswanath Nagarajan
                viswa@umich.edu
                Industrial & Operations Engineering
                University of Michigan[0m

Box rectangle:  [32m(90.0, 266.4) -> (522.0, 305.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mR. Ravi
                ravi@andrew.cmu.edu
                Tepper School of Business
                Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 726.3) -> (340.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Su Jia, Fatemeh Navidi, Viswanath Nagarajan and R. Ravi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/vvolume/23-1484.html.[0m



=== Processing ../JMLR 2024/Optimal First-Order Algorithms as a Function of Inequalities.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal First-Order Algorithms as a Function of Inequalities.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-69
                Submitted 10/21; Revised 3/23; Published 2/24[0m

Box rectangle:  [32m(94.1, 101.5) -> (518.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal First-Order Algorithms as a Function of Inequalities[0m

Box rectangle:  [32m(89.4, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChanwoo Park
                cpark97@mit.edu
                Department of Electrical Engineering and Computer Science
                Massachusetts Institute of Technology
                Massachusetts, United States of America[0m

Box rectangle:  [32m(89.4, 201.0) -> (522.0, 267.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErnest K. Ryu
                eryu@snu.ac.kr
                Department of Mathematical Sciences
                Interdisciplinary Program in Artificial Intelligence
                Seoul National University
                Seoul, Korea[0m

Box rectangle:  [32m(90.0, 726.3) -> (236.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Chanwoo Park and Ernest Ryu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1256.html.[0m



=== Processing ../JMLR 2024/Optimal Learning Policies for Differential Privacy in Multi-armed Bandits.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Learning Policies for Differential Privacy in Multi-armed Bandits.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 10/21; Revised 11/23; Published 9/24[0m

Box rectangle:  [32m(120.8, 101.6) -> (491.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Learning Policies for Differential Privacy in
                Multi-armed Bandits[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSiwei Wang†
                siweiwang@microsoft.com
                Microsoft Research Asia
                Beijing, China[0m

Box rectangle:  [32m(90.0, 194.8) -> (522.0, 247.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun Zhu∗
                dcszj@tsinghua.edu.cn
                Department of Computer Science and Technology, BNRist Center, Tsinghua AI Institute, Tsinghua-
                Bosch Joint ML Center, Tsinghua University, Beijing, China
                Pazhou Laboratory (Huangpu), Guangzhou, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (213.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Siwei Wang and Jun Zhu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1267.html.[0m



=== Processing ../JMLR 2024/Optimal Locally Private Nonparametric Classification with Public Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Locally Private Nonparametric Classification with Public Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-62
                Submitted 11/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(115.9, 101.6) -> (496.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Locally Private Nonparametric Classification
                with Public Data[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuheng Ma
                yma@ruc.edu.cn
                School of Statistics
                Renmin University of China
                100872 Beijing, China[0m

Box rectangle:  [32m(90.0, 213.3) -> (522.0, 266.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanfang Yang
                hyang@ruc.edu.cn
                Center for Applied Statistics, School of Statistics
                Renmin University of China
                100872 Beijing, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (235.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuheng Ma and Hanfang Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1563.html.[0m



=== Processing ../JMLR 2024/Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 2/23; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(105.3, 101.6) -> (506.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Scaling for the Proximal Langevin Algorithm in
                High Dimensions[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNatesh S. Pillai
                pillai@fas.harvard.edu
                Department of Statistics
                Harvard University
                MA 02138, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (178.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Natesh S. Pillai.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0139.html.[0m



=== Processing ../JMLR 2024/Optimal Weighted Random Forests.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimal Weighted Random Forests.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-81
                Submitted 5/23; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(184.0, 101.5) -> (428.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal Weighted Random Forests[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 193.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXinyu Chen
                SA21204192@mail.ustc.edu.cn
                International Institute of Finance
                School of Management
                University of Science and Technology of China
                Hefei, 230026, Anhui, China[0m

Box rectangle:  [32m(90.0, 211.0) -> (522.0, 259.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDalei Yu∗
                yudalei@126.com
                School of Mathematics and Statistics
                Xi’an Jiaotong University
                Xi’an, 710049, Shaanxi, China[0m

Box rectangle:  [32m(90.0, 278.5) -> (522.0, 385.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXinyu Zhang
                xinyu@amss.ac.cn
                Academy of Mathematics and Systems Science
                Chinese Academy of Sciences
                Beijing, 100190, China
                International Institute of Finance
                School of Management
                University of Science and Technology of China
                Hefei, 230026, Anhui, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (270.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xinyu Chen, Dalei Yu and Xinyu Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0607.html.[0m



=== Processing ../JMLR 2024/Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-62
                Submitted 8/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(107.9, 102.0) -> (504.1, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimistic Online Mirror Descent for Bridging
                Stochastic and Adversarial Online Convex Optimization[0m

Box rectangle:  [32m(90.0, 153.6) -> (522.0, 176.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSijia Chen
                chensj@lamda.nju.edu.cn
                National Key Laboratory for Novel Software Technology, Nanjing University, China[0m

Box rectangle:  [32m(90.0, 183.3) -> (522.0, 205.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYu-Jie Zhang
                yujie.zhang@ms.k.u-tokyo.ac.jp
                The University of Tokyo, Chiba, Japan[0m

Box rectangle:  [32m(90.0, 213.0) -> (522.0, 235.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWei-Wei Tu
                tuwwcn@gmail.com
                Artificial Productivity Inc., Beijing, China[0m

Box rectangle:  [32m(90.0, 242.7) -> (518.2, 253.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeng Zhao
                zhaop@lamda.nju.edu.cn[0m

Box rectangle:  [32m(90.0, 260.0) -> (522.0, 299.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLijun Zhang∗
                zhanglj@lamda.nju.edu.cn
                National Key Laboratory for Novel Software Technology, Nanjing University, China
                School of Artificial Intelligence, Nanjing University, China[0m

Box rectangle:  [32m(90.0, 324.9) -> (212.5, 334.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Francesco Orabona[0m

Box rectangle:  [32m(280.3, 358.6) -> (331.7, 370.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 376.4) -> (502.1, 434.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe stochastically extended adversarial (SEA) model, introduced by Sachs et al. (2022),
                serves as an interpolation between stochastic and adversarial online convex optimization.
                Under the smoothness condition on expected loss functions, it is shown that the ex-
                pected static regret of optimistic follow-the-regularized-leader (FTRL) depends on the
                cumulative stochastic variance σ2[0m

Box rectangle:  [32m(257.5, 422.9) -> (476.2, 436.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T and the cumulative adversarial variation Σ2[0m

Box rectangle:  [32m(109.9, 424.2) -> (502.1, 458.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T for
                convex functions. Sachs et al. (2022) also provide a regret bound based on the maxi-
                mal stochastic variance σ2[0m

Box rectangle:  [32m(224.9, 446.8) -> (433.8, 460.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax and the maximal adversarial variation Σ2[0m

Box rectangle:  [32m(109.9, 448.1) -> (502.1, 501.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax for strongly
                convex functions.
                Inspired by their work, we investigate the theoretical guarantees of
                optimistic online mirror descent (OMD) for the SEA model with smooth expected loss
                functions.
                For convex and smooth functions, we obtain the same O([0m

Box rectangle:  [32m(426.3, 475.0) -> (436.2, 484.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰁳[0m

Box rectangle:  [32m(468.5, 475.0) -> (478.5, 484.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰁳[0m

Box rectangle:  [32m(436.2, 482.8) -> (446.2, 494.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mσ2[0m

Box rectangle:  [32m(478.5, 482.8) -> (489.7, 494.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mΣ2[0m

Box rectangle:  [32m(109.9, 484.0) -> (502.1, 536.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T )
                regret bound, but with a relaxation of the convexity requirement from individual func-
                tions to expected functions. For strongly convex and smooth functions, we establish an
                O[0m

Box rectangle:  [32m(441.9, 484.0) -> (465.3, 496.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T +[0m

Box rectangle:  [32m(119.8, 511.3) -> (129.9, 525.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀃1[0m

Box rectangle:  [32m(133.1, 511.3) -> (137.7, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀃[0m

Box rectangle:  [32m(193.2, 511.3) -> (197.7, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀄[0m

Box rectangle:  [32m(213.9, 511.3) -> (223.1, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀃󰀃[0m

Box rectangle:  [32m(273.1, 511.3) -> (277.7, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀄[0m

Box rectangle:  [32m(286.0, 511.3) -> (290.6, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀃[0m

Box rectangle:  [32m(346.0, 511.3) -> (359.7, 521.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m󰀄󰀄󰀄[0m

Box rectangle:  [32m(137.7, 518.5) -> (147.7, 529.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mσ2[0m

Box rectangle:  [32m(143.4, 518.5) -> (181.9, 532.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax + Σ2[0m

Box rectangle:  [32m(223.1, 518.5) -> (233.1, 529.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mσ2[0m

Box rectangle:  [32m(228.7, 518.5) -> (264.6, 532.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T + Σ2[0m

Box rectangle:  [32m(290.6, 518.5) -> (300.6, 529.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mσ2[0m

Box rectangle:  [32m(296.3, 518.5) -> (334.8, 532.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax + Σ2[0m

Box rectangle:  [32m(361.9, 518.5) -> (492.7, 536.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mbound, better than their O((σ2[0m

Box rectangle:  [32m(199.4, 519.9) -> (212.1, 529.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mlog[0m

Box rectangle:  [32m(279.4, 519.9) -> (284.3, 529.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m/[0m

Box rectangle:  [32m(109.9, 524.6) -> (503.1, 541.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax
                +Σ2[0m

Box rectangle:  [32m(178.0, 524.6) -> (192.7, 531.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax[0m

Box rectangle:  [32m(330.8, 524.6) -> (345.5, 531.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax[0m

Box rectangle:  [32m(260.6, 524.9) -> (271.5, 531.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T[0m

Box rectangle:  [32m(125.6, 525.5) -> (130.3, 532.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mλ[0m

Box rectangle:  [32m(109.9, 531.8) -> (502.1, 560.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mmax) log T) result. For exp-concave and smooth functions, our approach yields a new
                O(d log(σ2[0m

Box rectangle:  [32m(151.3, 542.4) -> (188.4, 556.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T + Σ2[0m

Box rectangle:  [32m(109.9, 543.8) -> (502.1, 613.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1:T )) bound. Moreover, we introduce the first expected dynamic regret
                guarantee for the SEA model with convex and smooth expected functions, which is more
                favorable than static regret bounds in non-stationary environments. Furthermore, we ex-
                pand our investigation to scenarios with non-smooth expected loss functions and propose
                novel algorithms built upon optimistic OMD with an implicit update, successfully attaining
                both static and dynamic regret guarantees.[0m

Box rectangle:  [32m(90.0, 634.4) -> (180.3, 646.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 657.0) -> (522.0, 681.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOnline convex optimization (OCO) is a fundamental framework for online learning and has
                been applied in a variety of real-world applications such as spam filtering and portfolio[0m

Box rectangle:  [32m(93.7, 695.9) -> (195.9, 711.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Corresponding author.[0m

Box rectangle:  [32m(90.0, 726.5) -> (374.5, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc©2024 Sijia Chen, Yu-Jie Zhang, Wei-Wei Tu, Peng Zhao, and Lijun Zhang.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.1, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1072.html.[0m



=== Processing ../JMLR 2024/Optimistic Search  Change Point Estimation for Large-scale Data via Adaptive Logarithmic Queries.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimistic Search  Change Point Estimation for Large-scale Data via Adaptive Logarithmic Queries.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-64
                Submitted 7/23; Revised 4/24; Published 9/24[0m

Box rectangle:  [32m(93.8, 101.6) -> (518.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimistic Search: Change Point Estimation for Large-scale
                Data via Adaptive Logarithmic Queries[0m

Box rectangle:  [32m(90.0, 151.5) -> (518.2, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSolt Kov ́acs1,∗
                kovacssolt@gmail.com[0m

Box rectangle:  [32m(90.0, 169.2) -> (518.2, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHousen Li2,3,∗
                housen.li@mathematik.uni-goettingen.de[0m

Box rectangle:  [32m(90.0, 187.1) -> (518.2, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLorenz Haubner1
                lorenz.haubner@gmail.com[0m

Box rectangle:  [32m(90.0, 204.8) -> (518.2, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAxel Munk2,3
                munk@math.uni-goettingen.de[0m

Box rectangle:  [32m(90.0, 224.1) -> (522.0, 290.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter B ̈uhlmann1
                peter.buehlmann@stat.math.ethz.ch
                1Seminar f ̈ur Statistik, ETH Z ̈urich, 8092 Z ̈urich, Switzerland
                2Institute for Mathematical Stochastics, University of G ̈ottingen, 37077 G ̈ottingen, Germany
                3Cluster of Excellence Multiscale Bioimaging (MBExC), University of G ̈ottingen, Germany
                ∗The first two authors contributed equally to this work[0m

Box rectangle:  [32m(90.0, 726.3) -> (398.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Solt Kov ́acs, Housen Li, Lorenz Haubner, Axel Munk and Peter B ̈uhlmann.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0871.html.[0m



=== Processing ../JMLR 2024/Optimization-based Causal Estimation from Heterogeneous Environments.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimization-based Causal Estimation from Heterogeneous Environments.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 8/21; Revised 12/22; Published 4/24[0m

Box rectangle:  [32m(169.9, 101.6) -> (442.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimization-based Causal Estimation
                from Heterogeneous Environments[0m

Box rectangle:  [32m(88.2, 151.9) -> (521.9, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMingzhang Yin
                mingzhang.yin@warrington.ufl.edu
                Warrington College of Business
                University of Florida
                Gainesville, FL, 32611, USA[0m

Box rectangle:  [32m(88.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYixin Wang
                yixinw@umich.edu
                Department of Statistics
                University of Michigan
                Ann Arbor, MI, 48109, USA[0m

Box rectangle:  [32m(88.8, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid M. Blei
                david.blei@columbia.edu
                Department of Computer Science and Department of Statistics
                Columbia University
                New York, NY, 10027, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (284.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mingzhang Yin, Yixin Wang, David M. Blei.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1028.html.[0m



=== Processing ../JMLR 2024/Optimizing Noise for f-Differential Privacy via Anti-Concentration and Stochastic Dominance.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Optimizing Noise for f-Differential Privacy via Anti-Concentration and Stochastic Dominance.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 12/23; Revised 11/24; Published 11/24[0m

Box rectangle:  [32m(128.2, 82.7) -> (483.9, 115.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimizing Noise for f-Differential Privacy
                via Anti-Concentration and Stochastic Dominance[0m

Box rectangle:  [32m(72.0, 133.0) -> (540.0, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJordan Awan
                jawan@purdue.edu
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(72.0, 188.2) -> (540.0, 241.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAishwarya Ramasethu
                aishwarya.ramasethu@gmail.com
                Department of Statistics
                Purdue University
                West Lafayette, IN 47907, USA[0m

Box rectangle:  [32m(72.0, 266.1) -> (172.6, 276.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Pierre Alquier[0m

Box rectangle:  [32m(280.3, 301.7) -> (331.7, 313.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(91.9, 323.6) -> (520.1, 489.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this paper, we establish anti-concentration inequalities for additive noise mechanisms which
                achieve f-differential privacy (f-DP), a notion of privacy phrased in terms of a tradeofffunction
                f which limits the ability of an adversary to determine which individuals were in the database.
                We show that canonical noise distributions (CNDs), proposed by Awan and Vadhan (2023), match
                the anti-concentration bounds at half-integer values, indicating that their tail behavior is near-
                optimal. We also show that all CNDs are sub-exponential, regardless of the f-DP guarantee. In
                the case of log-concave CNDs, we show that they are the stochastically smallest noise compared to
                any other noise distributions with the same strong privacy guarantee. In terms of integer-valued
                noise, we propose a new notion of discrete CND and prove that a discrete CND always exists,
                can be constructed by rounding a continuous CND, and that the discrete CND is unique when
                designed for a statistic with sensitivity 1. We further show that the discrete CND at sensitivity 1 is
                stochastically smallest compared to other integer-valued noises. Our theoretical results shed light
                on the different types of privacy guarantees possible in the f-DP framework and can be incorporated
                in more complex mechanisms to optimize performance.[0m

Box rectangle:  [32m(91.9, 497.3) -> (520.1, 519.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                canonical noise distribution, total variation, discrete noise, log-concave distribution,
                sub-exponential distribution[0m

Box rectangle:  [32m(72.0, 544.1) -> (162.3, 556.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(72.0, 570.8) -> (540.1, 635.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDifferential privacy (DP), introduced by Dwork et al. (2006b), is the state-of-the-art framework
                for formal privacy protection. DP methods require the introduction of noise into data analyses,
                which obscures the contribution of any particular individual. Since its inception, DP has grown
                in popularity and is now employed by leading tech giants like Google (Erlingsson et al., 2014) and
                Apple (Tang et al., 2017), as well as by the US Census Bureau (Abowd, 2018).[0m

Box rectangle:  [32m(72.0, 643.7) -> (540.1, 722.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhile there are several variants of DP, they all quantify the privacy risk in terms of a similarity
                measure between the distributions of outputs, when the mechanism is applied to two databases
                that differ by just one individual’s data.
                The differences between these variants are primarily
                in the specific similarity measure. Recently, Dong et al. (2022) proposed f-DP, which is rooted
                in hypothesis testing.
                The f parameter in f-DP is a function which offers a more expressive
                quantification of the privacy risk compared to other notions of DP. The f-DP framework has the[0m

Box rectangle:  [32m(72.0, 743.5) -> (254.8, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Jordan Awan and Aishwarya Ramasethu.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1624.html.[0m



=== Processing ../JMLR 2024/Overparametrized Multi-layer Neural Networks  Uniform Concentration of Neural Tangent Kernel and Convergence of Stochastic Gradient Descent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Overparametrized Multi-layer Neural Networks  Uniform Concentration of Neural Tangent Kernel and Convergence of Stochastic Gradient Descent.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-83
                Submitted 6/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(90.4, 101.6) -> (521.7, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOverparametrized Multi-layer Neural Networks: Uniform
                Concentration of Neural Tangent Kernel and Convergence of
                Stochastic Gradient Descent∗[0m

Box rectangle:  [32m(90.0, 169.8) -> (522.0, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiaming Xu
                jx77@duke.edu
                The Fuqua School of Business
                Duke University
                Durham, NC 27708, USA[0m

Box rectangle:  [32m(90.0, 225.0) -> (522.0, 277.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanjing Zhu
                hz176@duke.edu
                The Fuqua School of Business
                Duke University
                Durham, NC 27708, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (229.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiaming Xu and Hanjing Zhu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (512.2, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0740.html.[0m



=== Processing ../JMLR 2024/PAMI  An Open-Source Python Library for Pattern Mining.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PAMI  An Open-Source Python Library for Pattern Mining.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-6
                Submitted 9/22; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(93.4, 101.6) -> (518.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPAMI: An Open-Source Python Library for Pattern Mining[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mR. Uday Kiran
                uday.rage@gmail.com
                The University of Aizu
                Aizu-Wakamatsu, Fukushima, 965-8580, Japan[0m

Box rectangle:  [32m(90.0, 175.6) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mP. Veena
                rage.vinny@gmail.com
                The University of Aizu
                Aizu-Wakamatsu, Fukushima, 965-8580, Japan[0m

Box rectangle:  [32m(90.0, 217.2) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMasashi Toyoda
                toyoda@tkl.iis.u-tokyo.ac.jp
                Institute of Industrial Science, The University of Tokyo
                Tokyo, 153-8505, Japan[0m

Box rectangle:  [32m(90.0, 260.4) -> (522.0, 326.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMasaru Kitsuregawa
                kitsure@tkl.iis.u-tokyo.ac.jp
                Research Organization of Information and Systems,
                Tokyo, 105-0001, Japan,
                The University of Tokyo,
                Tokyo, 113-8654, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (440.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Uday Kiran RAGE, Veena PAMALLA, Masashi TOYODA, Masaru KITSUREGAWA.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1026.html.[0m



=== Processing ../JMLR 2024/PAPAL  A Provable PArticle-based Primal-Dual ALgorithm for Mixed Nash Equilibrium.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PAPAL  A Provable PArticle-based Primal-Dual ALgorithm for Mixed Nash Equilibrium.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 11/23; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(92.8, 101.6) -> (519.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPAPAL: A Provable PArticle-based Primal-Dual ALgorithm
                for Mixed Nash Equilibrium[0m

Box rectangle:  [32m(89.5, 151.5) -> (522.0, 176.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShihong Ding∗
                dingshihong@stu.pku.edu.cn
                Peking University[0m

Box rectangle:  [32m(89.5, 181.2) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanze Dong∗†
                hendrydong@gmail.com
                Salesforce AI Research[0m

Box rectangle:  [32m(89.5, 209.8) -> (522.0, 235.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCong Fang  
                fangcong@pku.edu.cn
                Peking University[0m

Box rectangle:  [32m(89.5, 240.9) -> (522.0, 265.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhouchen Lin
                zlin@pku.edu.cn
                Peking University[0m

Box rectangle:  [32m(88.3, 272.1) -> (522.0, 297.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTong Zhang
                tongzhang@tongzhang-ml.org
                University of Illinois Urbana-Champaign[0m

Box rectangle:  [32m(90.0, 725.2) -> (380.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shihong Ding∗, Hanze Dong∗, Cong Fang, Zhouchen Lin, Tong Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1522.html.[0m



=== Processing ../JMLR 2024/Parallel-in-Time Probabilistic Numerical ODE Solvers.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Parallel-in-Time Probabilistic Numerical ODE Solvers.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 10/23; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(114.4, 101.6) -> (497.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mParallel-in-Time Probabilistic Numerical ODE Solvers[0m

Box rectangle:  [32m(88.4, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNathanael Bosch
                nathanael.bosch@uni-tuebingen.de
                T ̈ubingen AI Center, University of T ̈ubingen[0m

Box rectangle:  [32m(89.4, 163.6) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdrien Corenflos
                adrien.corenflos.stats@gmail.com
                Department of Electrical Engineering and Automation, Aalto University[0m

Box rectangle:  [32m(89.4, 193.3) -> (522.0, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFatemeh Yaghoobi
                fatemeh.yaghoobi@aalto.fi
                Department of Electrical Engineering and Automation, Aalto University[0m

Box rectangle:  [32m(88.8, 222.9) -> (522.0, 247.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFilip Tronarp
                filip.tronarp@matstat.lu.se
                Center for Mathematical Sciences, Lund University[0m

Box rectangle:  [32m(88.4, 252.6) -> (522.0, 276.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPhilipp Hennig
                philipp.hennig@uni-tuebingen.de
                T ̈ubingen AI Center, University of T ̈ubingen[0m

Box rectangle:  [32m(89.4, 283.9) -> (522.0, 309.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSimo S ̈arkk ̈a
                simo.sarkka@aalto.fi
                Department of Electrical Engineering and Automation, Aalto University[0m

Box rectangle:  [32m(90.0, 334.7) -> (185.3, 344.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ryan Adams[0m

Box rectangle:  [32m(280.3, 370.3) -> (331.7, 382.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.3, 388.7) -> (503.5, 568.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mProbabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical
                simulation of dynamical systems as problems of Bayesian state estimation. Aside from
                producing posterior distributions over ODE solutions and thereby quantifying the numerical
                approximation error of the method itself, one less-often noted advantage of this formalism
                is the algorithmic flexibility gained by formulating numerical simulation in the framework
                of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build
                on the time-parallel formulation of iterated extended Kalman smoothers to formulate a
                parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical
                system sequentially in time, as done by current probabilistic solvers, the proposed method
                processes all time steps in parallel and thereby reduces the computational complexity from
                linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our
                approach on a variety of ODEs and compare it to a range of both classic and probabilistic
                numerical ODE solvers.
                Keywords: probabilistic numerics, ordinary differential equations, numerical analysis,
                parallel-in-time methods, Bayesian filtering and smoothing.[0m

Box rectangle:  [32m(90.0, 589.9) -> (180.3, 601.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(88.7, 613.0) -> (524.2, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOrdinary differential equations (ODEs) are used throughout the sciences to describe the
                evolution of dynamical systems over time. In machine learning, ODEs provide a continuous
                description of certain neural networks (Chen et al., 2018) and optimization procedures
                (Helmke et al., 2012; Su et al., 2016), and are used in generative modeling with normalizing
                flows (Papamakarios et al., 2021) and diffusion models (Song et al., 2021), among others.
                Unfortunately, all but the simplest ODEs are too complex to be solved analytically. Therefore,
                numerical methods are required to obtain a solution. While a multitude of numerical solvers[0m

Box rectangle:  [32m(89.1, 726.4) -> (506.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Nathanael Bosch, Adrien Corenflos, Fatemeh Yaghoobi, Filip Tronarp, Philipp Hennig and Simo S ̈arkk ̈a.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1261.html.[0m



=== Processing ../JMLR 2024/Pareto Smoothed Importance Sampling.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pareto Smoothed Importance Sampling.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-58
                Submitted 7/19; Revised 8/22; Published 3/24[0m

Box rectangle:  [32m(166.1, 101.6) -> (446.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPareto Smoothed Importance Sampling[0m

Box rectangle:  [32m(89.3, 133.9) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAki Vehtari
                aki.vehtari@aalto.fi
                Department of Computer Science
                Aalto University[0m

Box rectangle:  [32m(89.4, 187.5) -> (522.0, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaniel Simpson
                dan@normalcomputing.ai
                Normal Computing[0m

Box rectangle:  [32m(88.8, 229.1) -> (522.0, 265.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrew Gelman
                gelman@stat.columbia.edu
                Departments of Statistics and Political Science
                Columbia University[0m

Box rectangle:  [32m(88.8, 282.7) -> (522.0, 318.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuling Yao
                yyao@flatironinstitute.org
                Center for Computational Mathematics
                Flatiron Institute[0m

Box rectangle:  [32m(88.8, 337.9) -> (522.0, 377.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonah Gabry
                jgabry@gmail.com
                Department of Statistics
                Columbia University[0m

Box rectangle:  [32m(90.0, 726.3) -> (407.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry.[0m

Box rectangle:  [32m(90.0, 741.0) -> (512.2, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/19-556.html.[0m



=== Processing ../JMLR 2024/Pearl  A Production-Ready Reinforcement Learning Agent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pearl  A Production-Ready Reinforcement Learning Agent.pdf') ---[0m

Box rectangle:  [32m(90.0, 23.7) -> (522.0, 30.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-30
                Submitted 2/24; Revised 6/24; Published 8/24[0m

Box rectangle:  [32m(101.5, 80.7) -> (510.9, 95.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPearl: A Production-Ready Reinforcement Learning Agent[0m

Box rectangle:  [32m(90.0, 113.2) -> (523.4, 147.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZheqing Zhu*, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel R. Jiang, Yi Wan, Yonathan Efroni,
                Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank
                Cheng, Zheng Wu, Wanqiao Xu[0m

Box rectangle:  [32m(88.5, 159.0) -> (335.1, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mApplied Reinforcement Learning Team, AI at Meta
                *Corresponding author. Please email billzhu@meta.com.[0m

Box rectangle:  [32m(90.0, 205.3) -> (164.2, 214.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Zeyi Wen[0m

Box rectangle:  [32m(280.3, 238.3) -> (331.7, 250.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.5, 254.5) -> (503.9, 374.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mReinforcement learning (RL) is a versatile framework for optimizing long-term goals. Although
                many real-world problems can be formalized with RL, learning and deploying a performant
                RL policy requires a system designed to address several important challenges, including the
                exploration-exploitation dilemma, partial observability, dynamic action spaces, and safety concerns.
                While the importance of these challenges has been well recognized, existing open-source RL
                libraries do not explicitly address them.
                This paper introduces Pearl, a Production-Ready
                RL software package designed to embrace these challenges in a modular way. In addition to
                presenting benchmarking results, we also highlight examples of Pearl’s ongoing industry adoption
                to demonstrate its advantages for production use cases. Pearl is open sourced on GitHub at
                github.com/facebookresearch/pearl and its official website is pearlagent.github.io.
                Keywords:
                Reinforcement learning, open-source software, Python, PyTorch[0m

Box rectangle:  [32m(90.0, 389.2) -> (180.2, 401.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.6, 410.0) -> (523.4, 718.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe field of reinforcement learning (RL) has achieved significant successes in recent years, including
                surpassing human-level performance in games (Mnih et al., 2015; Silver et al., 2017), controlling robots
                in complex manipulation tasks (Peng et al., 2018; Levine et al., 2016; Lee et al., 2020), optimizing
                large scale recommender systems (Xu et al., 2023), and fine-tuning of large language models (Ouyang
                et al., 2022). At the same time, open-source RL libraries, such as Tianshou (Weng et al., 2022) and
                Stable-Baselines 3 (Raffin et al., 2021), have emerged as important tools for reproducible research in
                RL. However, these existing libraries focus mostly on implementing different algorithms for policy
                learning, i.e. methods to learn a policy from a stream of data. Even though this is a core feature
                in RL, there is more to designing an RL solution that can be used for real-world applications and
                production systems. For example, how should the stream of data be collected? This is known
                as exploration, which is a well-known challenge in RL (Sutton and Barto, 2018). Another key
                question that system designers face when deploying RL solutions is: how should safety concerns be
                incorporated (Dulac-Arnold et al., 2019)?
                In this paper, we introduce Pearl, an open-source software package that aims to enable users
                to easily design practical RL solutions. Beyond just standard policy learning, Pearl also allows
                users to effortlessly incorporate additional features into their RL solutions, including intelligent
                exploration, safety constraints, risk preferences, state estimation under partial observability, and
                dynamic action spaces. Many of these features are critical for taking RL from research to deployment
                and are largely overlooked in existing libraries. To do this, we focus on agent modularity, i.e., a
                modular design of an RL agent that allows users to mix and match various features. Pearl is built
                on native PyTorch, supports GPU-enabled training, adheres to software engineering best practices,
                and is designed for distributed training, testing, and evaluation. Remaining sections of the paper
                give details about Pearl’s design and features, a comparison with existing open-source RL libraries,
                and a few examples of Pearl’s ongoing adoption in industry applications. We provide examples of
                using Pearl in Appendix A. Results of benchmarking experiments on various testbeds are shown in
                Appendix B.[0m

Box rectangle:  [32m(90.0, 744.2) -> (507.6, 759.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc
                ⃝2024 Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel R. Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang,
                Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu.[0m

Box rectangle:  [32m(90.0, 765.4) -> (489.1, 780.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-0196.html.[0m



=== Processing ../JMLR 2024/Penalized Overdamped and Underdamped Langevin Monte Carlo Algorithms for Constrained Sampling.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Penalized Overdamped and Underdamped Langevin Monte Carlo Algorithms for Constrained Sampling.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 12/22; Published 4/24[0m

Box rectangle:  [32m(121.6, 101.6) -> (490.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPenalized Overdamped and Underdamped Langevin
                Monte Carlo Algorithms for Constrained Sampling[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMert G ̈urb ̈uzbalaban
                mg1366@rutgers.edu
                Department of Management Science and Information Systems
                Rutgers Business School
                Piscataway, NJ 08854, United States of America[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanhan Hu
                yh586@scarletmail.rutgers.edu
                Department of Management Science and Information Systems
                Rutgers Business School
                Piscataway, NJ 08854, United States of America[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLingjiong Zhu
                zhu@math.fsu.edu
                Department of Mathematics
                Florida State University
                Tallahassee, FL 32306, United States of America[0m

Box rectangle:  [32m(90.0, 726.3) -> (307.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mert G ̈urb ̈uzbalaban, Yuanhan Hu, Lingjiong Zhu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1443.html.[0m



=== Processing ../JMLR 2024/Permuted and Unlinked Monotone Regression in R^d  an approach based on mixture modeling and optimal transport.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Permuted and Unlinked Monotone Regression in R^d  an approach based on mixture modeling and optimal transport.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 1/22; Published 5/24[0m

Box rectangle:  [32m(94.1, 100.6) -> (518.1, 136.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPermuted and Unlinked Monotone Regression in Rd: an
                approach based on mixture modeling and optimal transport[0m

Box rectangle:  [32m(90.0, 154.0) -> (522.0, 202.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartin Slawski
                mslawsk3@gmu.edu
                Department of Statistics
                George Mason University
                Fairfax, VA 22030-4444, USA[0m

Box rectangle:  [32m(90.0, 209.2) -> (522.0, 262.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBodhisattva Sen
                bodhi@stat.columbia.edu
                Department of Statistics
                Columbia University
                New York, NY 10027, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (256.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Martin Slawski and Bodhisattva Sen.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0058.html.[0m



=== Processing ../JMLR 2024/Personalized PCA  Decoupling Shared and Unique Features.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Personalized PCA  Decoupling Shared and Unique Features.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-82
                Submitted 7/22; Revised 8/23; Published 2/24[0m

Box rectangle:  [32m(94.2, 101.6) -> (517.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPersonalized PCA: Decoupling Shared and Unique Features[0m

Box rectangle:  [32m(88.3, 135.5) -> (522.0, 201.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNaichen Shi
                naichens@umich.edu
                Raed Al Kontar
                alkontar@umich.edu
                Department of Industrial & Operations Engineering
                University of Michigan
                Ann Arbor, MI 48109-2117, USA[0m

Box rectangle:  [32m(90.0, 240.6) -> (185.3, 250.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Martin Jaggi[0m

Box rectangle:  [32m(280.3, 276.1) -> (331.7, 288.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.5, 294.3) -> (504.0, 486.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this paper, we tackle a significant challenge in PCA: heterogeneity. When data are
                collected from different sources with heterogeneous trends while still sharing some congruency,
                it is critical to extract shared knowledge while retaining the unique features of each source.
                To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global
                and local principal components to encode both unique and shared features. We show that,
                under mild conditions, both unique and shared features can be identified and recovered by
                a constrained optimization problem, even if the covariance matrices are immensely different.
                Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent
                to solve the problem. The algorithm introduces a new group of operations called generalized
                retractions to handle orthogonality constraints, and only requires global PCs to be shared
                across sources. We prove the linear convergence of the algorithm under suitable assumptions.
                Comprehensive numerical experiments highlight PerPCA’s superior performance in feature
                extraction and prediction from heterogeneous datasets. As a systematic approach to decouple
                shared and unique features from heterogeneous datasets, PerPCA finds applications in several
                tasks, including video segmentation, topic extraction, and feature clustering.
                Keywords:
                Principal component analysis, personalization, heterogeneity.[0m

Box rectangle:  [32m(90.0, 507.3) -> (180.3, 519.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(88.7, 530.3) -> (523.6, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPrincipal component analysis (PCA) (F.R.S., 1901; Hotelling, 1933) unravels data features
                by finding a few principal components (PCs) from high dimensional data that explain the
                largest portion of the variance. Due to its effective feature learning and dimension reduction
                capability, PCA has seen immense success across various domains, including image processing
                (Deledalle et al., 2011; J ́egou and Chum, 2012), time series modeling (Yang and Shahabi,
                2004; Aguilera et al., 1999), bio-information (Reich et al., 2008; Novembre and Stephens,
                2008), condition monitoring (Pozo et al., 2018; Li et al., 2018b), and many more.
                However, since all data are equally weighted in standard PCA, an underlying assumption
                is that these data come from homogeneous distributions. This assumption, however, is often
                challenged in various scenarios, including the Internet of Things (IoT), where data do not
                come from a single source but a large number of distinct edge devices (or clients). The edge
                devices, from smartphones to connected vehicles, usually operate in different environments
                and conditions (Kontar et al., 2017, 2018). The data collected by edge devices are also[0m

Box rectangle:  [32m(89.1, 726.4) -> (243.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Naichen Shi and Raed Al Kontar.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0810.html.[0m



=== Processing ../JMLR 2024/PGMax  Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PGMax  Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-25
                Submitted 8/23; Revised 10/24; Published 12/24[0m

Box rectangle:  [32m(93.0, 101.6) -> (519.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPGMax: Factor Graphs for Discrete Probabilistic Graphical
                Models and Loopy Belief Propagation in JAX[0m

Box rectangle:  [32m(90.0, 153.2) -> (522.0, 274.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuangyao Zhou1
                stannis@google.com
                Antoine Dedieu1
                adedieu@google.com
                Nishanth Kumar2
                njk@mit.edu
                Wolfgang Lehrach1
                wpl@google.com
                Shrinu Kushagra1
                shrinukushagra@google.com
                Dileep George1
                dileepgeorge@google.com
                Miguel L ́azaro-Gredilla1
                lazarogredilla@google.com
                1 Google DeepMind
                2 Massachusetts Institute of Technology[0m

Box rectangle:  [32m(90.0, 726.3) -> (506.5, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Guangyao Zhou, Antoine Dedieu, Nishanth Kumar, Wolfgang Lehrach, Shrinu Kushagra, Dileep George,
                Miguel L ́azaro-Gredilla.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1010.html.[0m



=== Processing ../JMLR 2024/pgmpy  A Python Toolkit for Bayesian Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/pgmpy  A Python Toolkit for Bayesian Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-8
                Submitted 4/23; Revised 11/23; Published 9/24[0m

Box rectangle:  [32m(131.8, 101.6) -> (480.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mpgmpy: A Python Toolkit for Bayesian Networks[0m

Box rectangle:  [32m(90.0, 133.9) -> (518.2, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnkur Ankan
                ankur.ankan@ru.nl[0m

Box rectangle:  [32m(90.0, 153.2) -> (522.0, 178.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohannes Textor
                johannes.textor@ru.nl
                Institute of Computing and Information Sciences, Radboud University, Nijmegen, Netherlands[0m

Box rectangle:  [32m(90.0, 726.3) -> (251.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ankur Ankan and Johannes Textor.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0487.html.[0m



=== Processing ../JMLR 2024/PhAST  Physics-Aware  Scalable  and Task-Specific GNNs for Accelerated Catalyst Design.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PhAST  Physics-Aware  Scalable  and Task-Specific GNNs for Accelerated Catalyst Design.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-26
                Submitted 5/23; Revised 8/23; Published 3/24[0m

Box rectangle:  [32m(91.6, 101.5) -> (520.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPhAST: Physics-Aware, Scalable, and Task-Specific GNNs for
                Accelerated Catalyst Design[0m

Box rectangle:  [32m(89.4, 153.3) -> (522.0, 177.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexandre Duval*
                alexandre.duval@mila.quebec
                Mila, Inria, CentraleSupelec[0m

Box rectangle:  [32m(89.4, 195.0) -> (522.0, 219.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVictor Schmidt*
                schmidtv@mila.quebec
                Mila, Universtité de Montréal[0m

Box rectangle:  [32m(89.5, 236.8) -> (522.0, 260.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSantiago Miret
                santiago.miret@intel.com
                Intel Labs[0m

Box rectangle:  [32m(89.4, 278.4) -> (522.0, 302.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYoshua Bengio
                yoshua.bengio@mila.quebec
                Mila, Université de Montréal, CIFAR Fellow[0m

Box rectangle:  [32m(89.4, 320.1) -> (522.0, 344.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlex Hernández-García
                alex.hernandez-garcia@mila.quebec
                Mila, Université de Montréal[0m

Box rectangle:  [32m(89.4, 363.3) -> (522.0, 389.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Rolnick
                david.rolnick@mila.quebec
                Mila, McGill University[0m

Box rectangle:  [32m(90.0, 726.3) -> (513.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alexandre Duval, Victor Schmidt, Santiago Miret, Yoshua Bengio, Alex Hernandez Garcia, David Rolnick.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0680.html.[0m



=== Processing ../JMLR 2024/PirateNets  Physics-informed Deep Learning with Residual Adaptive Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PirateNets  Physics-informed Deep Learning with Residual Adaptive Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 3/24; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(100.6, 101.5) -> (511.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPirateNets: Physics-informed Deep Learning with Residual
                Adaptive Networks[0m

Box rectangle:  [32m(88.1, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSifan Wang
                sifan.wang@yale.edu
                Institution for Foundation of Data Science
                Yale University
                New Haven, CT 06520[0m

Box rectangle:  [32m(88.8, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBowen Li
                bowen.li@cityu.edu.hk
                Department of Mathematics
                City University of Hong Kong
                Hong Kong SAR[0m

Box rectangle:  [32m(89.4, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuhan Chen
                ychen239@ncsu.edu
                Department of Electrical and Computer Engineering
                North Carolina State University
                Raleigh, NC 27695[0m

Box rectangle:  [32m(88.3, 314.1) -> (522.0, 366.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mParis Perdikaris
                pgp@seas.upenn.edu
                Department of Mechanical Engineering and Applied Mechanics
                University of Pennsylvania
                Philadelphia, PA 19104[0m

Box rectangle:  [32m(90.0, 392.1) -> (240.6, 402.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Animashree Anandkumar[0m

Box rectangle:  [32m(280.3, 425.7) -> (331.7, 437.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 460.5) -> (503.8, 662.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhile physics-informed neural networks (PINNs) have become a popular deep learning
                framework for tackling forward and inverse problems governed by partial differential equa-
                tions (PDEs), their performance is known to degrade when larger and deeper neural network
                architectures are employed. Our study identifies that the root of this counter-intuitive
                behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable
                initialization schemes, which result in poor trainablity for the network derivatives, and
                ultimately lead to an unstable minimization of the PDE residual loss. To address this, we
                introduce Physics-Informed Residual Adaptive Networks (PirateNets), a novel architecture
                that is designed to facilitate stable and efficient training of deep PINN models. PirateNets
                leverage a novel adaptive residual connection, which allows the networks to be initialized as
                shallow networks that progressively deepen during training. We also show that the proposed
                initialization scheme allows us to encode appropriate inductive biases corresponding to
                a given PDE system into the network architecture. We provide comprehensive empiri-
                cal evidence showing that PirateNets are easier to optimize and can gain accuracy from
                considerably increased depth, ultimately achieving state-of-the-art results across various
                benchmarks.
                All code and data accompanying this manuscript will be made publicly
                available at https://github.com/PredictiveIntelligenceLab/jaxpi/tree/pirate.[0m

Box rectangle:  [32m(109.9, 683.2) -> (503.4, 705.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Deep learning; Physics-informed neural networks; Partial differential equations;
                Computational physics; Neural solvers[0m

Box rectangle:  [32m(89.1, 726.4) -> (332.0, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Sifan Wang, Bowen Li, Yuhan Chen and Paris Perdikaris.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-0313.html.[0m



=== Processing ../JMLR 2024/Policy Gradient Methods in the Presence of Symmetries and State Abstractions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Policy Gradient Methods in the Presence of Symmetries and State Abstractions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 10/23; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(90.6, 101.6) -> (521.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPolicy Gradient Methods in the Presence of Symmetries and
                State Abstractions[0m

Box rectangle:  [32m(90.0, 151.7) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPrakash Panangaden∗
                prakash@cs.mcgill.ca
                School of Computer Science, McGill University
                and Mila – Quebec AI Institute
                Montreal, QC, Canada[0m

Box rectangle:  [32m(90.0, 205.3) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSahand Rezaei-Shoshtari∗
                srezaei@cim.mcgill.ca
                School of Computer Science, McGill University
                and Mila – Quebec AI Institute
                Montreal, QC, Canada[0m

Box rectangle:  [32m(90.0, 258.9) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRosie Zhao∗
                rosiezhao@g.harvard.edu
                School of Engineering and Applied Sciences
                Harvard University
                Cambridge, MA, USA[0m

Box rectangle:  [32m(90.0, 312.6) -> (522.0, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid Meger
                dmeger@cim.mcgill.ca
                School of Computer Science, McGill University
                and Mila – Quebec AI Institute
                Montreal, QC, Canada[0m

Box rectangle:  [32m(90.0, 367.8) -> (522.0, 434.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDoina Precup
                dprecup@cs.mcgill.ca
                School of Computer Science, McGill University
                and Mila – Quebec AI Institute
                and DeepMind
                Montreal, QC, Canada[0m

Box rectangle:  [32m(90.0, 459.3) -> (191.6, 469.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Martha White[0m

Box rectangle:  [32m(280.3, 492.9) -> (331.7, 504.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 514.2) -> (502.1, 667.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mReinforcement learning (RL) on high-dimensional and complex problems relies on abstrac-
                tion for improved efficiency and generalization. In this paper, we study abstraction in the
                continuous-control setting, and extend the definition of Markov decision process (MDP)
                homomorphisms to the setting of continuous state and action spaces. We derive a policy
                gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our
                policy gradient results allow for leveraging approximate symmetries of the environment for
                policy optimization. Based on these theorems, we propose a family of actor-critic algorithms
                that are able to learn the policy and the MDP homomorphism map simultaneously, using
                the lax bisimulation metric. Finally, we introduce a series of environments with continuous
                symmetries to further demonstrate the ability of our algorithm for action abstraction in the
                presence of such symmetries. We demonstrate the effectiveness of our method on our envi-
                ronments, as well as on challenging visual control tasks from the DeepMind Control Suite.
                Our method’s ability to utilize MDP homomorphisms for representation learning leads to[0m

Box rectangle:  [32m(93.7, 695.7) -> (282.4, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Equal contributions; alphabetically ordered.[0m

Box rectangle:  [32m(90.0, 726.4) -> (450.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Prakash Panangaden, Sahand Rezaei-Shoshtari, Rosie Zhao, David Meger, Doina Precup.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1415.html.[0m



=== Processing ../JMLR 2024/Polygonal Unadjusted Langevin Algorithms  Creating stable and efficient adaptive algorithms for neural networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Polygonal Unadjusted Langevin Algorithms  Creating stable and efficient adaptive algorithms for neural networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 7/22; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(91.6, 101.6) -> (520.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPolygonal Unadjusted Langevin Algorithms: Creating stable
                and efficient adaptive algorithms for neural networks[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDong-Young Lim∗
                dlim@unist.ac.kr
                Department of Industrial Engineering
                Artificial Intelligence Graduate School
                UNIST
                Ulsan, South Korea[0m

Box rectangle:  [32m(90.0, 219.0) -> (522.0, 326.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSotirios Sabanis
                S.Sabanis@ed.ac.uk
                School of Mathematics
                The University of Edinburgh
                Edinburgh, UK
                The Alan Turing Institute
                London, UK
                National Technical University of Athens
                Athens, Greece[0m

Box rectangle:  [32m(90.0, 726.3) -> (262.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Dong-Young Lim and Sotirios Sabanis.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0796.html.[0m



=== Processing ../JMLR 2024/Post-Regularization Confidence Bands for Ordinary Differential Equations.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Post-Regularization Confidence Bands for Ordinary Differential Equations.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 5/22; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(170.4, 101.6) -> (441.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPost-Regularization Confidence Bands
                for Ordinary Differential Equations[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaowu Dai
                dai@stat.ucla.edu
                Department of Statistics and Data Science and Department of Biostatistics
                University of California, Los Angeles, CA 90095-1554, USA[0m

Box rectangle:  [32m(90.0, 195.1) -> (522.0, 234.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLexin Li
                lexinli@berkeley.edu
                Department of Biostatistics and Epidemiology
                University of California, Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (214.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Xiaowu Dai and Lexin Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0487.html.[0m



=== Processing ../JMLR 2024/Power of knockoff  The impact of ranking algorithm  augmented design  and symmetric statistic.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Power of knockoff  The impact of ranking algorithm  augmented design  and symmetric statistic.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 9/21; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(111.5, 101.6) -> (500.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPower of Knockoff: The Impact of Ranking Algorithm,
                Augmented Design, and Symmetric Statistic[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZheng Tracy Ke
                zke@fas.harvard.edu
                Department of Statistics
                Harvard University
                Cambridge, MA 02138, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun S. Liu
                jliu@stat.harvard.edu
                Department of Statistics
                Harvard University
                Cambridge, MA 02138, USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYucong Ma
                yucongma@g.harvard.edu
                Department of Statistics
                Harvard University
                Cambridge, MA 02138, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (179.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ke, Liu and Ma.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1137.html.[0m



=== Processing ../JMLR 2024/Predictive Inference with Weak Supervision.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Predictive Inference with Weak Supervision.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 3/23; Revised 5/24; Published 5/24[0m

Box rectangle:  [32m(150.7, 101.6) -> (461.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPredictive Inference with Weak Supervision[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaxime Cauchois
                maxime.cauchois@gmail.com
                Department of Statistics
                Stanford University
                Stanford, CA 94305-4020, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSuyash Gupta
                suyash028@gmail.com
                Department of Statistics
                Stanford University
                Stanford, CA 94305-4020, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlnur Ali
                alnurali@gmail.com
                Department of Statistics
                Stanford University
                Stanford, CA 94305-4020, USA[0m

Box rectangle:  [32m(90.0, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohn Duchi
                jduchi@stanford.edu
                Department of Statistics and electrical Engineering
                Stanford University
                Stanford, CA 94305-4020, USA[0m

Box rectangle:  [32m(90.0, 374.2) -> (177.3, 384.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Daniel Hsu[0m

Box rectangle:  [32m(280.3, 407.8) -> (331.7, 419.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 426.2) -> (502.2, 582.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe expense of acquiring labels in large-scale statistical machine learning makes partially
                and weakly-labeled data attractive, though it is not always apparent how to leverage such
                data for model fitting or validation. We present a methodology to bridge the gap between
                partial supervision and validation, developing a conformal prediction framework to provide
                valid predictive confidence sets—sets that cover a true label with a prescribed probability,
                independent of the underlying distribution—using weakly labeled data. To do so, we in-
                troduce a (necessary) new notion of coverage and predictive validity, then develop several
                application scenarios, providing efficient algorithms for classification and several large-scale
                structured prediction problems. We corroborate the hypothesis that the new coverage def-
                inition allows for tighter and more informative (but valid) confidence sets through several
                experiments.
                Keywords:
                Conformal inference, Confidence sets, Coverage validity, Weak supervision,
                Partial labels[0m

Box rectangle:  [32m(90.0, 603.5) -> (176.6, 615.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 626.6) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConsider the typical supervised learning pipeline that we teach students in statistical ma-
                chine learning: we collect data in (X, Y ) pairs, where Y is a label or target to be predicted;
                we pick a model and loss measuring the fidelity of the model to observed data; we choose
                the model minimizing the loss and validate it on held-out data. This picture obscures what
                is becoming one of the major challenges in this endeavor: that of actually collecting high-
                quality labeled data (Sculley et al., 2015; Donoho, 2017; Ratner et al., 2017; Gadre et al.,[0m

Box rectangle:  [32m(90.0, 726.4) -> (346.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Maxime Cauchois, Suyash Gupta, Alnur Ali and John Duchi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0253.html.[0m



=== Processing ../JMLR 2024/Pre-trained Gaussian Processes for Bayesian Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pre-trained Gaussian Processes for Bayesian Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-83
                Submitted 3/23; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(127.4, 101.7) -> (484.6, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPre-trained Gaussian Processes for Bayesian Optimization[0m

Box rectangle:  [32m(90.0, 135.4) -> (519.3, 146.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZi Wang
                WANGZI@GOOGLE.COM[0m

Box rectangle:  [32m(90.0, 153.2) -> (519.3, 164.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge E. Dahl
                GDAHL@GOOGLE.COM[0m

Box rectangle:  [32m(90.0, 170.9) -> (519.3, 181.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKevin Swersky
                KSWERSKY@GOOGLE.COM[0m

Box rectangle:  [32m(90.0, 188.6) -> (519.3, 199.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChansoo Lee
                CHANSOO@GOOGLE.COM[0m

Box rectangle:  [32m(90.0, 206.3) -> (519.3, 217.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZachary Nado
                ZNADO@GOOGLE.COM[0m

Box rectangle:  [32m(89.7, 224.0) -> (519.3, 234.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJustin Gilmer
                GILMER@GOOGLE.COM[0m

Box rectangle:  [32m(89.7, 241.7) -> (519.3, 252.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJasper Snoek
                JSNOEK@GOOGLE.COM[0m

Box rectangle:  [32m(89.6, 261.0) -> (521.7, 285.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZoubin Ghahramani
                ZOUBIN@GOOGLE.COM
                Google DeepMind[0m

Box rectangle:  [32m(90.0, 726.4) -> (519.1, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zi Wang, George E. Dahl, Kevin Swersky, Chansoo Lee, Zachary Nado, Justin Gilmer, Jasper Snoek and Zoubin Ghahramani.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0269.html.[0m



=== Processing ../JMLR 2024/Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-64
                Submitted 1/23; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(127.1, 101.5) -> (485.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mProbabilistic Forecasting with Generative Networks
                via Scoring Rule Minimization[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLorenzo Pacchiardi
                lorenzo.pacchiardi@gmail.com
                Department of Statistics, University of Oxford
                Oxford, OX1 3LB
                United Kingdom[0m

Box rectangle:  [32m(88.3, 205.1) -> (521.9, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRilwan A. Adewoyin∗
                rilwan.adewoyin@warwick.ac.uk
                Department of Statistics, University of Warwick
                Coventry, CV4 7AL
                United Kingdom[0m

Box rectangle:  [32m(88.3, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Dueben
                peter.dueben@ecmwf.int
                Earth System Modelling Section, European Centre for Medium-Range Weather Forecasts
                Reading, RG2 9AX
                United Kingdom[0m

Box rectangle:  [32m(88.3, 314.1) -> (521.9, 366.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRitabrata Dutta
                ritabrata.dutta@warwick.ac.uk
                Department of Statistics, University of Warwick
                Coventry, CV4 7AL
                United Kingdom[0m

Box rectangle:  [32m(90.0, 726.3) -> (408.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lorenzo Pacchiardi, Rilwan A. Adewoyin, Peter Dueben and Ritabrata Dutta.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0038.html.[0m



=== Processing ../JMLR 2024/PROMISE  Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PROMISE  Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-57
                Submitted 9/23; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(105.9, 101.6) -> (506.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPROMISE: Preconditioned Stochastic Optimization
                Methods by Incorporating Scalable Curvature Estimates[0m

Box rectangle:  [32m(90.0, 151.5) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZachary Frangella∗
                zfran@stanford.edu
                Department of Management Science and Engineering
                Stanford University[0m

Box rectangle:  [32m(90.0, 193.2) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPratik Rathore∗
                pratikr@stanford.edu
                Department of Electrical Engineering
                Stanford University[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShipu Zhao
                sz533@cornell.edu
                Department of Systems Engineering
                Cornell University[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMadeleine Udell
                udell@stanford.edu
                Department of Management Science and Engineering
                Stanford University[0m

Box rectangle:  [32m(90.0, 726.3) -> (377.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zachary Frangella, Pratik Rathore, Shipu Zhao, and Madeleine Udell.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1187.html.[0m



=== Processing ../JMLR 2024/PromptBench  A Unified Library for Evaluation of Large Language Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PromptBench  A Unified Library for Evaluation of Large Language Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-22
                Submitted 1/24; Revised 6/24; Published 8/24[0m

Box rectangle:  [32m(103.4, 101.6) -> (508.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPromptBench: A Unified Library for Evaluation of Large
                Language Models[0m

Box rectangle:  [32m(90.0, 153.1) -> (467.3, 165.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKaijie Zhu1,2∗, Qinlin Zhao1,3∗, Hao Chen4, Jindong Wang1†, Xing Xie1[0m

Box rectangle:  [32m(90.0, 166.8) -> (490.0, 192.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1Microsoft Research Asia
                2Institute of Automation, Chinese Academy of Sciences
                3University of Science and Technology of China
                4Carnegie Mellon University[0m

Box rectangle:  [32m(90.0, 726.3) -> (347.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0023.html.[0m



=== Processing ../JMLR 2024/ptwt - The PyTorch Wavelet Toolbox.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/ptwt - The PyTorch Wavelet Toolbox.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-7
                Submitted 5/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(173.4, 101.6) -> (438.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mptwt - The PyTorch Wavelet Toolbox[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMoritz Wolter
                moritz.wolter@uni-bonn.de
                High-Performance Computing and Analytics Lab, University of Bonn, Germany[0m

Box rectangle:  [32m(90.0, 175.6) -> (522.0, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFelix Blanke
                felix.blanke@scai.fraunhofer.de
                Fraunhofer Institute for Algorithms and Scientific Computing, Sankt Augustin, Germany[0m

Box rectangle:  [32m(90.0, 217.2) -> (522.0, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJochen Garcke
                garcke@ins.uni-bonn.de
                Institute for Numerical Simulation, University of Bonn
                and Fraunhofer Institute for Algorithms and Scientific Computing, Sankt Augustin, Germany[0m

Box rectangle:  [32m(90.0, 272.4) -> (522.0, 298.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCharles Tapley Hoyt
                cthoyt@gmail.com
                Northeastern University, Boston, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (351.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Moritz Wolter, Felix Blanke, Jochen Garcke and Charles Hoyt.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0636.html.[0m



=== Processing ../JMLR 2024/Pure Differential Privacy for Functional Summaries with a Laplace-like Process.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pure Differential Privacy for Functional Summaries with a Laplace-like Process.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 12/22; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(98.8, 101.6) -> (513.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPure Differential Privacy for Functional Summaries with a
                Laplace-like Process[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHaotian Lin
                hzl435@psu.edu
                Department of Statistics,
                The Pennsylvania State University,
                University Park, PA 16802, USA[0m

Box rectangle:  [32m(90.0, 213.3) -> (522.0, 266.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatthew Reimherr
                mreimherr@psu.edu
                Department of Statistics,
                The Pennsylvania State University,
                University Park, PA 16802, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (254.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Haotian Lin and Matthew Reimherr.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1384.html.[0m



=== Processing ../JMLR 2024/Pursuit of the Cluster Structure of Network Lasso  Recovery Condition and Non-convex Extension.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pursuit of the Cluster Structure of Network Lasso  Recovery Condition and Non-convex Extension.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 10/21; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(125.6, 101.6) -> (486.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPursuit of the Cluster Structure of Network Lasso:
                Recovery Condition and Non-convex Extension[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShotaro Yagishita
                a15.fjng@g.chuo-u.ac.jp
                Department of Industrial and Systems Engineering
                Chuo University
                1-13-27 Kasuga, Bunkyo-ku, Tokyo, 112-8551, Japan[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun-ya Gotoh
                jgoto@kc.chuo-u.ac.jp
                Department of Data Science for Business Innovation
                Chuo University
                1-13-27 Kasuga, Bunkyo-ku, Tokyo, 112-8551, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (256.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shotaro Yagishita and Jun-ya Gotoh.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1190.html.[0m



=== Processing ../JMLR 2024/PyDMD  A Python Package for Robust Dynamic Mode Decomposition.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PyDMD  A Python Package for Robust Dynamic Mode Decomposition.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-9
                Submitted 5/24; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(108.5, 101.6) -> (503.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPyDMD: A Python Package for Robust Dynamic Mode
                Decomposition[0m

Box rectangle:  [32m(90.0, 153.2) -> (522.0, 341.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSara M. Ichinaga1
                sarami7@uw.edu
                Francesco Andreuzzi2,4
                andreuzzi.francesco@gmail.com
                Nicola Demo2
                nicola.demo@sissa.it
                Marco Tezzele3
                marco.tezzele@emory.edu
                Karl Lapo5
                karl-eric.lapo@uibk.ac.at
                Gianluigi Rozza2
                gianluigi.rozza@sissa.it
                Steven L. Brunton6
                sbrunton@uw.edu
                J. Nathan Kutz1
                kutz@uw.edu
                1 Department of Applied Mathematics, University of Washington, Seattle, WA 98195, USA
                2 Mathematics Area, mathLab, SISSA, via Bonomea 265, I-34136 Trieste, Italy
                3 Department of Mathematics, Emory University, Atlanta, GA 30322, USA
                4 CERN, Geneva, Switzerland
                5 Department of Atmospheric and Cryospheric Sciences, University of Innsbruck, Innsbruck, Austria
                6 Department of Mechanical Engineering, University of Washington, Seattle, WA 98195, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (520.8, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sara M. Ichinaga, Francesco Andreuzzi, Nicola Demo, Marco Tezzele, Karl Lapo, Gianluigi Rozza, Steven L.
                Brunton, J. Nathan Kutz.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0739.html.[0m



=== Processing ../JMLR 2024/Pygmtools  A Python Graph Matching Toolkit.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Pygmtools  A Python Graph Matching Toolkit.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-7
                Submitted 5/23; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(139.3, 101.6) -> (472.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPygmtools: A Python Graph Matching Toolkit[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 323.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRunzhong Wang
                runzhong.wang@sjtu.edu.cn
                Ziao Guo
                ziao.guo@sjtu.edu.cn
                Wenzheng Pan
                pwz1121@sjtu.edu.cn
                Jiale Ma
                heatingma@sjtu.edu.cn
                Yikai Zhang
                1065543666@sjtu.edu.cn
                Nan Yang
                advancing-sheep@sjtu.edu.cn
                Qi Liu
                purewhite@sjtu.edu.cn
                Longxuan Wei
                weilongxuan189034@sjtu.edu.cn
                Hanxue Zhang
                zhx jiaxue@sjtu.edu.cn
                Chang Liu
                only-changer@sjtu.edu.cn
                Zetian Jiang
                maple jzt@sjtu.edu.cn
                Xiaokang Yang
                xkyang@sjtu.edu.cn
                Junchi Yan∗
                yanjunchi@sjtu.edu.cn
                CSE Department and MoE Key Lab of AI, Shanghai Jiao Tong University, Shanghai, 200240, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (498.8, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Runzhong Wang, Ziao Guo, Wenzheng Pan, Jiale Ma, Yikai Zhang, Nan Yang, Qi Liu, Longxuan Wei,
                Hanxue Zhang, Chang Liu, Zetian Jiang, Xiaokang Yang and Junchi Yan.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0572.html.[0m



=== Processing ../JMLR 2024/PyGOD  A Python Library for Graph Outlier Detection.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PyGOD  A Python Library for Graph Outlier Detection.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-9
                Submitted 7/23; Revised 2/24; Published 5/24[0m

Box rectangle:  [32m(106.6, 101.6) -> (505.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPyGOD: A Python Library for Graph Outlier Detection[0m

Box rectangle:  [32m(90.0, 135.2) -> (522.0, 283.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKay Liu1∗
                zliu234@uic.edu
                Yingtong Dou1,2∗
                yidou@visa.com
                Xueying Ding3
                xding2@andrew.cmu.edu
                Xiyang Hu4
                xiyanghu@asu.edu
                Ruitong Zhang5
                zhangruitong.zrt@alibaba-inc.com
                Hao Peng6,7
                penghcs@gmail.com
                Lichao Sun8
                lis221@lehigh.edu
                Philip S. Yu1
                psyu@uic.edu
                1University of Illinois Chicago, 2Visa Research, 3Carnegie Mellon University, 4Arizona State Uni-
                versity, 5Alibaba Group, 6Kunming University of Science and Technology, 7Shantou University,
                8Lehigh University[0m

Box rectangle:  [32m(90.0, 726.3) -> (508.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kay Liu, Yingtong Dou, Xueying Ding, Xiyang Hu, Ruitong Zhang, Hao Peng, Lichao Sun, Philip S. Yu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0963.html.[0m



=== Processing ../JMLR 2024/PyPop7  A Pure-Python Library for Population-Based Black-Box Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/PyPop7  A Pure-Python Library for Population-Based Black-Box Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-28
                Submitted 3/23; Revised 7/24; Published 10/24[0m

Box rectangle:  [32m(111.9, 101.6) -> (500.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPyPop7: A Pure-Python Library for Population-Based
                Black-Box Optimization[0m

Box rectangle:  [32m(90.0, 151.5) -> (518.2, 164.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQiqi Duan1,2,∗
                11749325@mail.sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 169.2) -> (518.2, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuochen Zhou2,∗
                12132378@mail.sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 187.0) -> (518.2, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChang Shao3,∗
                chang.shao@student.uts.edu.au[0m

Box rectangle:  [32m(90.0, 204.8) -> (518.2, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhuowei Wang4
                zhuowei.wang@csiro.au[0m

Box rectangle:  [32m(90.0, 222.5) -> (518.2, 235.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMingyang Feng5
                11856010@mail.sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 240.2) -> (518.2, 252.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuwei Huang2
                12332473@mail.sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 257.9) -> (518.2, 270.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYajing Tan2
                12332416@mail.sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 275.6) -> (518.2, 288.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYijun Yang6
                altmanyang@tencent.com[0m

Box rectangle:  [32m(90.0, 293.4) -> (518.2, 305.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQi Zhao2
                zhaoq@sustech.edu.cn[0m

Box rectangle:  [32m(90.0, 312.6) -> (522.0, 406.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuhui Shi2,†
                shiyh@sustech.edu.cn
                1Harbin Institute of Technology, Harbin, China
                2Southern University of Science and Technology, Shenzhen, China
                3University of Technology Sydney, Sydney, Australia
                4Space and Astronomy, CSIRO, Marshfield, Australia
                5University of Birmingham, Birmingham, UK
                6Tencent Inc., Shenzhen, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (514.2, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Qiqi Duan, Guochen Zhou, Chang Shao, Zhuowei Wang, Mingyang Feng, Yuwei Huang, Yajing Tan, Yijun
                Yang, Qi Zhao, and Yuhui Shi.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0386.html.[0m



=== Processing ../JMLR 2024/QDax  A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/QDax  A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-16
                Submitted 8/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQDax: A Library for Quality-Diversity and Population-based
                Algorithms with Hardware Acceleration[0m

Box rectangle:  [32m(89.5, 151.6) -> (522.0, 283.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFelix Chalumeau* 1
                f.chalumeau@instadeep.com
                Bryan Lim* 2
                bryan.lim16@imperial.ac.uk
                Rapha ̈el Boige
                r.boige@instadeep.com
                Maxime Allard
                m.allard20@imperial.ac.uk
                Luca Grillotti
                luca.grillotti16@imperial.ac.uk
                Manon Flageat
                manon.flageat18@imperial.ac.uk
                Valentin Mac ́e
                v.mace@instadeep.com
                Guillaume Richard
                g.richard@instadeep.com
                Arthur Flajolet
                a.flajolet@instadeep.com
                Thomas Pierrot** 1
                t.pierrot@instadeep.com
                Antoine Cully** 2
                a.cully@imperial.ac.uk[0m

Box rectangle:  [32m(88.1, 303.6) -> (382.1, 328.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1InstaDeep
                2Department of Computing, Imperial College London
                * Equal Contribution
                ** Equal Supervision[0m

Box rectangle:  [32m(90.0, 726.3) -> (507.3, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Felix Chalumeau*, Bryan Lim*, Rapha ̈el Boige, Maxime Allard, Luca Grillotti, Manon Flageat, Valentin
                Mac ́e, Guillaume Richard, Arthur Flajolet, Thomas Pierrot**, Antoine Cully**.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1027.html.[0m



=== Processing ../JMLR 2024/Random Forest Weighted Local Fréchet Regression with Random Objects.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Random Forest Weighted Local Fréchet Regression with Random Objects.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-69
                Submitted 6/23; Revised 11/23; Published 3/24[0m

Box rectangle:  [32m(105.1, 102.2) -> (506.9, 134.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRandom Forest Weighted Local Fréchet Regression with Random
                Objects[0m

Box rectangle:  [32m(90.0, 153.9) -> (521.7, 200.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRui Qiu
                RQIU_STAT@OUTLOOK.COM
                School of Statistics, KLATASDS-MOE
                East China Normal University
                Shanghai 200062, China[0m

Box rectangle:  [32m(90.0, 205.6) -> (521.8, 254.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhou Yu∗
                ZYU@STAT.ECNU.EDU.CN
                School of Statistics, KLATASDS-MOE
                East China Normal University
                Shanghai 200062, China[0m

Box rectangle:  [32m(90.0, 262.7) -> (521.8, 313.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuoqing Zhu
                RQZHU@ILLINOIS.EDU
                Department of Statistics
                University of Illinois at Urbana-Champaign
                Champaign, IL 61820, USA[0m

Box rectangle:  [32m(90.0, 726.4) -> (235.3, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rui Qiu, Zhou Yu, and Ruoqing Zhu.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0811.html.[0m



=== Processing ../JMLR 2024/Random Fully Connected Neural Networks as Perturbatively Solvable Hierarchies.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Random Fully Connected Neural Networks as Perturbatively Solvable Hierarchies.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-58
                Submitted 5/23; Revised 6/24; Published 8/24[0m

Box rectangle:  [32m(90.0, 101.6) -> (522.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRandom Fully Connected Neural Networks as Perturbatively
                Solvable Hierarchies[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoris Hanin
                bhanin@princeton.edu
                Department of Operations Research
                and Financial Engineering
                Princeton University
                Princeton, NJ 08544, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (164.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Boris Hanin.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0643.html.[0m



=== Processing ../JMLR 2024/Random measure priors in Bayesian recovery from sketches.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Random measure priors in Bayesian recovery from sketches.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 8/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(95.4, 101.6) -> (516.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRandom measure priors in Bayesian recovery from sketches[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMario Beraha
                mario.beraha@polimi.it
                Polytechnic University of Milan, Milan, Italy[0m

Box rectangle:  [32m(90.0, 163.6) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStefano Favaro
                stefano.favaro@unito.it
                University of Torino and Collegio Carlo Alberto, Torino, Italy[0m

Box rectangle:  [32m(90.0, 194.9) -> (522.0, 220.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMatteo Sesia
                sesia@marshall.usc.edu
                University of Southern California, Los Angeles, California, United States[0m

Box rectangle:  [32m(90.0, 726.4) -> (301.0, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mario Beraha, Stefano Favaro, and Matteo Sesia.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1058.html.[0m



=== Processing ../JMLR 2024/Random Smoothing Regularization in Kernel Gradient Descent Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Random Smoothing Regularization in Kernel Gradient Descent Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-88
                Submitted 5/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(111.9, 108.8) -> (500.3, 141.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRandom Smoothing Regularization in Kernel Gradient
                Descent Learning[0m

Box rectangle:  [32m(90.0, 165.9) -> (522.0, 202.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLiang Ding ∗
                liang ding@fudan.edu.cn
                Fudan University
                Shanghai, China[0m

Box rectangle:  [32m(90.0, 215.1) -> (522.0, 251.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTianyang Hu
                hutianyang.up@outlook.com
                Huawei Noah’s Ark Lab
                Shenzhen, China[0m

Box rectangle:  [32m(90.0, 263.9) -> (522.0, 300.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiahang Jiang
                jjiangbc@connect.ust.hk
                The Hong Kong University of Science and Technology
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 312.7) -> (522.0, 348.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDonghao Li
                dlibf@connect.ust.hk
                The Hong Kong University of Science and Technology
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 361.6) -> (522.0, 421.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWenjia Wang
                wenjiawang@hkust-gz.edu.cn
                The Hong Kong University of Science and Technology (Guangzhou)
                Guangzhou, China
                The Hong Kong University of Science and Technology
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 435.9) -> (522.0, 475.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuan Yao
                yuany@ust.hk
                The Hong Kong University of Science and Technology
                Hong Kong SAR, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (416.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Liang Ding, Tianyang Hu, Jiahang Jiang, Donghao Li, Wenjia Wang, Yuan Yao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0580.html.[0m



=== Processing ../JMLR 2024/Random Subgraph Detection Using Queries.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Random Subgraph Detection Using Queries.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-25
                Submitted 4/22; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(151.0, 101.6) -> (461.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRandom Subgraph Detection Using Queries[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWasim Huleihel
                wasimh@tauex.tau.ac.il
                Department of Electrical Engineering-Systems
                Tel Aviv university
                Tel Aviv 6997801, Israel[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArya Mazumdar
                arya@ucsd.edu
                Halıcıo ̆glu Data Science Institute
                University of California San Diego
                La Jolla, CA 92093, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 281.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSoumyabrata Pal
                soumyabratapal13@gmail.com
                Adobe Research, India
                Bangalore, INDIA[0m

Box rectangle:  [32m(90.0, 726.3) -> (313.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Wasim Huleihel, Arya Mazumdar, Soumyabrata Pal.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0395.html.[0m



=== Processing ../JMLR 2024/Rates of convergence for density estimation with generative adversarial networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Rates of convergence for density estimation with generative adversarial networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 24 (2023) 1-47
                Submitted 1/23; Revised 11/23; Published 12/23[0m

Box rectangle:  [32m(151.9, 101.6) -> (460.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRates of convergence for density estimation
                with generative adversarial networks[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNikita Puchkin
                npuchkin@hse.ru
                HSE University, Russian Federation
                Institute for Information transmission Problems, Russian Federation[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSergey Samsonov
                svsamsonov@hse.ru
                HSE University, Russian Federation
                Institute for Information transmission Problems, Russian Federation[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDenis Belomestny
                denis.belomestny@uni-due.de
                Duisburg-Essen University, Germany
                HSE University, Russian Federation[0m

Box rectangle:  [32m(90.0, 276.7) -> (522.0, 312.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEric Moulines
                eric.moulines@polytechnique.edu
                 ́Ecole Polytechnique, France
                Mohamed Bin Zayed University of AI, United Arab Emirates[0m

Box rectangle:  [32m(90.0, 320.0) -> (522.0, 359.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexey Naumov
                anaumov@hse.ru
                HSE University, Russian Federation
                Steklov Mathematical Institute of Russian Academy of Sciences, Russian Federation[0m

Box rectangle:  [32m(90.0, 384.4) -> (179.7, 394.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Aarti Singh[0m

Box rectangle:  [32m(280.3, 419.9) -> (331.7, 431.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 437.8) -> (502.1, 569.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this work we undertake a thorough study of the non-asymptotic properties of the vanilla
                generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-
                Shannon (JS) divergence between the underlying density p∗and the GAN estimate with a
                significantly better statistical error term compared to the previously known results. The
                advantage of our bound becomes clear in application to nonparametric density estimation.
                We show that the JS-divergence between the GAN estimate and p∗decays as fast as
                (log n/n)2β/(2β+d), where n is the sample size and β determines the smoothness of p∗. This
                rate of convergence coincides (up to logarithmic factors) with minimax optimal for the
                considered class of densities.
                Keywords:
                generative model, oracle inequality, Jensen-Shannon risk, minimax rates,
                nonparametric density estimation.[0m

Box rectangle:  [32m(90.0, 590.4) -> (180.3, 602.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 609.1) -> (522.0, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLet X1, . . . , Xn be i.i.d. random elements with values in X ⊆Rd drawn from a distribution
                P ∗. We assume that P ∗admits a density p∗with respect to a dominating measure μ. The
                measure μ is not necessarily absolutely continuous with respect to the Lebesgue measure,
                it can be the counting measure or the Hausdorffmeasure on a low-dimensional manifold
                as well.
                Our goal is to estimate p∗based on a finite sample.
                The problem of density
                estimation was extensively studied in the literature and encounters numerous approaches
                such as kernel (see, e.g., (Tsybakov, 2008, Section 1.2) and (McDonald, 2017)) and k-[0m

Box rectangle:  [32m(90.0, 726.4) -> (454.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2023 Nikita Puchkin, Sergey Samsonov, Denis Belomestny, Eric Moulines, and Alexey Naumov.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v24/23-0062.html.[0m



=== Processing ../JMLR 2024/Recursive Estimation of Conditional Kernel Mean Embeddings.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Recursive Estimation of Conditional Kernel Mean Embeddings.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-35
                Submitted 2/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(170.3, 101.7) -> (441.8, 133.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRecursive Estimation of
                Conditional Kernel Mean Embeddings[0m

Box rectangle:  [32m(90.0, 153.4) -> (522.0, 166.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmbrus Tam ́as1,2
                ambrus.tamas@sztaki.hu[0m

Box rectangle:  [32m(90.0, 169.7) -> (522.0, 182.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBal ́azs Csan ́ad Cs ́aji1,2
                balazs.csaji@sztaki.hu[0m

Box rectangle:  [32m(90.0, 190.1) -> (333.0, 228.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1Institute for Computer Science and Control (SZTAKI)
                Hungarian Research Network (HUN-REN),
                Kende utca 13-17, Budapest, H-1111, Hungary[0m

Box rectangle:  [32m(90.0, 236.4) -> (353.0, 274.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m2Department of Probability Theory and Statistics
                Institute of Mathematics, E ̈otv ̈os Lor ́and University (ELTE)
                P ́azm ́any P ́eter S ́et ́any 1 / C, Budapest, H-1117, Hungary[0m

Box rectangle:  [32m(90.0, 726.3) -> (264.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ambrus Tam ́as & Bal ́azs Csan ́ad Cs ́aji.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0168.html.[0m



=== Processing ../JMLR 2024/Regimes of No Gain in Multi-class Active Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Regimes of No Gain in Multi-class Active Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-31
                Submitted 2/23; Revised 10/23; Published 3/24[0m

Box rectangle:  [32m(124.7, 101.6) -> (487.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRegimes of No Gain in Multi-class Active Learning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGan Yuan
                gan.yuan@columbia.edu
                Department of Statistics
                Columbia University in the City of New York
                New York, NY 10027, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunfan Zhao
                yz3685@columbia.edu
                Department of Industrial Engineering and Operations Research
                Columbia University in the City of New York
                New York, NY 10027, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSamory Kpotufe
                skk2175@columbia.edu
                Department of Statistics
                Columbia University in the City of New York
                New York, NY 10027, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (291.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Gan Yuan, Yunfan Zhao and Samory Kpotufe.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0234.html.[0m



=== Processing ../JMLR 2024/Regret Analysis of Bilateral Trade with a Smoothed Adversary.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Regret Analysis of Bilateral Trade with a Smoothed Adversary.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 12/23; Published 7/24[0m

Box rectangle:  [32m(114.5, 101.7) -> (498.0, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRegret Analysis of Bilateral Trade with a Smoothed Adversary[0m

Box rectangle:  [32m(89.6, 135.4) -> (521.8, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicolò Cesa-Bianchi
                NICOLO.CESA-BIANCHI@UNIMI.IT
                Università degli Studi di Milano and Politecnico di Milano,
                Milan, Italy[0m

Box rectangle:  [32m(89.6, 177.1) -> (521.8, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTommaso Cesari
                TCESARI@UOTTAWA.CA
                University of Ottawa,
                Ottawa, Canada[0m

Box rectangle:  [32m(89.6, 218.7) -> (521.8, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRoberto Colomboni
                ROBERTO.COLOMBONI@UNIMI.IT
                Università degli Studi di Milano and Politecnico di Milano,
                Milan, Italy[0m

Box rectangle:  [32m(89.7, 260.3) -> (521.8, 294.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFederico Fusco
                FUSCOF@DIAG.UNIROMA1.IT
                Sapienza Università di Roma,
                Rome, Italy[0m

Box rectangle:  [32m(89.7, 303.5) -> (521.8, 341.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStefano Leonardi
                LEONARDI@DIAG.IT
                Sapienza Università di Roma,
                Rome, Italy[0m

Box rectangle:  [32m(90.0, 726.4) -> (422.1, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nicolò Cesa-Bianchi, Tommaso Cesari, Roberto Colomboni, Federico Fusco, Stefano Leonardi.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1627.html.[0m



=== Processing ../JMLR 2024/Representation Learning via Manifold Flattening and Reconstruction.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Representation Learning via Manifold Flattening and Reconstruction.pdf') ---[0m

Box rectangle:  [32m(72.0, 22.9) -> (539.9, 30.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-47
                Submitted 5/23; Revised 3/24; Published 4/24[0m

Box rectangle:  [32m(117.0, 82.7) -> (495.1, 115.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRepresentation Learning via Manifold Flattening and
                Reconstruction[0m

Box rectangle:  [32m(72.0, 133.0) -> (540.0, 181.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael Psenka
                psenka@eecs.berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley
                Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(72.0, 186.6) -> (540.0, 234.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDruv Pai
                druvpai@berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley
                Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(72.0, 240.2) -> (540.0, 288.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVishal Raman
                vraman@berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley
                Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(72.0, 293.7) -> (540.0, 341.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShankar Sastry
                sastry@coe.berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley
                Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(72.0, 348.9) -> (540.0, 401.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYi Ma
                yima@eecs.berkeley.edu
                Department of Electrical Engineering and Computer Science
                University of California, Berkeley
                Berkeley, CA 94720-1776, USA[0m

Box rectangle:  [32m(72.0, 743.4) -> (345.9, 751.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Michael Psenka, Druv Pai, Vishal Raman, Shankar Sastry, Yi Ma.[0m

Box rectangle:  [32m(72.0, 758.1) -> (509.4, 775.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0615.html.[0m



=== Processing ../JMLR 2024/Resource-Efficient Neural Networks for Embedded Systems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Resource-Efficient Neural Networks for Embedded Systems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 8/18; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(96.0, 101.6) -> (516.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mResource-Efficient Neural Networks for Embedded Systems[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWolfgang Roth∗
                roth@tugraz.at
                Graz University of Technology, Austria
                Laboratory of Signal Processing and Speech Communication[0m

Box rectangle:  [32m(90.0, 175.2) -> (521.9, 211.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mG ̈unther Schindler∗
                guenther.schindler@ziti.uni-heidelberg.de
                Heidelberg University, Germany
                Institute of Computer Engineering[0m

Box rectangle:  [32m(90.0, 216.9) -> (521.9, 253.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBernhard Klein∗
                bernhard.klein@ziti.uni-heidelberg.de
                Heidelberg University, Germany
                Institute of Computer Engineering[0m

Box rectangle:  [32m(90.0, 258.8) -> (522.0, 294.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobert Peharz
                robert.peharz@tugraz.at
                Graz University of Technology, Austria
                Institute of Theoretical Computer Science[0m

Box rectangle:  [32m(90.0, 300.4) -> (522.0, 336.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSebastian Tschiatschek
                sebastian.tschiatschek@univie.ac.at
                University of Vienna, Austria
                Faculty of Computer Science[0m

Box rectangle:  [32m(90.0, 342.1) -> (522.0, 378.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHolger Fr ̈oning
                holger.froening@ziti.uni-heidelberg.de
                Heidelberg University, Germany
                Institute of Computer Engineering[0m

Box rectangle:  [32m(90.0, 383.7) -> (522.0, 419.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFranz Pernkopf
                pernkopf@tugraz.at
                Graz University of Technology, Austria
                Laboratory of Signal Processing and Speech Communication[0m

Box rectangle:  [32m(90.0, 426.9) -> (522.0, 452.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZoubin Ghahramani
                zoubin@eng.cam.ac.uk
                University of Cambridge, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (521.9, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Wolfgang Roth, G ̈unther Schindler, Bernhard Klein, Robert Peharz, Sebastian Tschiatschek, Holger Fr ̈oning,
                Franz Pernkopf and Zoubin Ghahramani.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/18-566.html.[0m



=== Processing ../JMLR 2024/Rethinking Discount Regularization  New Interpretations  Unintended Consequences  and Solutions for Regularization in Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Rethinking Discount Regularization  New Interpretations  Unintended Consequences  and Solutions for Regularization in Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-48
                Submitted 1/24; Revised 6/24; Published 8/24[0m

Box rectangle:  [32m(110.1, 101.6) -> (502.1, 151.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRethinking Discount Regularization:
                New Interpretations, Unintended Consequences, and
                Solutions for Regularization in Reinforcement Learning[0m

Box rectangle:  [32m(90.0, 169.8) -> (521.9, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSarah Rathnam
                sarah rathnam@g.harvard.edu
                John A. Paulson School of Engineering and Applied Sciences
                Harvard University
                Cambridge, MA 02138 USA[0m

Box rectangle:  [32m(90.0, 223.4) -> (522.0, 259.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSonali Parbhoo
                s.parbhoo@imperial.ac.uk
                Imperial College London
                London SW7 2BX, UK[0m

Box rectangle:  [32m(90.0, 266.6) -> (522.0, 360.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSiddharth Swaroop
                siddharth@seas.harvard.edu
                Weiwei Pan
                weiweipan@g.harvard.edu
                Susan A. Murphy
                samurphy@g.harvard.edu
                Finale Doshi-Velez
                finale@seas.harvard.edu
                John A. Paulson School of Engineering and Applied Sciences
                Harvard University
                Cambridge, MA 02138 USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (521.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Sarah Rathnam, Sonali Parbhoo, Siddharth Swaroop, Weiwei Pan, Susan A. Murphy, and Finale Doshi-Velez.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0087.html.[0m



=== Processing ../JMLR 2024/Revisiting RIP Guarantees for Sketching Operators on Mixture Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Revisiting RIP Guarantees for Sketching Operators on Mixture Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (521.9, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-68
                Submitted 1/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(150.7, 101.6) -> (461.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRevisiting RIP Guarantees
                for Sketching Operators on Mixture Models[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 192.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAyoub Belhadji
                ayoub.belhadji@gmail.com
                R ́emi Gribonval
                remi.gribonval@inria.fr
                Univ Lyon, ENS de Lyon, Inria, CNRS, UCBL, LIP UMR 5668, Lyon, France[0m

Box rectangle:  [32m(90.0, 726.4) -> (256.3, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ayoub Belhadji and R ́emi Gribonval.[0m

Box rectangle:  [32m(90.0, 741.2) -> (512.2, 758.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0044.html.[0m



=== Processing ../JMLR 2024/Risk Measures and Upper Probabilities  Coherence and Stratification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Risk Measures and Upper Probabilities  Coherence and Stratification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-100
                Submitted 6/22; Revised 1/24; Published 5/24[0m

Box rectangle:  [32m(165.4, 101.6) -> (449.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRisk Measures and Upper Probabilities:
                Coherence and Stratification[0m

Box rectangle:  [32m(88.3, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristian Fr ̈ohlich
                christian.froehlich@uni-tuebingen.de
                Robert C. Williamson
                bob.williamson@uni-tuebingen.de
                University of T ̈ubingen
                and T ̈ubingen AI Center
                T ̈ubingen, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (288.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Christian Fr ̈ohlich and Robert C. Williamson.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0641.html.[0m



=== Processing ../JMLR 2024/RLtools  A Fast  Portable Deep Reinforcement Learning Library for Continuous Control.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/RLtools  A Fast  Portable Deep Reinforcement Learning Library for Continuous Control.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-19
                Submitted 2/24; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(110.0, 101.5) -> (502.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRLtools: A Fast, Portable Deep Reinforcement Learning
                Library for Continuous Control[0m

Box rectangle:  [32m(149.8, 142.3) -> (462.5, 191.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonas Eschmann1,2
                Dario Albani2
                Giuseppe Loianno1
                1New York University
                2Technology Innovation Institute
                {jonas.eschmann,loiannog}@nyu.edu
                dario.albani@tii.ae[0m

Box rectangle:  [32m(90.0, 726.3) -> (323.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jonas Eschmann, Dario Albani, and Giuseppe Loianno.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0248.html.[0m



=== Processing ../JMLR 2024/Robust Black-Box Optimization for Stochastic Search and Episodic Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Robust Black-Box Optimization for Stochastic Search and Episodic Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-44
                Submitted 5/22; Revised 12/23; Published 5/24[0m

Box rectangle:  [32m(100.5, 101.6) -> (511.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobust Black-Box Optimization for Stochastic Search and
                Episodic Reinforcement Learning[0m

Box rectangle:  [32m(89.4, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaximilian H ̈uttenrauch
                maximilian.huettenrauch@kit.edu
                Gerhard Neumann
                gerhard.neumann@kit.edu
                Department of Computer Science
                Karlsruhe Institute of Technology
                Karlsruhe[0m

Box rectangle:  [32m(90.0, 726.3) -> (300.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Maximilian H ̈uttenrauch and Gerhard Neumann.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0564.html.[0m



=== Processing ../JMLR 2024/Robust Principal Component Analysis using Density Power Divergence.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Robust Principal Component Analysis using Density Power Divergence.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 8/23; Published 10/24[0m

Box rectangle:  [32m(95.0, 101.6) -> (517.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobust Principal Component Analysis using Density Power
                Divergence[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 233.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSubhrajyoty Roy
                roysubhra98@gmail.com
                Ayanendranath Basu
                ayanbasu@isical.ac.in
                Abhik Ghosh
                abhik.ghosh@isical.ac.in
                Interdisciplinary Statistical Research Unit
                Indian Statistical Institute
                Kolkata - 700108, West Bengal, India[0m

Box rectangle:  [32m(90.0, 726.3) -> (334.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Subhrajyoty Roy, Ayanendranath Basu and Abhik Ghosh.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1096.html.[0m



=== Processing ../JMLR 2024/Robust Spectral Clustering with Rank Statistics.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Robust Spectral Clustering with Rank Statistics.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-81
                Submitted 4/23; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(134.9, 101.6) -> (477.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobust Spectral Clustering with Rank Statistics[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoshua Cape
                jrcape@wisc.edu
                Department of Statistics
                University of Wisconsin–Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXianshi Yu
                xyu384@wisc.edu
                Department of Computer Sciences
                University of Wisconsin–Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonquil Z. Liao
                zliao42@wisc.edu
                Department of Statistics
                University of Wisconsin–Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (292.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Joshua Cape, Xianshi Yu, and Jonquil Z. Liao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0552.html.[0m



=== Processing ../JMLR 2024/Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 1/24; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(111.9, 101.5) -> (500.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSample Complexity of Neural Policy Mirror Descent for
                Policy Optimization on Low-Dimensional Manifolds[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhenghao Xu
                zhenghaoxu@gatech.edu
                H. Milton Stewart School of Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiang Ji
                xiangj@princeton.edu
                Department of Electrical and Computer Engineering
                Princeton University
                Princeton, NJ 08544, USA[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMinshuo Chen
                minshuochen@princeton.edu
                Department of Electrical and Computer Engineering
                Princeton University
                Princeton, NJ 08544, USA[0m

Box rectangle:  [32m(90.0, 312.5) -> (522.0, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMengdi Wang
                mengdiw@princeton.edu
                Department of Electrical and Computer Engineering
                Princeton University
                Princeton, NJ 08544, USA[0m

Box rectangle:  [32m(90.0, 367.7) -> (522.0, 420.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTuo Zhao
                tourzhao@gatech.edu
                H. Milton Stewart School of Industrial and Systems Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (379.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zhenghao Xu, Xiang Ji, Minshuo Chen, Mengdi Wang, and Tuo Zhao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0066.html.[0m



=== Processing ../JMLR 2024/Sample Complexity of Variance-Reduced Distributionally Robust Q-Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sample Complexity of Variance-Reduced Distributionally Robust Q-Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-77
                Submitted 4/23; Revised 8/24; Published 10/24[0m

Box rectangle:  [32m(113.4, 101.6) -> (498.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSample Complexity of
                Variance-Reduced Distributionally Robust Q-Learning[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShengbo Wang
                shengbo.wang@stanford.edu
                Department of Management Science and Engineering
                Stanford University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNian Si
                niansi@ust.hk
                Department of Industrial Engineering and Decision Analytics
                The Hong Kong University of Science and Technology[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 271.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJose Blanchet
                jose.blanchet@stanford.edu
                Department of Management Science and Engineering
                Stanford University[0m

Box rectangle:  [32m(90.0, 278.3) -> (522.0, 317.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhengyuan Zhou
                zz26@stern.nyu.edu
                Stern School of Business
                New York University[0m

Box rectangle:  [32m(90.0, 726.3) -> (332.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou.[0m

Box rectangle:  [32m(90.0, 740.7) -> (506.3, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/23-0526.html.[0m



=== Processing ../JMLR 2024/Sample-efficient Adversarial Imitation Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sample-efficient Adversarial Imitation Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 3/23; Revised 12/23; Published 1/24[0m

Box rectangle:  [32m(141.7, 101.5) -> (470.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSample-efficient Adversarial Imitation Learning[0m

Box rectangle:  [32m(89.5, 135.5) -> (522.0, 269.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDahuin Jung
                annajung0625@snu.ac.kr
                Electrical and Computer Engineering
                Seoul National University, Seoul 08826, Republic of Korea
                Hyungyu Lee
                rucy74@snu.ac.kr
                Electrical and Computer Engineering
                Seoul National University, Seoul 08826, Republic of Korea
                Sungroh Yoon∗
                sryoon@snu.ac.kr
                Electrical and Computer Engineering
                Interdisciplinary Program in Artificial Intelligence
                Seoul National University, Seoul 08826, Republic of Korea[0m

Box rectangle:  [32m(90.0, 308.3) -> (188.7, 318.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Scott Niekum[0m

Box rectangle:  [32m(280.3, 341.9) -> (331.7, 353.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 360.6) -> (503.5, 575.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImitation learning, in which learning is performed by demonstration, has been studied
                and advanced for sequential decision-making tasks in which a reward function is not
                predefined. However, imitation learning methods still require numerous expert demonstration
                samples to successfully imitate an expert’s behavior. To improve sample efficiency, we utilize
                self-supervised representation learning, which can generate vast training signals from the
                given data. In this study, we propose a self-supervised representation-based adversarial
                imitation learning method to learn state and action representations that are robust to
                diverse distortions and temporally predictive, on non-image control tasks. In particular, in
                comparison with existing self-supervised learning methods for tabular data, we propose a
                different corruption method for state and action representations that is robust to diverse
                distortions. We theoretically and empirically observe that making an informative feature
                manifold with less sample complexity significantly improves the performance of imitation
                learning. The proposed method shows a 39% relative improvement over existing adversarial
                imitation learning methods on MuJoCo in a setting limited to 100 expert state-action
                pairs. Moreover, we conduct comprehensive ablations and additional experiments using
                demonstrations with varying optimality to provide insights into a range of factors.
                Keywords:
                imitation learning, adversarial imitation learning, self-supervised learning,
                data efficiency[0m

Box rectangle:  [32m(90.0, 596.6) -> (180.2, 608.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.7, 618.9) -> (523.4, 670.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mImitation learning (IL) is widely used in sequential decision-making tasks, where the design
                of a reward function is complicated or uncertain. When a reward is sparse (Reddy et al.,
                2019) or an optimal reward function is unknown, IL finds an optimal policy that relies only
                on expert demonstrations. Owing to recent development in deep neural networks, the range[0m

Box rectangle:  [32m(93.7, 684.9) -> (485.4, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Corresponding Author
                A preliminary version of this manuscript was presented at Deep RL Workshop, NeurIPS 2022.[0m

Box rectangle:  [32m(89.1, 726.4) -> (296.8, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Dahuin Jung, Hyungyu Lee, and Sungroh Yoon.[0m

Box rectangle:  [32m(90.0, 740.8) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0314.html.[0m



=== Processing ../JMLR 2024/Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scalable High-Dimensional Multivariate Linear Regression for Feature-Distributed Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 7/23; Revised 6/24; Published 7/24[0m

Box rectangle:  [32m(99.4, 101.6) -> (512.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScalable High-Dimensional Multivariate Linear Regression
                for Feature-Distributed Data[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShuo-Chieh Huang
                shuochieh@chicagobooth.edu
                Ruey S. Tsay
                ruey.tsay@chicagobooth.edu
                Booth School of Business
                University of Chicago
                Chicago, IL 60637, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (258.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shuo-Chieh Huang and Ruey S. Tsay.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0882.html.[0m



=== Processing ../JMLR 2024/Scalable Resampling in Massive Generalized Linear Models via Subsampled Residual Bootstrap.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scalable Resampling in Massive Generalized Linear Models via Subsampled Residual Bootstrap.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 7/23; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(96.0, 101.6) -> (516.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScalable Resampling in Massive Generalized Linear Models
                via Subsampled Residual Bootstrap[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIndrila Ganguly
                iganguly@fredhutch.org
                Biostatistics, Bioinformatics and Epidemiology Program
                Fred Hutchinson Cancer Center
                Seattle, WA 98109, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSrijan Sengupta
                ssengup2@ncsu.edu
                Department of Statistics
                North Carolina State University
                Raleigh, NC 27695-7103, USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSujit Ghosh
                sujit.ghosh@ncsu.edu
                Department of Statistics
                North Carolina State University
                Raleigh, NC 27695-7103, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (308.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Indrila Ganguly, Srijan Sengupta, and Sujit Ghosh.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0916.html.[0m



=== Processing ../JMLR 2024/Scaled Conjugate Gradient Method for Nonconvex Optimization in Deep Neural Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scaled Conjugate Gradient Method for Nonconvex Optimization in Deep Neural Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-37
                Submitted 7/22; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(126.5, 101.6) -> (485.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScaled Conjugate Gradient Method for Nonconvex
                Optimization in Deep Neural Networks[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNaoki Sato
                naoki310303@gmail.com
                Computer Science Course,
                Graduate School of Science and Technology,
                Meiji University, Kanagawa 214-8571, Japan[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKoshiro Izumi
                koshi-izumi@secom.co.jp
                Visual Solutions Department 1,
                Technology Development Division,
                SECOM Co., Ltd., Tokyo 181-8528, Japan[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 299.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHideaki Iiduka
                iiduka@cs.meiji.ac.jp
                Department of Computer Science,
                Meiji University, Kanagawa 214-8571, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (294.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Naoki Sato, Koshiro Izumi, and Hideaki Iiduka.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0815.html.[0m



=== Processing ../JMLR 2024/Scaling Instruction-Finetuned Language Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scaling Instruction-Finetuned Language Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 7/23; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(140.1, 101.5) -> (472.0, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScaling Instruction-Finetuned Language Models[0m

Box rectangle:  [32m(89.4, 135.5) -> (523.7, 297.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHyung Won Chung∗
                h.w.chung27@gmail.com
                Le Hou∗
                lehou@google.com
                Shayne Longpre∗
                slongpre@media.mit.edu
                Barret Zoph†
                barretzoph@gmail.com
                Yi Tai†
                ytay017@gmail.com
                William Fedus†
                liam.fedus@gmail.com
                Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
                Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
                Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav
                Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav
                Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,
                Jason Wei∗[0m

Box rectangle:  [32m(90.0, 322.7) -> (224.3, 332.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ruslan Salakhutdinov[0m

Box rectangle:  [32m(280.3, 356.3) -> (331.7, 368.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.6, 374.2) -> (503.4, 566.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFinetuning language models on a collection of datasets phrased as instructions has been
                shown to improve model performance and generalization to unseen tasks. In this paper
                we explore instruction finetuning with a particular focus on (1) scaling the number of
                tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find
                that instruction finetuning with the above aspects dramatically improves performance on
                a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot,
                CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation,
                RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks
                outperforms PaLM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves
                state-of-the-art performance on several benchmarks (at time of release), such as 75.2%
                on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong
                few-shot performance even compared to much larger models, such as PaLM 62B. Overall,
                instruction finetuning is a general method for improving the performance and usability of
                pretrained language models.
                Keywords:
                Natural Language Processing, Language Models, Instruction Finetuning,
                Chain-of-Thought Reasoning, Bias & Toxicity[0m

Box rectangle:  [32m(90.0, 586.9) -> (180.2, 598.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(89.6, 609.5) -> (521.9, 647.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAn important goal of artificial intelligence is to develop models that can generalize to
                unseen tasks. In natural language processing (NLP), pretrained language models have made
                significant progress towards this goal, as they can perform tasks given natural language[0m

Box rectangle:  [32m(93.7, 663.0) -> (522.1, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Equal contribution. Work done while at Google.
                †. Core contributor. Work done while at Google.
                1. Public
                checkpoints:
                https://github.com/google-research/t5x/blob/main/docs/models.md#
                flan-t5-checkpoints.[0m

Box rectangle:  [32m(89.1, 726.4) -> (355.6, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Hyung Won Chung, Le Hou, Shayne Longpre, Jason Wei, et al..[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0870.html.[0m



=== Processing ../JMLR 2024/Scaling Speech Technology to 1 000+ Languages.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scaling Speech Technology to 1 000+ Languages.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 10/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(138.7, 101.5) -> (473.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScaling Speech Technology to 1,000+ Languages[0m

Box rectangle:  [32m(89.5, 135.1) -> (452.4, 147.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVineel Pratap∗
                Andros Tjandra∗
                Bowen Shi∗
                Paden Tomasello[0m

Box rectangle:  [32m(89.5, 152.4) -> (411.7, 165.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArun Babu
                Sayani Kundu†
                Ali Elkahky‡
                Zhaoheng Ni[0m

Box rectangle:  [32m(89.5, 171.5) -> (466.0, 182.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mApoorv Vyas
                Maryam Fazel-Zarandi
                Alexei Baevski
                Yossi Adi[0m

Box rectangle:  [32m(89.5, 187.0) -> (456.3, 199.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaohui Zhang
                Wei-Ning Hsu
                Alexis Conneau§
                Michael Auli∗[0m

Box rectangle:  [32m(89.5, 216.6) -> (142.0, 226.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFAIR, Meta[0m

Box rectangle:  [32m(90.0, 726.3) -> (522.0, 753.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky,
                Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis
                Conneau, Michael Auli.[0m

Box rectangle:  [32m(90.0, 759.9) -> (517.0, 777.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1318.html.[0m



=== Processing ../JMLR 2024/Scaling the Convex Barrier with Sparse Dual Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Scaling the Convex Barrier with Sparse Dual Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 1/21; Revised 11/22; Published 1/24[0m

Box rectangle:  [32m(104.3, 101.6) -> (507.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mScaling the Convex Barrier with Sparse Dual Algorithms[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 242.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlessandro De Palma
                adepalma@robots.ox.ac.uk
                Harkirat Singh Behl
                harkirat@robots.ox.ac.uk
                Rudy Bunel
                bunel.rudy@gmail.com
                Philip H.S. Torr
                phst@robots.ox.ac.uk
                M. Pawan Kumar
                pawan@robots.ox.ac.uk
                Department of Engineering Science
                University of Oxford
                Oxford OX1 3PJ[0m

Box rectangle:  [32m(90.0, 726.3) -> (479.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H.S. Torr and M. Pawan Kumar.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0076.html.[0m



=== Processing ../JMLR 2024/Seeded Graph Matching for the Correlated Gaussian Wigner Model via the Projected Power Method.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Seeded Graph Matching for the Correlated Gaussian Wigner Model via the Projected Power Method.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.5) -> (522.0, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-43
                Submitted 4/22; Revised 7/23; Published 1/24[0m

Box rectangle:  [32m(90.2, 101.3) -> (521.8, 133.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSeeded Graph Matching for the Correlated Gaussian Wigner
                Model via the Projected Power Method[0m

Box rectangle:  [32m(90.0, 153.1) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErnesto Araya
                ernesto-javier.araya-valdivia@inria.fr
                UMR 8524 - Laboratoire Paul Painlevé, F-59000
                Inria, Univ. Lille, CNRS[0m

Box rectangle:  [32m(90.0, 194.7) -> (522.0, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGuillaume Braun
                guillaume.braun@riken.jp
                Riken-AIP, Tokyo[0m

Box rectangle:  [32m(90.0, 226.0) -> (522.0, 263.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHemant Tyagi
                hemant.tyagi@inria.fr
                UMR 8524 - Laboratoire Paul Painlevé, F-59000
                Inria, Univ. Lille, CNRS[0m

Box rectangle:  [32m(90.0, 302.6) -> (201.9, 312.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Tina Eliassi-Rad[0m

Box rectangle:  [32m(280.3, 336.3) -> (331.7, 348.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 354.2) -> (502.1, 569.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn the graph matching problem we observe two graphs G, H and the goal is to find an
                assignment (or matching) between their vertices such that some measure of edge agreement
                is maximized. We assume in this work that the observed pair G, H has been drawn from
                the Correlated Gaussian Wigner (CGW) model – a popular model for correlated weighted
                graphs – where the entries of the adjacency matrices of G and H are independent Gaussians
                and each edge of G is correlated with one edge of H (determined by the unknown matching)
                with the edge correlation described by a parameter σ ∈[0, 1). In this paper, we analyse the
                performance of the projected power method (PPM) as a seeded graph matching algorithm
                where we are given an initial partially correct matching (called the seed) as side information.
                We prove that if the seed is close enough to the ground-truth matching, then with high
                probability, PPM iteratively improves the seed and recovers the ground-truth matching
                (either partially or exactly) in O(log n) iterations. Our results prove that PPM works even
                in regimes of constant σ, thus extending the analysis in (Mao et al., 2023) for the sparse
                Correlated Erdős-Renyi (CER) model to the (dense) CGW model. As a byproduct of our
                analysis, we see that the PPM framework generalizes some of the state-of-art algorithms
                for seeded graph matching.
                We support and complement our theoretical findings with
                numerical experiments on synthetic data.
                Keywords:
                graph matching, correlated Wigner model, projected power method.[0m

Box rectangle:  [32m(90.0, 590.2) -> (180.2, 602.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 612.8) -> (522.0, 705.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn the graph matching problem we are given as input two graphs G and H with an equal
                number of vertices, and the objective is to find a bijective function, or matching, between the
                vertices of G and H such that the alignment between the edges of G and H is maximized.
                This problem appears in many applications such as computer vision (Sun et al., 2020),
                network de-anonymization (Narayanan and Shmatikov, 2009), pattern recognition (Conte
                et al., 2004; Emmert-Streib et al., 2016), protein-protein interactions and computational
                biology (Zaslavskiy et al., 2009b; Singh et al., 2009). In computer vision, for example, it is[0m

Box rectangle:  [32m(90.0, 726.3) -> (312.1, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Ernesto Araya, Guillaume Braun and Hemant Tyagi.[0m

Box rectangle:  [32m(90.0, 740.8) -> (516.9, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0402.html.[0m



=== Processing ../JMLR 2024/Semi-supervised Inference for Block-wise Missing Data without Imputation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Semi-supervised Inference for Block-wise Missing Data without Imputation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 12/21; Revised 2/24; Published 2/24[0m

Box rectangle:  [32m(111.7, 101.6) -> (500.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSemi-supervised Inference for Block-wise Missing Data
                without Imputation[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShanshan Song
                songss@tongji.edu.cn
                School of Mathematical Sciences and School of Economics and Management
                Tongji University
                Shanghai, 200092, China[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuanyuan Lin∗
                ylin@sta.cuhk.edu.hk
                Department of Statistics
                The Chinese University of Hong Kong
                Hong Kong, China[0m

Box rectangle:  [32m(90.0, 260.3) -> (522.0, 327.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYong Zhou∗
                yzhou@fem.ecnu.edu.cn
                Key Laboratory of Advanced Theory and Application in Statistics and Data Science, MOE,
                Academy of Statistics and Interdisciplinary Sciences and School of Statistics
                East China Normal University
                Shanghai, 200062, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (296.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Shanshan Song, Yuanyuan Lin and Yong Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1504.html.[0m



=== Processing ../JMLR 2024/Sharp analysis of power iteration for tensor PCA.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sharp analysis of power iteration for tensor PCA.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 1/24; Revised 5/24; Published 6/24[0m

Box rectangle:  [32m(126.9, 101.6) -> (485.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSharp Analysis of Power Iteration for Tensor PCA[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuchen Wu
                wuyc14@wharton.upenn.edu
                Department of Statistics and Data Science
                University of Pennsylvania
                Philadelphia, PA 19104-6303, USA[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKangjie Zhou
                kangjie@stanford.edu
                Department of Statistics
                Stanford University
                Stanford, CA 94305-2004, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (232.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuchen Wu and Kangjie Zhou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0006.html.[0m



=== Processing ../JMLR 2024/Sharpness-Aware Minimization and the Edge of Stability.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sharpness-Aware Minimization and the Edge of Stability.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-20
                Submitted 10/23; Revised 4/24; Published 6/24[0m

Box rectangle:  [32m(195.8, 101.6) -> (416.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSharpness-Aware Minimization
                and the Edge of Stability[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPhilip M. Long
                plong@google.com
                Peter L. Bartlett∗
                peterbartlett@google.com
                Google
                1600 Amphitheatre Parkway
                Mountain View, CA 94040[0m

Box rectangle:  [32m(90.0, 726.3) -> (258.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Philip M. Long and Peter L. Bartlett.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1285.html.[0m



=== Processing ../JMLR 2024/Simple Cycle Reservoirs are Universal.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Simple Cycle Reservoirs are Universal.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-28
                Submitted 8/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(170.9, 101.6) -> (441.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSimple Cycle Reservoirs are Universal[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBoyu Li
                boyuli@nmsu.edu
                Department of Mathematical Sciences
                New Mexico State University
                Las Cruces, New Mexico, 88003, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRobert Simon Fong
                r.s.fong@bham.ac.uk
                School of Computer Science
                University of Birmingham
                Birmingham, B15 2TT, UK[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter Tiˇno
                p.tino@bham.ac.uk
                School of Computer Science
                University of Birmingham
                Birmingham, B15 2TT, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (289.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Boyu Li, Robert Simon Fong, and Peter Tiˇno.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1075.html.[0m



=== Processing ../JMLR 2024/skscope  Fast Sparsity-Constrained Optimization in Python.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/skscope  Fast Sparsity-Constrained Optimization in Python.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-9
                Submitted 11/23; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(96.4, 101.6) -> (515.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mskscope: Fast Sparsity-Constrained Optimization in Python[0m

Box rectangle:  [32m(90.0, 135.3) -> (522.0, 242.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZezhi Wang1, Junxian Zhu2
                homura@mail.ustc.edu.cn, junxian@nus.edu.sg
                Xueqin Wang1, Jin Zhu3
                wangxq20@ustc.edu.cn, J.Zhu69@lse.ac.uk
                Huiyang Peng1, Peng Chen1
                {kisstherain, chenpeng1}@mail.ustc.edu.cn
                Anran Wang1, Xiaoke Zhang1
                {wanganran, zxk170091}@mail.ustc.edu.cn
                1 Department of Statistics and Finance/International Institute of Finance, School of Management,
                University of Science and Technology of China, Hefei, Anhui, China
                2 Saw Swee Hock School of Public Health, National University of Singapore, Singapore
                3 Department of Statistics, London School of Economics and Political Science, London, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (516.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zezhi Wang, Junxian Zhu, Xueqin Wang, Jin Zhu, Huiyang Peng, Peng Chen, Anran Wang, Xiaoke Zhang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (512.2, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1574.html.[0m



=== Processing ../JMLR 2024/Sparse Graphical Linear Dynamical Systems.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sparse Graphical Linear Dynamical Systems.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 7/23; Revised 5/24; Published 7/24[0m

Box rectangle:  [32m(149.0, 101.6) -> (463.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSparse Graphical Linear Dynamical Systems[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmilie Chouzenoux
                emilie.chouzenoux@inria.fr
                Center for Visual Computing
                Inria, University Paris Saclay
                91190 Gif-sur-Yvette, France[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mV ́ıctor Elvira
                victor.elvira@ed.ac.uk
                School of Mathematics
                University of Edinburgh
                EH9 3FD Edinburgh, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (260.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Emilie Chouzenoux and V ́ıctor Elvira.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0878.html.[0m



=== Processing ../JMLR 2024/Sparse NMF with Archetypal Regularization  Computational and Robustness Properties.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sparse NMF with Archetypal Regularization  Computational and Robustness Properties.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-62
                Submitted 3/21; Revised 11/23; Published 1/24[0m

Box rectangle:  [32m(145.2, 101.6) -> (467.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSparse NMF with Archetypal Regularization:
                Computational and Robustness Properties[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKayhan Behdin
                behdink@mit.edu
                Operations Research Center
                Massachusetts Institute of Technology
                Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRahul Mazumder
                rahulmaz@mit.edu
                MIT Sloan School of Management and Operations Research Center
                Massachusetts Institute of Technology
                Cambridge, MA 02139, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (261.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Kayhan Behdin and Rahul Mazumder.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-0233.html.[0m



=== Processing ../JMLR 2024/Sparse Recovery With Multiple Data Streams  An Adaptive Sequential Testing Approach.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sparse Recovery With Multiple Data Streams  An Adaptive Sequential Testing Approach.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 11/22; Revised 9/24; Published 9/24[0m

Box rectangle:  [32m(92.0, 101.6) -> (520.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSparse Recovery With Multiple Data Streams: An Adaptive
                Sequential Testing Approach[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWeinan Wang
                wwang@snap.com
                Snap Inc.
                2850 Ocean Park Blvd.
                Santa Monica, CA, 90405, USA[0m

Box rectangle:  [32m(90.0, 217.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBowen Gang
                bgang@fudan.edu.cn
                Department of Statistics and Data Science
                Fudan University
                Shanghai, 200433, China[0m

Box rectangle:  [32m(90.0, 284.5) -> (522.0, 337.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWenguang Sun
                wgsun@zju.edu.cn
                School of Management and Center for Data Science
                Zhejiang University
                Hangzhou, 310013, China[0m

Box rectangle:  [32m(90.0, 362.5) -> (215.9, 372.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Pradeep Ravikumar[0m

Box rectangle:  [32m(280.3, 398.0) -> (331.7, 410.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 416.1) -> (502.1, 605.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMultistage design has been utilized across a variety of scientific fields, enabling the
                adaptive allocation of sensing resources to effectively eliminate null locations and localize
                signals. We present a decision-theoretic framework for multi-stage adaptive testing that
                minimizes the total number of measurements while ensuring pre-specified constraints on
                both the false positive rate (FPR) and the missed discovery rate (MDR). Our method,
                SMART, explicitly addresses the often-overlooked aspect of uncertainty quantification in
                machine learning algorithms, incorporating it at every decision stage. This enables SMART
                to respond adaptively to important patterns in the data streams, adjusting its decisions
                based on the strength of evidence at specific locations. By leveraging technical tools and
                key concepts from multiple testing, adaptive thresholding, and compound decision theory,
                SMART not only enhances the aggregation of information across individual tests but also
                allows for varying thresholds tailored to the observed data, thereby ensuring effective error
                rate control and resulting in significant savings on total study costs. Through compre-
                hensive analyses of large-scale A/B tests, high-throughput screening, and image analysis,
                we demonstrate that our approach yields substantial efficiency gains and improved control
                over error rates compared to existing methodologies.[0m

Box rectangle:  [32m(109.9, 616.4) -> (502.1, 638.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: Compound Decision Problem; Distilled Sensing; False Discovery Rate; Missed
                Discovery Rate; Sequential Probability Ratio Test[0m

Box rectangle:  [32m(90.0, 658.6) -> (180.3, 670.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 680.8) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn high-dimensional data analysis, recovering the support of a sparse vector is a fundamental
                challenge. We study the problem of estimating the support of a high-dimensional sparse[0m

Box rectangle:  [32m(90.0, 726.4) -> (300.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Weinan Wang, Bowen Gang and Wenguang Sun.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1310.html.[0m



=== Processing ../JMLR 2024/Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 5/23; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(97.1, 101.6) -> (514.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSparse Representer Theorems for Learning in Reproducing
                Kernel Banach Spaces[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRui Wang
                rwang11@jlu.edu.cn
                School of Mathematics
                Jilin University
                Changchun, 130012, P. R. China[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 273.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuesheng Xu
                y1xu@odu.edu
                Mingsong Yan
                myan007@odu.edu
                Department of Mathematics and Statistics
                Old Dominion University
                Norfolk, VA 23529, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (283.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rui Wang, Yuesheng Xu and Mingsong Yan.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0645.html.[0m



=== Processing ../JMLR 2024/Spatial meshing for general Bayesian multivariate models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Spatial meshing for general Bayesian multivariate models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 1/22; Revised 2/24; Published 3/24[0m

Box rectangle:  [32m(204.4, 101.5) -> (407.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpatial meshing for general
                Bayesian multivariate models[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichele Peruzzi
                peruzzi@umich.edu
                Department of Biostatistics
                University of Michigan
                Ann Arbor, MI 48109-2029, USA[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDavid B. Dunson
                dunson@duke.edu
                Department of Statistical Science
                Duke University
                Durham, NC 27708-0251, USA[0m

Box rectangle:  [32m(90.0, 285.0) -> (188.2, 294.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Debdeep Pati[0m

Box rectangle:  [32m(280.3, 320.5) -> (331.7, 332.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 339.7) -> (502.1, 553.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mQuantifying spatial and/or temporal associations in multivariate geolocated data of differ-
                ent types is achievable via spatial random effects in a Bayesian hierarchical model, but se-
                vere computational bottlenecks arise when spatial dependence is encoded as a latent Gaus-
                sian process (GP) in the increasingly common large scale data settings on which we focus.
                The scenario worsens in non-Gaussian models because the reduced analytical tractabil-
                ity leads to additional hurdles to computational efficiency. In this article, we introduce
                Bayesian models of spatially referenced data in which the likelihood or the latent process
                (or both) are not Gaussian. First, we exploit the advantages of spatial processes built via
                directed acyclic graphs, in which case the spatial nodes enter the Bayesian hierarchy and
                lead to posterior sampling via routine Markov chain Monte Carlo (MCMC) methods. Sec-
                ond, motivated by the possible inefficiencies of popular gradient-based sampling approaches
                in the multivariate contexts on which we focus, we introduce the simplified manifold pre-
                conditioner adaptation (SiMPA) algorithm which uses second order information about the
                target but avoids expensive matrix operations. We demostrate the performance and effi-
                ciency improvements of our methods relative to alternatives in extensive synthetic and real
                world remote sensing and community ecology applications with large scale data at up to
                hundreds of thousands of spatial locations and up to tens of outcomes. Software for the
                proposed methods is part of R package meshed, available on CRAN.[0m

Box rectangle:  [32m(109.9, 558.5) -> (502.1, 580.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                Multivariate models, Directed acyclic graph, Gaussian process, non-Gaussian
                data, Markov chain Monte Carlo, Langevin algorithms.[0m

Box rectangle:  [32m(90.0, 602.6) -> (180.2, 614.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 626.6) -> (521.9, 705.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeolocated data are routinely collected in many fields and motivate the development of
                geostatistical models based on Gaussian processes (GPs). GPs are appealing due to their
                analytical tractability, their flexibility via a multitude of covariance or kernel choices, and
                their ability to effectively represent and quantify uncertainty.
                When Gaussian distribu-
                tional assumptions are appropriate, GPs may be used directly as correlation models for the
                multivariate response. Otherwise, flexible models of multivariate spatial association can in[0m

Box rectangle:  [32m(90.0, 726.4) -> (263.1, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Michele Peruzzi and David B. Dunson.[0m

Box rectangle:  [32m(90.0, 740.8) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0083.html.[0m



=== Processing ../JMLR 2024/Spectral Analysis of the Neural Tangent Kernel for Deep Residual Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Spectral Analysis of the Neural Tangent Kernel for Deep Residual Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 5/22; Published 4/24[0m

Box rectangle:  [32m(104.1, 101.4) -> (508.0, 133.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpectral Analysis of the Neural Tangent Kernel for Deep
                Residual Networks[0m

Box rectangle:  [32m(90.0, 151.7) -> (522.0, 199.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuval Belfer*
                yuval.belfer@weizmann.ac.il
                Department of Math and CS
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 205.3) -> (522.0, 253.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAmnon Geifman*
                amnon.geifman@weizmann.ac.il
                Department of Math and CS
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 258.9) -> (522.0, 306.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMeirav Galun
                meirav.galun@weizmann.ac.il
                Department of Math and CS
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 314.0) -> (522.0, 366.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRonen Basri
                ronen.basri@weizmann.ac.il
                Department of Math and CS
                Weizmann Institute of Science
                Rehovot, Israel[0m

Box rectangle:  [32m(90.0, 726.3) -> (339.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuval Belfer, Amnon Geifman, Meirav Galun, Ronen Basri.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0597.html.[0m



=== Processing ../JMLR 2024/Spectral learning of multivariate extremes.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Spectral learning of multivariate extremes.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-36
                Submitted 11/21; Revised 12/23; Published 4/24[0m

Box rectangle:  [32m(157.0, 101.6) -> (455.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpectral learning of multivariate extremes[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarco Avella Medina
                marco.avella@columbia.edu
                Department of Statistics
                Columbia University
                New York, NY, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRichard A. Davis
                rdavis@stat.columbia.edu
                Department of Statistics
                Columbia University
                New York, NY, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGennady Samorodnitsky
                gs18@cornell.edu
                School of Operations Research and Information Engineering
                Cornell University
                Ithaca, NY, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (379.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Marco Avella Medina, Richard A. Davis and Gennady Samorodnitsky.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1367.html.[0m



=== Processing ../JMLR 2024/Spectral Regularized Kernel Goodness-of-Fit Tests.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Spectral Regularized Kernel Goodness-of-Fit Tests.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 8/23; Published 10/24[0m

Box rectangle:  [32m(126.5, 101.6) -> (485.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpectral Regularized Kernel Goodness-of-Fit Tests[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 215.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOmar Hagrass
                oih3@psu.edu
                Bharath K. Sriperumbudur
                bks18@psu.edu
                Bing Li
                bxl9@psu.edu
                Department of Statistics
                Pennsylvania State University
                University Park, PA, 16802 USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (326.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Omar Hagrass, Bharath K. Sriperumbudur and Bing Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1031.html.[0m



=== Processing ../JMLR 2024/Spherical Rotation Dimension Reduction with Geometric Loss Functions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Spherical Rotation Dimension Reduction with Geometric Loss Functions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 4/23; Revised 3/24; Published 6/24[0m

Box rectangle:  [32m(103.3, 101.6) -> (508.9, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpherical Rotation Dimension Reduction with Geometric
                Loss Functions[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHengrui Luo
                hrluo@rice.edu
                Lawrence Berkeley National Laboratory
                Berkeley, CA, 94720, USA
                Department of Statistics, Rice University
                Houston, TX, 77005, USA[0m

Box rectangle:  [32m(90.0, 217.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJeremy E. Purvis
                purvisj@email.unc.edu
                Department of Genetics
                University of North Carolina at Chapel Hill
                Chapel Hill, NC 27599, USA[0m

Box rectangle:  [32m(90.0, 272.6) -> (522.0, 325.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDidong Li
                didongli@unc.edu
                Department of Biostatistics
                University of North Carolina at Chapel Hill
                Chapel Hill, NC 27599, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (281.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hengrui Luo, Jeremy Purvis and Didong Li.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0547.html.[0m



=== Processing ../JMLR 2024/Split Conformal Prediction and Non-Exchangeable Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Split Conformal Prediction and Non-Exchangeable Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 11/23; Revised 7/24; Published 7/24[0m

Box rectangle:  [32m(110.7, 101.5) -> (501.4, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSplit Conformal Prediction and Non-Exchangeable Data[0m

Box rectangle:  [32m(89.5, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRoberto I. Oliveira
                rimfo@impa.br
                IMPA, Rio de Janeiro, Brazil.[0m

Box rectangle:  [32m(89.5, 163.5) -> (522.0, 187.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPaulo Orenstein
                pauloo@impa.br
                IMPA, Rio de Janeiro, Brazil.[0m

Box rectangle:  [32m(89.5, 193.2) -> (522.0, 217.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThiago Ramos
                thiagorr@impa.br
                IMPA, Rio de Janeiro, Brazil.[0m

Box rectangle:  [32m(89.5, 224.5) -> (522.0, 250.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoão Vitor Romano
                joao.vitor@impa.br
                IMPA, Rio de Janeiro, Brazil.[0m

Box rectangle:  [32m(90.0, 275.4) -> (180.6, 285.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Chris Oates[0m

Box rectangle:  [32m(280.3, 309.0) -> (331.7, 320.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.6, 328.1) -> (503.5, 469.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSplit conformal prediction (CP) is arguably the most popular CP method for uncertainty
                quantification, enjoying both academic interest and widespread deployment. However, the
                original theoretical analysis of split CP makes the crucial assumption of data exchangeability,
                which hinders many real-world applications. In this paper, we present a novel theoretical
                framework based on concentration inequalities and decoupling properties of the data,
                proving that split CP remains valid for many non-exchangeable processes by adding a small
                coverage penalty. Through experiments with both real and synthetic data, we show that
                our theoretical results translate to good empirical performance under non-exchangeability,
                e.g., for time series and spatiotemporal data. Compared to recent conformal algorithms
                designed to counter specific exchangeability violations, we show that split CP is competitive
                in terms of coverage and interval size, with the benefit of being extremely simple and orders
                of magnitude faster than alternatives.[0m

Box rectangle:  [32m(109.9, 475.0) -> (503.4, 497.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                concentration inequalities, conformal prediction, non-exchangeable data,
                β-mixing, uncertainty quantification[0m

Box rectangle:  [32m(90.0, 519.0) -> (180.2, 531.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 542.7) -> (523.4, 713.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mConformal prediction (CP), introduced by Vovk et al. (2005), is a set of techniques for
                quantifying uncertainty in the predictions of any model, under very general assumptions on
                the data-generating distribution. CP yields finite-sample coverage guarantees of many kinds,
                and has generated much recent interest (Shafer and Vovk, 2008; Lei et al., 2018; Romano
                et al., 2019; Angelopoulos and Bates, 2023; Cauchois et al., 2021).
                A concrete and popular formulation of CP is split conformal prediction (Papadopoulos
                et al., 2002; Lei et al., 2018). Consider a regression setting where the data is a random
                sample (Xi, Yi)n
                i=1 of covariate and response pairs (Xi, Yi) ∈X × Y. Split CP proceeds
                as follows: (a) partition the data indices in three parts: a training set Itrain, a calibration
                set Ical and a test set Itest, each with sizes ntrain, ncal and ntest; (b) train a nonconformity
                score bstrain : X × Y →R, for example the residual bstrain(x, y) = |y −bμ(x)| of an arbitrary
                model bμ trained on (Xi, Yi)i∈Itrain; (c) compute the empirical (1 −α)-quantile bq1−α of[0m

Box rectangle:  [32m(89.1, 726.4) -> (406.7, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Roberto I. Oliveira, Paulo Orenstein, Thiago Ramos and João Vitor Romano.[0m

Box rectangle:  [32m(90.0, 740.8) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1553.html.[0m



=== Processing ../JMLR 2024/Stability and L2-penalty in Model Averaging.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stability and L2-penalty in Model Averaging.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 7/23; Revised 10/24; Published 10/24[0m

Box rectangle:  [32m(148.4, 101.6) -> (463.6, 117.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStability and L2-penalty in Model Averaging[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 201.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHengkun Zhu
                hkzhu@cnu.edu.cn
                Guohua Zou
                ghzou@amss.ac.cn
                School of Mathematical Sciences
                Capital Normal University
                Beijing 100048, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (235.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hengkun Zhu and Guohua Zou.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0853.html.[0m



=== Processing ../JMLR 2024/Stable and Consistent Density-Based Clustering via Multiparameter Persistence.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stable and Consistent Density-Based Clustering via Multiparameter Persistence.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-74
                Submitted 10/21; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(122.9, 101.6) -> (489.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStable and Consistent Density-Based Clustering via
                Multiparameter Persistence[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Rolle
                alexander.rolle@tum.de
                Department of Mathematics
                Technical University of Munich
                Boltzmannstraße 3, 85748 Garching, Germany[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLuis Scoccola
                luis.scoccola@maths.ox.ac.uk
                Mathematical Institute
                University of Oxford
                Woodstock Road, Oxford OX2 6GG, United Kingdom[0m

Box rectangle:  [32m(90.0, 726.3) -> (249.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alexander Rolle and Luis Scoccola.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1185.html.[0m



=== Processing ../JMLR 2024/Stable Implementation of Probabilistic ODE Solvers.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stable Implementation of Probabilistic ODE Solvers.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-29
                Submitted 12/20; Revised 7/22; Published 4/24[0m

Box rectangle:  [32m(121.2, 101.6) -> (490.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStable Implementation of Probabilistic ODE Solvers[0m

Box rectangle:  [32m(90.0, 133.6) -> (522.0, 170.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicholas Kr ̈amer∗
                nicholas.kraemer@uni-tuebingen.de
                T ̈ubingen AI Center, University of T ̈ubingen
                Maria-von-Linden-Straße 6, T ̈ubingen, Germany[0m

Box rectangle:  [32m(90.0, 177.1) -> (522.0, 216.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPhilipp Hennig
                philipp.hennig@uni-tuebingen.de
                T ̈ubingen AI Center, University of T ̈ubingen
                Maria-von-Linden-Straße 6, T ̈ubingen, Germany[0m

Box rectangle:  [32m(90.0, 726.3) -> (257.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Nicholas Kr ̈amer and Philipp Hennig.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/20-1423.html.[0m



=== Processing ../JMLR 2024/Stage-Aware Learning for Dynamic Treatments.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stage-Aware Learning for Dynamic Treatments.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 10/23; Published 12/24[0m

Box rectangle:  [32m(138.5, 101.6) -> (473.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStage-Aware Learning for Dynamic Treatments[0m

Box rectangle:  [32m(88.3, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanwen Ye
                hanweny@uci.edu
                Department of Statistics
                University of California
                Irvine, CA 92617, USA[0m

Box rectangle:  [32m(88.3, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWenzhuo Zhou
                wenzhuz3@uci.edu
                Department of Statistics
                University of California
                Irvine, CA 92617, USA[0m

Box rectangle:  [32m(88.3, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuoqing Zhu
                rqzhu@illinois.edu
                Department of Statistics
                University of Illinois
                Urbana-Champaign, IL 61820, USA[0m

Box rectangle:  [32m(88.3, 296.3) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnnie Qu
                aqu2@uci.edu
                Department of Statistics
                University of California
                Irvine, CA 92617, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (317.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Hanwen Ye, Wenzhuo Zhou, Ruoqing Zhu, Annie Qu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1422.html.[0m



=== Processing ../JMLR 2024/Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II  non-compact symmetric spaces.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II  non-compact symmetric spaces.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 1/23; Revised 7/24; Published 8/24[0m

Box rectangle:  [32m(90.0, 101.5) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStationary Kernels and Gaussian Processes on Lie Groups and
                their Homogeneous Spaces II: non-compact symmetric spaces[0m

Box rectangle:  [32m(89.5, 151.6) -> (522.0, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIskander Azangulov∗
                iska.azn@gmail.com
                St. Petersburg State University and University of Oxford[0m

Box rectangle:  [32m(89.5, 181.3) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrei Smolensky∗
                andrei.smolensky@gmail.com
                St. Petersburg State University and Neapolis University Pafos[0m

Box rectangle:  [32m(88.3, 211.1) -> (522.0, 235.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Terenin
                avt28@cornell.edu
                University of Cambridge and Cornell University[0m

Box rectangle:  [32m(89.5, 242.4) -> (522.0, 268.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mViacheslav Borovitskiy
                viacheslav.borovitskiy@gmail.com
                ETH Zürich[0m

Box rectangle:  [32m(90.0, 726.3) -> (427.1, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Iskander Azangulov, Andrei Smolensky, Alexander Terenin, Viacheslav Borovitskiy.[0m

Box rectangle:  [32m(90.0, 739.2) -> (495.0, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/23-0115.html.[0m



=== Processing ../JMLR 2024/Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I  the compact case.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces I  the compact case.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 12/22; Revised 11/23; Published 3/24[0m

Box rectangle:  [32m(90.0, 101.5) -> (522.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStationary Kernels and Gaussian Processes on Lie Groups and
                their Homogeneous Spaces I: the compact case[0m

Box rectangle:  [32m(89.5, 151.5) -> (522.0, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIskander Azangulov∗
                iska.azn@gmail.com
                St. Petersburg State University and University of Oxford[0m

Box rectangle:  [32m(89.5, 181.1) -> (522.0, 205.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrei Smolensky∗
                andrei.smolensky@gmail.com
                St. Petersburg State University and Neapolis University Pafos[0m

Box rectangle:  [32m(88.3, 211.1) -> (522.0, 235.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Terenin
                avt28@cornell.edu
                University of Cambridge and Cornell University[0m

Box rectangle:  [32m(89.5, 242.4) -> (522.0, 268.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mViacheslav Borovitskiy
                viacheslav.borovitskiy@gmail.com
                ETH Zürich[0m

Box rectangle:  [32m(90.0, 726.3) -> (427.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Iskander Azangulov, Andrei Smolensky, Alexander Terenin, Viacheslav Borovitskiy.[0m

Box rectangle:  [32m(90.0, 739.2) -> (495.0, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are
                provided at http://jmlr.org/papers/v25/22-1434.html.[0m



=== Processing ../JMLR 2024/Statistical analysis for a penalized EM algorithm in high-dimensional mixture linear regression model.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Statistical analysis for a penalized EM algorithm in high-dimensional mixture linear regression model.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-85
                Submitted 3/23; Revised 10/23; Published 7/24[0m

Box rectangle:  [32m(124.0, 101.6) -> (488.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStatistical analysis for a penalized EM algorithm in
                high-dimensional mixture linear regression model[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 274.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNing Wang
                ningwangbnu@bnu.edu.cn
                Center for Statistics and Data Science
                Beijing Normal University
                Zhuhai, Guangdong, China
                Xin Zhang
                henry@stat.fsu.edu
                Qing Mai
                qmai@fsu.edu
                Department of Statistics
                Florida State University
                Tallahassee, FL, 32306, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (260.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ning Wang, Xin Zhang and Qing Mai.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0296.html.[0m



=== Processing ../JMLR 2024/Statistical Inference for Fairness Auditing.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Statistical Inference for Fairness Auditing.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 6/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(158.2, 101.6) -> (453.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStatistical Inference for Fairness Auditing[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJohn J. Cherian
                jcherian@stanford.edu
                Department of Statistics
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 189.1) -> (522.0, 241.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmmanuel J. Cand`es
                candes@stanford.edu
                Departments of Mathematics and Statistics
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (277.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 John J. Cherian and Emmanuel J. Cand`es.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0739.html.[0m



=== Processing ../JMLR 2024/Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 11/22; Revised 2/24; Published 4/24[0m

Box rectangle:  [32m(97.7, 101.6) -> (514.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStatistical Optimality of Divide and Conquer Kernel-based
                Functional Linear Regression[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJiading Liu
                20210180088@fudan.edu.cn
                School of Mathematical Sciences
                Fudan University, Shanghai 200433, China[0m

Box rectangle:  [32m(90.0, 206.7) -> (522.0, 286.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLei Shi∗
                leishi@fudan.edu.cn
                School of Mathematical Sciences and Shanghai
                Key Laboratory for Contemporary Applied Mathematics
                Fudan University, Shanghai 200433, China
                Shanghai Artificial Intelligence Laboratory
                701 Yunjin Road, Shanghai 200232, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (207.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jiading Liu and Lei Shi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1326.html.[0m



=== Processing ../JMLR 2024/Stochastic Approximation with Decision-Dependent Distributions  Asymptotic Normality and Optimality.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stochastic Approximation with Decision-Dependent Distributions  Asymptotic Normality and Optimality.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 7/22; Revised 12/23; Published 2/24[0m

Box rectangle:  [32m(118.1, 101.6) -> (494.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStochastic Approximation with Decision-Dependent
                Distributions: Asymptotic Normality and Optimality[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoshua Cutler
                jocutler@uw.edu
                Department of Mathematics
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(89.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMateo D ́ıaz
                mateodd@jhu.edu
                Department of Applied Mathematics and Statistics
                Johns Hopkins University
                Baltimore, MD 21218, USA[0m

Box rectangle:  [32m(88.3, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDmitriy Drusvyatskiy
                ddruv@uw.edu
                Department of Mathematics
                University of Washington
                Seattle, WA 98195, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (322.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Joshua Cutler, Mateo D ́ıaz, and Dmitriy Drusvyatskiy.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0832.html.[0m



=== Processing ../JMLR 2024/Stochastic-Constrained Stochastic Optimization with Markovian Data.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stochastic-Constrained Stochastic Optimization with Markovian Data.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-69
                Submitted 12/23; Published 12/24[0m

Box rectangle:  [32m(118.9, 101.6) -> (493.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStochastic-Constrained Stochastic Optimization with
                Markovian Data[0m

Box rectangle:  [32m(89.4, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYeongjong Kim
                kim.yj@postech.ac.kr
                Department of Mathematics
                POSTECH
                Pohang 37673, South Korea[0m

Box rectangle:  [32m(89.4, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDabeen Lee
                dabeenl@kaist.ac.kr
                Department of Industrial and Systems Engineering
                KAIST
                Daejeon 34141, South Korea[0m

Box rectangle:  [32m(90.0, 726.3) -> (240.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yeongjong Kim and Dabeen Lee.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1630.html.[0m



=== Processing ../JMLR 2024/Stochastic Modified Flows  Mean-Field Limits and Dynamics of Stochastic Gradient Descent.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stochastic Modified Flows  Mean-Field Limits and Dynamics of Stochastic Gradient Descent.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-27
                Submitted 2/23; Revised 1/24; Published 1/24[0m

Box rectangle:  [32m(127.7, 101.6) -> (484.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStochastic Modified Flows, Mean-Field Limits and
                Dynamics of Stochastic Gradient Descent[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 235.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBenjamin Gess
                benjamin.gess@math.uni-bielefeld.de
                Fakult ̈at f ̈ur Mathematik
                Universit ̈at Bielefeld
                33615 Bielefeld, Germany
                and
                Max Planck Institute for Mathematics in the Sciences
                04103 Leipzig, Germany[0m

Box rectangle:  [32m(90.0, 241.3) -> (522.0, 289.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSebastian Kassing
                sebastian.kassing@math.uni-bielefeld.de
                Fakult ̈at f ̈ur Mathematik
                Universit ̈at Bielefeld
                33615 Bielefeld, Germany[0m

Box rectangle:  [32m(90.0, 296.5) -> (522.0, 403.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVitalii Konarovskyi
                vitalii.konarovskyi@uni-hamburg.de
                Fakult ̈at f ̈ur Mathematik, Informatik und
                Naturwissenschaften
                Universit ̈at Hamburg
                20146 Hamburg, Germany
                and
                Institute of Mathematics of NAS of Ukraine
                01024 Kyiv, Ukraine[0m

Box rectangle:  [32m(90.0, 726.3) -> (339.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Benjamin Gess, Sebastian Kassing and Vitalii Konarovskyi.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0220.html.[0m



=== Processing ../JMLR 2024/Stochastic Regularized Majorization-Minimization with weakly convex and multi-convex surrogates.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Stochastic Regularized Majorization-Minimization with weakly convex and multi-convex surrogates.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-83
                Submitted 3/23; Revised 9/23; Published 10/24[0m

Box rectangle:  [32m(131.5, 101.5) -> (480.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStochastic Regularized Majorization-Minimization
                with weakly convex and multi-convex surrogates[0m

Box rectangle:  [32m(90.0, 153.1) -> (522.0, 192.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHanbaek Lyu∗
                hlyu@math.wisc.edu
                Department of Mathematics
                University of Wisconsin - Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (135.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lyu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0349.html.[0m



=== Processing ../JMLR 2024/Structured Dynamic Pricing  Optimal Regret in a Global Shrinkage Model.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Structured Dynamic Pricing  Optimal Regret in a Global Shrinkage Model.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-46
                Submitted 10/23; Revised 5/24; Published 7/24[0m

Box rectangle:  [32m(106.9, 101.5) -> (505.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStructured Dynamic Pricing: Optimal Regret in a Global
                Shrinkage Model[0m

Box rectangle:  [32m(88.3, 151.8) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRashmi Ranjan Bhuyan
                bhuyanr@usc.edu
                Department of Data Sciences and Operations, Marshall School of Business
                University of Southern California, Los Angeles, CA 90089 , USA[0m

Box rectangle:  [32m(88.3, 193.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAdel Javanmard
                ajavanma@usc.edu
                Department of Data Sciences and Operations, Marshall School of Business
                University of Southern California, Los Angeles, CA 90089 , USA[0m

Box rectangle:  [32m(89.3, 235.0) -> (522.0, 259.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSungchul Kim
                sukim@adobe.com
                Adobe Research, 345 Park Avenue, San Jose, CA 95110 USA[0m

Box rectangle:  [32m(88.3, 264.7) -> (522.0, 300.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGourab Mukherjee
                gourab@usc.edu
                Department of Data Sciences and Operations, Marshall School of Business
                University of Southern California, Los Angeles, CA 90089 , USA[0m

Box rectangle:  [32m(89.3, 306.3) -> (522.0, 330.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRyan A. Rossi
                ryrossi@adobe.com
                Adobe Research, 345 Park Avenue, San Jose, CA 95110 USA[0m

Box rectangle:  [32m(89.3, 336.0) -> (522.0, 360.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTong Yu
                tyu@adobe.com
                Adobe Research, 345 Park Avenue, San Jose, CA 95110 USA[0m

Box rectangle:  [32m(89.3, 367.3) -> (522.0, 393.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHandong Zhao
                hazhao@adobe.com
                Adobe Research, 345 Park Avenue, San Jose, CA 95110 USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (501.9, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rashmi Ranjan Bhuyan, Adel Javanmard, Sungchul Kim, Gourab Mukherjee, Ryan A. Rossi, Tong Yu,
                Handong Zhao.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1365.html.[0m



=== Processing ../JMLR 2024/Structured Optimal Variational Inference for Dynamic Latent Space Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Structured Optimal Variational Inference for Dynamic Latent Space Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 5/22; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(113.2, 101.7) -> (498.9, 133.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStructured Optimal Variational Inference for Dynamic
                Latent Space Models[0m

Box rectangle:  [32m(90.0, 152.0) -> (522.0, 200.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeng Zhao
                pzhao@udel.edu
                Department of Applied Economics and Statistics
                University of Delaware
                Newark, DE 19716, USA[0m

Box rectangle:  [32m(90.0, 205.6) -> (518.2, 217.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnirban Bhattacharya
                anirbanb@stat.tamu.edu[0m

Box rectangle:  [32m(90.0, 223.3) -> (518.2, 235.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDebdeep Pati
                debdeep@stat.tamu.edu[0m

Box rectangle:  [32m(90.0, 242.6) -> (522.0, 295.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBani K. Mallick
                bmallick@stat.tamu.edu
                Department of Statistics
                Texas A&M University
                College Station, TX 77843, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (381.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Peng Zhao, Anirban Bhattacharya, Debdeep Pati and Bani K. Mallick.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0514.html.[0m



=== Processing ../JMLR 2024/Studying the Interplay between Information Loss and Operation Loss in Representations for Classification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Studying the Interplay between Information Loss and Operation Loss in Representations for Classification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-71
                Submitted 12/21; Revised 4/24; Published 9/24[0m

Box rectangle:  [32m(116.2, 101.6) -> (496.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mStudying the Interplay between Information Loss and
                Operation Loss in Representations for Classification[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJorge F. Silva
                jorgesil.edu@gmail.com
                Information and Decision System Group
                Universidad de Chile
                Santiago, 8370448, Chile[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFelipe Tobar
                f.tobar@imperial.ac.uk
                Department of Mathematics & I-X
                Imperial College London
                London, W12 0BZ, UK[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMario Vicu ̃na
                mario.vicuna@ing.uchile.cl
                Information and Decision System Group
                Universidad de Chile
                Santiago, 8370448, Chile[0m

Box rectangle:  [32m(90.0, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFelipe Cordova
                felipecordova@ug.uchile.cl
                Information and Decision System Group
                Universidad de Chile
                Santiago, 8370448, Chile[0m

Box rectangle:  [32m(90.0, 726.3) -> (340.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jorge F. Silva, Felipe Tobar, Mario Vicu ̃na, Felipe Cordova.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1551.html.[0m



=== Processing ../JMLR 2024/Sum-of-norms clustering does not separate nearby balls.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Sum-of-norms clustering does not separate nearby balls.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 5/21; Revised 3/24; Published 4/24[0m

Box rectangle:  [32m(109.6, 101.6) -> (502.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSum-of-norms clustering does not separate nearby balls[0m

Box rectangle:  [32m(88.8, 133.9) -> (522.0, 193.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlexander Dunlap
                dunlap@math.duke.edu
                Courant Institute of Mathematical Sciences
                New York University
                New York, NY 10012, USA
                Current address: Duke University, Durham, NC 27708, USA[0m

Box rectangle:  [32m(89.2, 213.0) -> (522.0, 292.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJean-Christophe Mourrat
                jean-christophe.mourrat@ens-lyon.fr
                Ecole Normale Sup ́erieure de Lyon and CNRS
                Lyon, France,
                and Courant Institute of Mathematical Sciences
                New York University
                New York, NY 10012, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (301.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alexander Dunlap and Jean-Christophe Mourrat.[0m

Box rectangle:  [32m(90.0, 741.0) -> (512.2, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-0495.html.[0m



=== Processing ../JMLR 2024/Survival Kernets  Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Survival Kernets  Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-78
                Submitted 6/22; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(93.4, 101.6) -> (518.7, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSurvival Kernets: Scalable and Interpretable
                Deep Kernel Survival Analysis with an Accuracy Guarantee[0m

Box rectangle:  [32m(90.0, 153.5) -> (522.0, 206.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge H. Chen
                georgechen@cmu.edu
                Heinz College of Information Systems and Public Policy
                Carnegie Mellon University
                Pittsburgh, PA 15213, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (180.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 George H. Chen.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0667.html.[0m



=== Processing ../JMLR 2024/Tangential Wasserstein Projections.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Tangential Wasserstein Projections.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-41
                Submitted 5/23; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(184.5, 101.5) -> (427.5, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTangential Wasserstein Projections[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFlorian Gunsilius
                ffg@umich.edu
                Department of Economics
                University of Michigan
                Ann Arbor, MI 48109-1220, USA[0m

Box rectangle:  [32m(90.0, 187.4) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMeng Hsuan Hsieh
                rexhsieh@umich.edu
                Ross School of Business
                University of Michigan
                Ann Arbor, MI 48109-1234, USA[0m

Box rectangle:  [32m(90.0, 242.6) -> (522.0, 295.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMyung Jin Lee
                leemjin@tamu.edu
                Mays Business School
                Texas A&M University
                College Station, TX 77843-0001, USA[0m

Box rectangle:  [32m(90.0, 320.6) -> (200.5, 330.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Quentin Berthet[0m

Box rectangle:  [32m(280.3, 356.1) -> (331.7, 368.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 372.9) -> (502.1, 503.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe develop a notion of projections between sets of probability measures using the geometric
                properties of the 2-Wasserstein space. In contrast to existing methods, it is designed for
                multivariate probability measures that need not be regular, and is computationally efficient
                to implement via regression.
                The idea is to work on tangent cones of the Wasserstein
                space using generalized geodesics. Its structure and computational properties make the
                method applicable in a variety of settings where probability measures need not be regular,
                from causal inference to the analysis of object data. An application to estimating causal
                effects yields a generalization of the synthetic controls method for systems with general
                heterogeneity described via multivariate probability measures.
                Keywords:
                Optimal Transport, Wasserstein distance, Generalized geodesics, Projection,
                Tangent Cone, Causal Inference, Synthetic Controls[0m

Box rectangle:  [32m(90.0, 523.3) -> (180.2, 535.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 544.7) -> (521.9, 704.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe concept of projections, that is, approximating a target quantity of interest by an opti-
                mally weighted combination of other quantities, is of fundamental relevance in mathematics,
                statistics, and machine learning. Statistical projections are generally defined between ran-
                dom variables in appropriately defined linear spaces (e.g. van der Vaart, 2000, chapter 11).
                In modern statistics and machine learning applications, the objects of interest are often
                probability measures themselves. Examples range from object- and functional data (e.g.
                Marron and Alonso, 2014) to causal inference with individual heterogeneity (e.g. Athey and
                Imbens, 2015).
                A notion of projection between sets of probability measures should be applicable between
                any set of general probability measures, replicate geometric properties of the target measure,
                and possess good computational and statistical properties. We introduce such a notion of
                projection between sets of general probability measures supported on Euclidean spaces. It[0m

Box rectangle:  [32m(90.0, 726.4) -> (337.9, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Florian Gunsilius, Meng Hsuan Hsieh, and Myung Jin Lee.[0m

Box rectangle:  [32m(90.0, 740.8) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0708.html.[0m



=== Processing ../JMLR 2024/Targeted Separation and Convergence with Kernel Discrepancies.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Targeted Separation and Convergence with Kernel Discrepancies.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-50
                Submitted 10/22; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(171.6, 101.6) -> (440.5, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTargeted Separation and Convergence
                with Kernel Discrepancies[0m

Box rectangle:  [32m(90.0, 152.9) -> (522.0, 177.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlessandro Barp†
                alessandro.barp@ucl.ac.uk
                University College London & The Alan Turing Institute, GB[0m

Box rectangle:  [32m(90.0, 182.5) -> (522.0, 206.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCarl-Johann Simon-Gabriel†⋆
                cjsg@mirelo.ai
                Mirelo AI[0m

Box rectangle:  [32m(90.0, 212.5) -> (522.0, 236.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMark Girolami
                mag92@cam.ac.uk
                University of Cambridge & The Alan Turing Institute, GB[0m

Box rectangle:  [32m(90.0, 243.5) -> (522.0, 269.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLester Mackey†
                lmackey@microsoft.com
                Microsoft Research, New England, US[0m

Box rectangle:  [32m(90.0, 726.3) -> (423.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, and Lester Mackey.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1123.html.[0m



=== Processing ../JMLR 2024/Tensor-train methods for sequential state and parameter learning in state-space models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Tensor-train methods for sequential state and parameter learning in state-space models.pdf') ---[0m

Box rectangle:  [32m(90.0, 42.0) -> (519.1, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 6/23; Revised 7/24; Published[0m

Box rectangle:  [32m(105.5, 101.6) -> (506.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTensor-train methods for sequential state and parameter
                learning in state-space models[0m

Box rectangle:  [32m(90.0, 153.5) -> (521.9, 219.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYiran Zhao
                yiran.zhao@sydney.edu.au
                Tiangang Cui
                tiangang.cui@sydney.edu.au
                School of Mathematics and Statistics
                University of Sydney
                New South Wales 2006, Australia[0m

Box rectangle:  [32m(90.0, 726.4) -> (232.7, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yiran Zhao and Tiangang Cui.[0m

Box rectangle:  [32m(90.0, 741.2) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0743.html.[0m



=== Processing ../JMLR 2024/The good  the bad and the ugly sides of data augmentation  An implicit spectral regularization perspective.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/The good  the bad and the ugly sides of data augmentation  An implicit spectral regularization perspective.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-85
                Submitted 11/22; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(98.9, 101.5) -> (515.4, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe good, the bad and the ugly sides of data augmentation:
                An implicit spectral regularization perspective[0m

Box rectangle:  [32m(88.8, 151.8) -> (522.0, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChi-Heng Lin
                cl3385@gatech.edu
                School of Electrical & Computer Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(88.8, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChiraag Kaushik
                ckaushik7@gatech.edu
                School of Electrical & Computer Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(88.8, 258.6) -> (522.0, 318.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEva L. Dyer∗
                evadyer@gatech.edu
                Coulter Department of Biomedical Engineering
                School of Electrical & Computer Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(88.8, 325.8) -> (522.0, 392.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVidya Muthukumar∗
                vmuthukumar8@gatech.edu
                School of Electrical & Computer Engineering
                School of Industrial & Systems Engineering
                Georgia Institute of Technology
                Atlanta, GA 30332, USA[0m

Box rectangle:  [32m(90.0, 417.6) -> (215.9, 427.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Pradeep Ravikumar[0m

Box rectangle:  [32m(280.3, 453.2) -> (331.7, 465.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 472.7) -> (503.7, 674.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mData augmentation (DA) is a powerful workhorse for bolstering performance in modern
                machine learning. Specific augmentations like translations and scaling in computer vision
                are traditionally believed to improve generalization by generating new (artificial) data from
                the same distribution. However, this traditional viewpoint does not explain the success of
                prevalent augmentations in modern machine learning (e.g. randomized masking, cutout,
                mixup), that greatly alter the training data distribution. In this work, we develop a new
                theoretical framework to characterize the impact of a general class of DA on underparam-
                eterized and overparameterized linear model generalization. Our framework reveals that
                DA induces implicit spectral regularization through a combination of two distinct effects:
                a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a
                training-data-dependent manner, and b) uniformly boosting the entire spectrum of the
                data covariance matrix through ridge regression. These effects, when applied to popular
                augmentations, give rise to a wide variety of phenomena, including discrepancies in gen-
                eralization between over-parameterized and under-parameterized regimes and differences
                between regression and classification tasks. Our framework highlights the nuanced and
                sometimes surprising impacts of DA on generalization, and serves as a testbed for novel
                augmentation design.[0m

Box rectangle:  [32m(93.7, 695.9) -> (267.8, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m*. Both senior authors contributed equally.[0m

Box rectangle:  [32m(89.1, 726.4) -> (367.0, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Chi-Heng Lin, Chiraag Kaushik, Eva L. Dyer, Vidya Muthukumar.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1312.html.[0m



=== Processing ../JMLR 2024/The Loss Landscape of Deep Linear Neural Networks  a Second-order Analysis.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/The Loss Landscape of Deep Linear Neural Networks  a Second-order Analysis.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-76
                Submitted 4/23; Revised 4/24; Published 8/24[0m

Box rectangle:  [32m(90.4, 101.7) -> (521.9, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe Loss Landscape of Deep Linear Neural Networks: a Second-order
                Analysis[0m

Box rectangle:  [32m(89.4, 153.4) -> (521.8, 199.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEl Mehdi Achour
                ACHOUR@MATHC.RWTH-AACHEN.DE
                Department of Mathematics
                RWTH Aachen University
                Aachen, Germany[0m

Box rectangle:  [32m(89.6, 207.0) -> (521.8, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFrançois Malgouyres
                FRANCOIS.MALGOUYRES@MATH.UNIV-TOULOUSE.FR
                Institut de Mathématiques de Toulouse ; UMR 5219
                Université de Toulouse ; CNRS
                UPS IMT F-31062 Toulouse Cedex 9, France[0m

Box rectangle:  [32m(89.8, 274.1) -> (521.8, 298.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSébastien Gerchinovitz
                SEBASTIEN.GERCHINOVITZ@IRT-SAINTEXUPERY.FR
                Institut de Recherche Technologique Saint Exupéry, Toulouse, France[0m

Box rectangle:  [32m(90.0, 323.5) -> (174.1, 333.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Martin Jaggi[0m

Box rectangle:  [32m(283.8, 359.3) -> (328.2, 371.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.5, 382.7) -> (503.8, 500.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe study the optimization landscape of deep linear neural networks with square loss. It is known
                that, under weak assumptions, there are no spurious local minima and no local maxima. However,
                the existence and diversity of non-strict saddle points, which can play a role in first-order algorithms’
                dynamics, have only been lightly studied. We go a step further with a complete analysis of the
                optimization landscape at order 2. Among all critical points, we characterize global minimizers,
                strict saddle points, and non-strict saddle points. We enumerate all the associated critical values.
                The characterization is simple, involves conditions on the ranks of partial matrix products, and sheds
                some light on global convergence or implicit regularization that has been proved or observed when
                optimizing linear neural networks. In passing, we provide an explicit parameterization of the set of
                all global minimizers and exhibit large sets of strict and non-strict saddle points.[0m

Box rectangle:  [32m(109.9, 509.9) -> (503.3, 531.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords: Deep learning, landscape analysis, non-convex optimization, second-order geometry,
                strict saddle points, non-strict saddle points, global minimizers, implicit regularization[0m

Box rectangle:  [32m(90.0, 558.1) -> (168.0, 570.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 586.1) -> (523.4, 705.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDeep learning has been widely used recently due to its good empirical performances in image
                recognition, natural language processing, and speech recognition, among other fields. However,
                there is still a gap between theory and practice. One of the aspects that are partially missing in
                the picture is why gradient-based algorithms can achieve low training error despite a non-convex
                objective. Another partially open question is why they generalize well to unseen data despite many
                more parameters than the number of points in the training set, and how implicit regularization can
                help. One important research direction analyses the landscape of the empirical risk. In this paper, we
                characterize the local structures around critical points of the empirical risk, for deep linear neural
                networks with the square loss.[0m

Box rectangle:  [32m(89.4, 726.7) -> (333.9, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 El Mehdi Achour, François Malgouyres, and Sébastien Gerchinovitz.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0493.html.[0m



=== Processing ../JMLR 2024/The Non-Overlapping Statistical Approximation to Overlapping Group Lasso.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/The Non-Overlapping Statistical Approximation to Overlapping Group Lasso.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-70
                Submitted 9/22; Revised 2/24; Published 4/24[0m

Box rectangle:  [32m(154.5, 101.7) -> (457.5, 131.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe Non-Overlapping Statistical Approximation to
                Overlapping Group Lasso[0m

Box rectangle:  [32m(90.0, 151.1) -> (521.8, 197.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMingyu Qi
                MQ3SQ@VIRGINIA.EDU
                Department of Statistics
                University of Virginia
                Charlottesville, VA 22904, USA[0m

Box rectangle:  [32m(90.0, 206.3) -> (521.8, 257.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTianxi Li
                TIANXILI@UMN.EDU
                School of Statistics
                University of Minnesota, Twin Cities
                Minneapolis, MN 55455, USA[0m

Box rectangle:  [32m(90.0, 738.6) -> (187.2, 746.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mingyu Qi, Tianxi Li.[0m

Box rectangle:  [32m(90.0, 752.9) -> (515.4, 770.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/22-1105.html.[0m



=== Processing ../JMLR 2024/The Nyström method for convex loss functions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/The Nyström method for convex loss functions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-60
                Submitted 6/23; Revised 11/24; Published 11/24[0m

Box rectangle:  [32m(140.0, 101.6) -> (472.2, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe Nystr ̈om method for convex loss functions[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 158.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAndrea Della Vecchia
                andrea.dellavecchia@epfl.ch
                EPFL and Swiss Finance Institute, Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 163.6) -> (522.0, 187.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErnesto De Vito
                ernesto.devito@unige.it
                MaLGa Center - DIMA - Universit`a di Genova, Italy[0m

Box rectangle:  [32m(90.0, 193.3) -> (522.0, 217.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJaouad Mourtada
                jaouad.mourtada@ensae.fr
                CREST, ENSAE - Institut Polytechnique de Paris, France[0m

Box rectangle:  [32m(90.0, 224.5) -> (522.0, 277.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLorenzo Rosasco
                lorenzo.rosasco@unige.it
                MaLGa Center - DIBRIS - Universit`a di Genova, Italy
                CBMM - Massachusets Institute of Technology, Cambridge, MA, USA
                Istituto Italiano di Tecnologia, Genoa, Italy[0m

Box rectangle:  [32m(90.0, 726.3) -> (403.6, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Andrea Della Vecchia, Ernesto De Vito, Jaouad Mourtada, Lorenzo Rosasco.[0m

Box rectangle:  [32m(90.0, 741.0) -> (512.2, 758.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-0768.html.[0m



=== Processing ../JMLR 2024/Three-Way Trade-Off in Multi-Objective Learning  Optimization  Generalization and Conflict-Avoidance.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Three-Way Trade-Off in Multi-Objective Learning  Optimization  Generalization and Conflict-Avoidance.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-53
                Submitted 10/23; Revised 3/24; Published 5/24[0m

Box rectangle:  [32m(122.8, 113.5) -> (489.3, 145.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThree-Way Trade-Off in Multi-Objective Learning:
                Optimization, Generalization and Conflict-Avoidance[0m

Box rectangle:  [32m(89.4, 164.7) -> (522.0, 201.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLisha Chen†
                chenl21@rpi.edu
                Department of Electrical, Computer & Systems Engineering
                Rensselaer Polytechnic Institute, United States[0m

Box rectangle:  [32m(89.4, 206.4) -> (522.0, 242.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHeshan Fernando†
                fernah@rpi.edu
                Department of Electrical, Computer & Systems Engineering
                Rensselaer Polytechnic Institute, United States[0m

Box rectangle:  [32m(88.3, 248.3) -> (522.0, 284.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYiming Ying
                yiming.ying@sydney.edu.au
                School of Mathematics and Statistics
                University of Sydney, NSW, Australia[0m

Box rectangle:  [32m(89.4, 291.5) -> (522.0, 330.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTianyi Chen
                chentianyi19@gmail.com
                Department of Electrical, Computer & Systems Engineering
                Rensselaer Polytechnic Institute, United States[0m

Box rectangle:  [32m(90.0, 356.0) -> (212.5, 365.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Francesco Orabona[0m

Box rectangle:  [32m(280.3, 389.6) -> (331.7, 401.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 412.6) -> (503.7, 614.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMulti-objective learning (MOL) often arises in machine learning problems when there
                are multiple data modalities or tasks. One critical challenge in MOL is the potential
                conflict among different objectives during the optimization process. Recent works have
                developed various dynamic weighting algorithms for MOL, where the central idea is to find
                an update direction that avoids conflicts among objectives. Albeit its appealing intuition,
                empirical studies show that dynamic weighting methods may not outperform static ones. To
                understand this theory-practice gap, we focus on a stochastic variant of MGDA, the Multi-
                objective gradient with Double sampling (MoDo), and study the generalization performance
                and its interplay with optimization through the lens of algorithmic stability in the framework
                of statistical learning theory. We find that the key rationale behind MGDA—updating along
                conflict-avoidant direction—may hinder dynamic weighting algorithms from achieving the
                optimal O(1/√n) population risk, where n is the number of training samples. We further
                demonstrate the impact of dynamic weights on the three-way trade-off among optimization,
                generalization, and conflict avoidance unique in MOL. We showcase the generality of our
                theoretical framework by analyzing other algorithms under the framework. Experiments
                on various multi-task learning benchmarks are performed to demonstrate the practical
                applicability. Code is available at https://github.com/heshandevaka/Trade-Off-MOL.[0m

Box rectangle:  [32m(109.9, 629.0) -> (503.5, 651.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                multi-objective optimization, statistical learning theory, algorithm stability,
                Pareto stationarity, gradient conflict[0m

Box rectangle:  [32m(98.3, 681.1) -> (522.3, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m.Symbol † denotes equal contribution. Preliminary results in this paper were presented in part at the 2023
                Advances in Neural Information Processing Systems.[0m

Box rectangle:  [32m(89.1, 726.4) -> (336.0, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Lisha Chen, Heshan Fernando, Yiming Ying, Tianyi Chen.[0m

Box rectangle:  [32m(90.0, 740.8) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1287.html.[0m



=== Processing ../JMLR 2024/Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-78
                Submitted 5/23; Revised 12/23; Published 3/24[0m

Box rectangle:  [32m(104.6, 101.6) -> (507.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTight Convergence Rate Bounds for Optimization Under
                Power Law Spectral Conditions[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMaksim Velikanov
                maksim.velikanov@tii.ae
                Technology Innovation Institute, Abu Dhabi, UAE;
                CMAP, Ecole Polytechnique, Paris, France[0m

Box rectangle:  [32m(90.0, 195.1) -> (522.0, 247.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDmitry Yarotsky
                d.yarotsky@skoltech.ru
                Center for Artificial Intelligence Technology
                Skolkovo Institute of Science and Technology
                Moscow, Russia[0m

Box rectangle:  [32m(90.0, 726.3) -> (269.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Maksim Velikanov and Dmitry Yarotsky.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0698.html.[0m



=== Processing ../JMLR 2024/Topological Analysis for Detecting Anomalies in dependent sequences  application to Time Series.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Topological Analysis for Detecting Anomalies in dependent sequences  application to Time Series.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 6/24; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(103.6, 102.4) -> (508.6, 134.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTopological Analysis for Detecting Anomalies (TADA) in
                dependent sequences: application to Time Series.[0m

Box rectangle:  [32m(89.2, 152.7) -> (522.0, 188.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFr ́ed ́eric Chazal
                frederic.chazal@inria.fr
                Inria Saclay
                91120, Palaiseau, France[0m

Box rectangle:  [32m(88.3, 194.3) -> (522.0, 230.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCl ́ement Levrard
                clement.levrard@univ-rennes1.fr
                Universit ́e de Rennes
                35000, Rennes, France[0m

Box rectangle:  [32m(89.2, 237.5) -> (522.0, 276.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMartin Royer
                martin.royer@inria.fr
                Inria Saclay, IRT SystemX
                91120, Palaiseau, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (301.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Fr ́ed ́eric Chazal, Cl ́ement Levrard, Martin Royer.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0853.html.[0m



=== Processing ../JMLR 2024/Topological Node2vec  Enhanced Graph Embedding via Persistent Homology.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Topological Node2vec  Enhanced Graph Embedding via Persistent Homology.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-26
                Submitted 9/23; Revised 2/24; Published 4/24[0m

Box rectangle:  [32m(108.0, 101.6) -> (504.1, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTopological Node2vec: Enhanced Graph Embedding via
                Persistent Homology[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYasuaki Hiraoka
                hiraoka.yasuaki.6z@kyoto-u.ac.jp
                Institute for the Advanced Study of Human Biology
                Kyoto University Institute for Advanced Study
                Kyoto University, Kyoto 606-8501, Japan[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYusuke Imoto
                imoto.yusuke.4e@kyoto-u.ac.jp
                Institute for the Advanced Study of Human Biology
                Kyoto University Institute for Advanced Study
                Kyoto University, Kyoto 606-8501, Japan[0m

Box rectangle:  [32m(90.0, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTh ́eo Lacombe
                theo.lacombe@univ-eiffel.fr
                Laboratoire d’Informatique Gaspard Monge,
                Univ. Gustave Eiffel, CNRS, LIGM, F-77454
                Marne-la-Vall ́ee, France[0m

Box rectangle:  [32m(90.0, 312.6) -> (522.0, 360.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKillian Meehan
                meehan.killianfrancis.8m@kyoto-u.ac.jp
                Institute for the Advanced Study of Human Biology
                Kyoto University Institute for Advanced Study
                Kyoto University, Kyoto 606-8501, Japan[0m

Box rectangle:  [32m(90.0, 367.8) -> (522.0, 407.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mToshiaki Yachimura
                toshiaki.yachimura.a4@tohoku.ac.jp
                Mathematical Science Center for Co-creative Society,
                Tohoku University, Sendai 980-0845, Japan[0m

Box rectangle:  [32m(90.0, 726.3) -> (437.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yasuaki Hiraoka, Yusuke Imoto, Th ́eo Lacombe, Killian Meehan, Toshiaki Yachimura.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1185.html.[0m



=== Processing ../JMLR 2024/TopoX  A Suite of Python Packages for Machine Learning on Topological Domains.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/TopoX  A Suite of Python Packages for Machine Learning on Topological Domains.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-8
                Submitted 1/24; Revised 11/24; Published 11/24[0m

Box rectangle:  [32m(93.4, 101.5) -> (518.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTopoX: A Suite of Python Packages for Machine Learning on
                Topological Domains[0m

Box rectangle:  [32m(90.0, 145.6) -> (522.0, 554.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMustafa Hajij∗
                mhajij@usfca.edu
                Mathilde Papillon∗
                papillon@ucsb.edu
                Florian Frantzen∗
                florian.frantzen@cs.rwth-aachen.de
                Jens Agerberg
                jensag@kth.se
                Ibrahem AlJabea
                ialjab2@lsu.edu
                Rubén Ballester
                ruben.ballester@ub.edu
                Claudio Battiloro
                cbattiloro@hsph.harvard.edu
                Guillermo Bernárdez
                guillermo.bernardez@upc.edu
                Tolga Birdal
                tbirdal@imperial.ac.uk
                Aiden Brent
                aidenjb81@gmail.com
                Peter Chin
                peter.chin@dartmouth.edu
                Sergio Escalera
                sescalera@ub.edu
                Simone Fiorellino
                simone.fiorellino@uniroma1.it
                Odin HoffGardaa
                odin.garda@uib.no
                Gurusankar Gopalakrishnan
                ggopalakrishnan@dons.usfca.edu
                Devendra Govil
                dgovil@dons.usfca.edu
                Josef Hoppe
                hoppe@cs.rwth-aachen.de
                Maneel Reddy Karri
                mkarri@dons.usfca.edu
                Jude Khouja
                Jude@latynt.com
                Manuel Lecha
                manuellecha@ub.edu
                Neal Livesay
                nlivesay@ptc.com
                Jan Meißner
                philipp.meissner@rwth-aachen.de
                Soham Mukherjee
                mukher26@purdue.edu
                Alexander Nikitin
                alexander.nikitin@aalto.fi
                Theodore Papamarkou
                theo@zjnu.edu.cn
                Jaro Prílepok
                jaroslav.prilepok@student.manchester.ac.uk
                Karthikeyan Natesan Ramamurthy
                knatesa@us.ibm.com
                Paul Rosen
                prosen@sci.utah.edu
                Aldo Guzmán-Sáenz
                aldo.guzman.saenz@ibm.com
                Alessandro Salatiello
                salatiello.alessandro@gmail.com
                Shreyas N. Samaga
                ssamaga@purdue.edu
                Simone Scardapane
                simone.scardapane@uniroma1.it
                Michael T. Schaub
                schaub@cs.rwth-aachen.de
                Luca Scofano
                luca.scofano@uniroma1.it
                Indro Spinelli
                indro.spinelli@uniroma1.it
                Lev Telyatnikov
                lev.telyatnikov@uniroma1.it
                Quang Truong
                cong.minh.quang.truong.th@dartmouth.edu
                Robin Walters
                r.walters@northeastern.edu
                Maosheng Yang
                m.yang-2@tudelft.nl
                Olga Zaghen
                o.zaghen@uva.nl
                Ghada Zamzmi
                ghadh@mail.usf.edu
                Ali Zia
                ali.zia@anu.edu.au
                Nina Miolane
                ninamiolane@ucsb.edu[0m

Box rectangle:  [32m(90.0, 581.9) -> (216.1, 591.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Joaquin Vanschoren[0m

Box rectangle:  [32m(280.3, 610.3) -> (331.7, 622.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 627.1) -> (502.1, 685.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe introduce TopoX, a Python software suite that provides reliable and user-friendly build-
                ing blocks for computing and machine learning on topological domains that extend graphs:
                hypergraphs, simplicial, cellular, path and combinatorial complexes. TopoX consists of three
                packages: TopoNetX facilitates constructing and computing on these domains, including
                working with nodes, edges and higher-order cells; TopoEmbedX provides methods to embed[0m

Box rectangle:  [32m(93.7, 695.9) -> (247.0, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. * Equal contribution as first author[0m

Box rectangle:  [32m(90.0, 726.4) -> (517.6, 768.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg, Ibrahem AlJabea, Rubén Ballester, Claudio Battiloro,
                Guillermo Bernárdez, Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Simone Fiorellino, Odin HoffGardaa, Gurusankar
                Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan Meißner, Soham
                Mukherjee, Alexander Nikitin, Theodore Papamarkou, Jaro Prílepok, Karthikeyan Natesan Ramamurthy, Paul Rosen, Aldo
                Guzmán-Sáenz, Alessandro Salatiello, Shreyas N. Samaga, Simone Scardapane, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev
                Telyatnikov, Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali Zia, Nina Miolane.[0m

Box rectangle:  [32m(90.0, 774.9) -> (447.3, 788.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/24-0110.html.[0m



=== Processing ../JMLR 2024/Towards Explainable Evaluation Metrics for Machine Translation.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Towards Explainable Evaluation Metrics for Machine Translation.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-49
                Submitted 4/22; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(118.1, 101.6) -> (494.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTowards Explainable Evaluation Metrics for Machine
                Translation[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristoph Leiter
                christoph.leiter@uni-mannheim.de
                Natural Language Learning Group
                University of Mannheim
                B6 26, 68159 Mannheim, Germany[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPiyawat Lertvittayakumjorn
                pl1515@imperial.ac.uk
                Imperial College London[0m

Box rectangle:  [32m(90.0, 235.1) -> (522.0, 259.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMarina Fomicheva
                m.fomicheva@sheffield.ac.uk
                University of Sheffield[0m

Box rectangle:  [32m(90.0, 264.8) -> (522.0, 300.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWei Zhao
                wei.zhao@abdn.ac.uk
                University of Aberdeen
                Heidelberg Institute for Theoretical Studies[0m

Box rectangle:  [32m(90.0, 306.4) -> (522.0, 330.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYang Gao
                gaostayyang@google.com
                Royal Holloway, University of London[0m

Box rectangle:  [32m(90.0, 337.7) -> (522.0, 363.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSteffen Eger
                steffen.eger@uni-mannheim.de
                University of Mannheim[0m

Box rectangle:  [32m(90.0, 388.5) -> (174.9, 398.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ivan Titov[0m

Box rectangle:  [32m(280.3, 422.1) -> (331.7, 434.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 441.5) -> (502.2, 606.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnlike classical lexical overlap metrics such as BLEU, most current evaluation metrics
                for machine translation (for example, COMET or BERTScore) are based on black-box
                large language models. They often achieve strong correlations with human judgments, but
                recent research indicates that the lower-quality classical metrics remain dominant, one of
                the potential reasons being that their decision processes are more transparent. To foster
                more widespread acceptance of novel high-quality metrics, explainability thus becomes
                crucial. In this concept paper, we identify key properties as well as key goals of explainable
                machine translation metrics and provide a comprehensive synthesis of recent techniques,
                relating them to our established goals and properties. In this context, we also discuss the
                latest state-of-the-art approaches to explainable metrics based on generative models such
                as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches,
                including natural language explanations. We hope that our work can help catalyze and
                guide future research on explainable evaluation metrics and, mediately, also contribute to
                better and more transparent machine translation systems.[0m

Box rectangle:  [32m(109.9, 612.5) -> (502.1, 634.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKeywords:
                evaluation metrics, explainability, interpretability, machine translation, ma-
                chine translation evaluation[0m

Box rectangle:  [32m(90.0, 656.7) -> (176.6, 668.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 680.8) -> (522.0, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe field of evaluation metrics for Natural Language Generation (NLG), especially machine
                translation (MT) is in a crisis (Marie et al., 2021). Despite the development of multiple[0m

Box rectangle:  [32m(90.0, 726.4) -> (513.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao and Steffen Eger.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0416.html.[0m



=== Processing ../JMLR 2024/Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 12/23; Revised 5/24; Published 6/24[0m

Box rectangle:  [32m(90.2, 101.6) -> (522.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTowards Optimal Sobolev Norm Rates for the Vector-Valued
                Regularized Least-Squares Algorithm[0m

Box rectangle:  [32m(88.3, 147.3) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhu Li∗
                zhu.li@ucl.ac.uk
                Gatsby Computational Neuroscience Unit
                University College London
                London, W1T 4JG, UK[0m

Box rectangle:  [32m(88.3, 200.8) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDimitri Meunier∗
                dimitri.meunier.21@ucl.ac.uk
                Gatsby Computational Neuroscience Unit
                University College London
                London, W1T 4JG, UK[0m

Box rectangle:  [32m(89.4, 259.0) -> (522.0, 307.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMattes Mollenhauer
                mattes.mollenhauer@merantix-momentum.com
                Merantix Momentum
                Merantix AI Campus
                Max–Urich–Straße 3, 13355 Berlin, Germany[0m

Box rectangle:  [32m(88.3, 314.2) -> (522.0, 367.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mArthur Gretton
                arthur.gretton@gmail.com
                Gatsby Computational Neuroscience Unit
                University College London
                London, W1T 4JG, UK[0m

Box rectangle:  [32m(90.0, 392.2) -> (177.3, 402.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Daniel Hsu[0m

Box rectangle:  [32m(280.3, 425.8) -> (331.7, 437.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 443.9) -> (502.4, 611.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe present the first optimal rates for infinite-dimensional vector-valued ridge regression on
                a continuous scale of norms that interpolate between L2 and the hypothesis space, which
                we consider as a vector-valued reproducing kernel Hilbert space. These rates allow to
                treat the misspecified case in which the true regression function is not contained in the
                hypothesis space. We combine standard assumptions on the capacity of the hypothesis space
                with a novel tensor product construction of vector-valued interpolation spaces in order to
                characterize the smoothness of the regression function. Our upper bound not only attains
                the same rate as real-valued kernel ridge regression, but also removes the assumption that
                the target regression function is bounded. For the lower bound, we reduce the problem to
                the scalar setting using a projection argument. We show that these rates are optimal in
                most cases and independent of the dimension of the output space. We illustrate our results
                for the special case of vector-valued Sobolev spaces.
                Keywords:
                Statistical learning, regularized least squares, optimal rates, interpolation
                norms.[0m

Box rectangle:  [32m(90.0, 632.6) -> (180.3, 644.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 655.3) -> (522.0, 679.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOptimal learning rates for least-squares regression with scalar outputs have been studied
                extensively in the context of reproducing kernel Hilbert spaces (RKHS) over the last two[0m

Box rectangle:  [32m(90.0, 691.1) -> (173.1, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗*Equal Contribution[0m

Box rectangle:  [32m(89.1, 726.4) -> (353.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Zhu Li; Dimitri Meunier; Mattes Mollenhauer; Arthur Gretton.[0m

Box rectangle:  [32m(90.0, 741.0) -> (522.2, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1663.html.[0m



=== Processing ../JMLR 2024/Towards Unbiased Exploration in Partial Label Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Towards Unbiased Exploration in Partial Label Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-56
                Submitted 7/23; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(105.6, 101.6) -> (506.6, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTowards Unbiased Exploration in Partial Label Learning[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZsolt Zombori
                zombori@renyi.hu
                Alfr ́ed R ́enyi Institute of Mathematics
                E ̈otv ̈os Lor ́and University
                Budapest, Hungary[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAgapi Rissaki
                rissaki.agapi@gmail.com
                Khoury College of Computer Sciences
                Northeastern University
                Boston, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 277.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKrist ́of Szab ́o
                krist.sz13@gmail.com
                Alfr ́ed R ́enyi Institute of Mathematics
                Budapest, Hungary[0m

Box rectangle:  [32m(90.0, 282.7) -> (522.0, 330.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWolfgang Gatterbauer
                wgatterbauer@northeastern.edu
                Khoury College of Computer Sciences
                Northeastern University
                Boston, USA[0m

Box rectangle:  [32m(90.0, 337.9) -> (522.0, 390.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMichael Benedikt
                michael.benedikt@cs.ox.ac.uk
                Department of Computer Science
                University of Oxford
                Oxford, UK[0m

Box rectangle:  [32m(90.0, 726.3) -> (441.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Zsolt Zombori, Agapi Rissaki, Krist ́of Szab ́o, Wolfgang Gatterbauer, Michael Benedikt.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0868.html.[0m



=== Processing ../JMLR 2024/Trained Transformers Learn Linear Models In-Context.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Trained Transformers Learn Linear Models In-Context.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 8/23; Published 1/24[0m

Box rectangle:  [32m(112.3, 101.6) -> (499.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTrained Transformers Learn Linear Models In-Context[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRuiqi Zhang
                rqzhang@berkeley.edu
                Department of Statistics
                University of California, Berkeley
                367 Evans Hall, Berkeley, CA 94720-3860, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 247.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSpencer Frei
                sfrei@ucdavis.edu
                Department of Statistics
                University of California, Davis
                4118 Mathematical Sciences Building
                399 Crocker Ave., Davis, CA 95616, USA[0m

Box rectangle:  [32m(90.0, 254.6) -> (522.0, 348.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPeter L. Bartlett
                peter@berkeley.edu
                Department of Statistics and Department of Electrical Engineering and Computer Sciences
                University of California, Berkeley
                367 Evans Hall, Berkeley, CA 94720-3860, USA
                Google DeepMind
                1600 Amphitheatre Parkway
                Mountain View, CA 94040, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (299.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Ruiqi Zhang, Spencer Frei and Peter L. Bartlett.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1042.html.[0m



=== Processing ../JMLR 2024/Training Integrable Parameterizations of Deep Neural Networks in the Infinite-Width Limit.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Training Integrable Parameterizations of Deep Neural Networks in the Infinite-Width Limit.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-130
                Submitted 10/21; Revised 5/24; Published 7/24[0m

Box rectangle:  [32m(114.9, 101.6) -> (497.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTraining Integrable Parameterizations of Deep Neural
                Networks in the Infinite-Width Limit[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKarl Hajjar
                hajjarkarl@gmail.com
                Laboratoire de Math ́ematiques d’Orsay
                Universit ́e Paris-Saclay
                91405 Orsay, France[0m

Box rectangle:  [32m(90.0, 217.4) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mL ́ena ̈ıc Chizat
                lenaic.chizat@epfl.ch
                Institut de Math ́ematiques
                 ́Ecole Polytechnique F ́ed ́erale de Lausanne
                Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 272.6) -> (522.0, 325.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristophe Giraud
                christophe.giraud@universite-paris-saclay.fr
                Laboratoire de Math ́ematiques d’Orsay
                Universit ́e Paris-Saclay
                91405 Orsay, France[0m

Box rectangle:  [32m(90.0, 364.1) -> (200.3, 374.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Benjamin Guedj[0m

Box rectangle:  [32m(280.3, 399.6) -> (331.7, 411.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 416.4) -> (502.1, 666.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTo theoretically understand the behavior of trained deep neural networks, it is necessary to
                study the dynamics induced by gradient methods from a random initialization. However,
                the nonlinear and compositional structure of these models make these dynamics difficult
                to analyze. To overcome these challenges, large-width asymptotics have recently emerged
                as a fruitful viewpoint and led to practical insights on real-world deep networks. For two-
                layer neural networks, it has been understood via these asymptotics that the nature of
                the trained model radically changes depending on the scale of the initial random weights,
                ranging from a kernel regime (for large initial variance) to a feature learning regime (for
                small initial variance). For deeper networks more regimes are possible, and in this paper
                we study in detail a specific choice of “small” initialization corresponding to “mean-field”
                limits of neural networks, which we call integrable parameterizations (IPs).
                First, we show that under standard i.i.d. zero-mean initialization, integrable parame-
                terizations of neural networks with more than four layers start at a stationary point in the
                infinite-width limit and no learning occurs. We then propose various methods to avoid this
                trivial behavior and analyze in detail the resulting dynamics. In particular, one of these
                methods consists in using large initial learning rates, and we show that it is equivalent to
                a modification of the recently proposed maximal update parameterization μP. We confirm
                our results with numerical experiments on image classification tasks, which additionally
                show a strong difference in behavior between various choices of activation functions that is
                not yet captured by theory.
                Keywords:
                Neural networks, infinite-width limit, gradient methods.[0m

Box rectangle:  [32m(90.0, 726.4) -> (312.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Karl Hajjar, L ́ena ̈ıc Chizat, and Christophe Giraud.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1260.html.[0m



=== Processing ../JMLR 2024/Transfer learning for tensor Gaussian graphical models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Transfer learning for tensor Gaussian graphical models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 11/22; Revised 2/24; Published 12/24[0m

Box rectangle:  [32m(112.0, 101.6) -> (500.1, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTransfer learning for tensor Gaussian graphical models[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mMingyang Ren
                mingyangren@sjtu.edu.cn
                School of Mathematical Sciences
                Shanghai Jiao Tong University
                Minhang, Shanghai, China[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYaoming Zhen
                yaoming.zhen@utoronto.ca
                Department of Statistical Sciences
                University of Toronto
                Toronto, Ontario, Canada[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJunhui Wang
                junhuiwang@cuhk.edu.hk
                Department of Statistics
                The Chinese University of Hong Kong
                Shatin, Hong Kong, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (304.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Mingyang Ren, Yaoming Zhen, and Junhui Wang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1313.html.[0m



=== Processing ../JMLR 2024/Transfer Learning with Uncertainty Quantification  Random Effect Calibration of Source to Target (RECaST).pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Transfer Learning with Uncertainty Quantification  Random Effect Calibration of Source to Target (RECaST).pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 11/22; Revised 9/24; Published 10/24[0m

Box rectangle:  [32m(91.8, 101.6) -> (520.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTransfer Learning with Uncertainty Quantification: Random
                Effect Calibration of Source to Target (RECaST)[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 187.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJimmy Hickey
                jhickey@ncsu.edu
                Department of Statistics
                North Carolina State University[0m

Box rectangle:  [32m(90.0, 193.5) -> (522.0, 229.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJonathan P. Williams
                jwilli27@ncsu.edu
                Department of Statistics, North Carolina State University
                Centre for Advanced Study, Norwegian Academy of Science and Letters[0m

Box rectangle:  [32m(90.0, 236.7) -> (522.0, 275.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEmily C. Hector
                ehector@ncsu.edu
                Department of Statistics
                North Carolina State University[0m

Box rectangle:  [32m(90.0, 726.3) -> (323.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Jimmy Hickey, Jonathan P. Williams, Emily C. Hector.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1369.html.[0m



=== Processing ../JMLR 2024/Transport-based Counterfactual Models.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Transport-based Counterfactual Models.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-59
                Submitted 12/21; Revised 2/24; Published 5/24[0m

Box rectangle:  [32m(165.2, 101.6) -> (446.9, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTransport-based Counterfactual Models[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLucas De Lara
                lucas.de lara@math.univ-toulouse.fr
                Institut de Math ́ematiques de Toulouse
                Universit ́e Paul Sabatier
                Toulouse, France[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlberto Gonz ́alez-Sanz
                ag4855@columbia.edu
                Department of Statistics
                Columbia University
                New York, United States[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNicholas Asher
                nicholas.asher@irit.fr
                Institut de Recherche en Informatique de Toulouse
                CNRS
                Toulouse, France[0m

Box rectangle:  [32m(90.0, 294.7) -> (522.0, 342.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLaurent Risser
                laurent.risser@math.univ-toulouse.fr
                Institut de Math ́ematiques de Toulouse
                CNRS
                Toulouse, France[0m

Box rectangle:  [32m(90.0, 349.8) -> (522.0, 402.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJean-Michel Loubes
                loubes@math.univ-toulouse.fr
                Institut de Math ́ematiques de Toulouse
                Universit ́e Paul Sabatier
                Toulouse, France[0m

Box rectangle:  [32m(90.0, 726.3) -> (463.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Lucas De Lara, Alberto Gonz ́alez-Sanz, Nicholas Asher, Laurent Risser, Jean-Michel Loubes.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1440.html.[0m



=== Processing ../JMLR 2024/Triple Component Matrix Factorization  Untangling Global  Local  and Noisy Components.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Triple Component Matrix Factorization  Untangling Global  Local  and Noisy Components.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-76
                Submitted 3/24; Revised 10/24; Published 11/24[0m

Box rectangle:  [32m(93.7, 101.6) -> (520.8, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTriple Component Matrix Factorization: Untangling Global,
                Local, and Noisy Components[0m

Box rectangle:  [32m(88.3, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNaichen Shi
                naichens@umich.edu
                Department of Industrial & Operations Engineering
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(88.3, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSalar Fattahi
                fattahi@umich.edu
                Department of Industrial & Operations Engineering
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(88.3, 260.3) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRaed Al Kontar ∗
                alkontar@umich.edu
                Department of Industrial & Operations Engineering
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 338.6) -> (222.8, 348.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Mahdi Soltanolkotabi[0m

Box rectangle:  [32m(280.3, 372.2) -> (331.7, 384.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.4, 389.5) -> (504.1, 616.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this work, we study the problem of common and unique feature extraction from noisy data.
                When we have N observation matrices from N different and associated sources corrupted
                by sparse and potentially gross noise, can we recover the common and unique components
                from these noisy observations? This is a challenging task as the number of parameters
                to estimate is approximately thrice the number of observations. Despite the difficulty, we
                propose an intuitive alternating minimization algorithm called triple component matrix
                factorization (TCMF) to recover the three components exactly. TCMF is distinguished from
                existing works in literature thanks to two salient features. First, TCMF is a principled method
                to separate the three components given noisy observations provably. Second, the bulk of the
                computation in TCMF can be distributed. On the technical side, we formulate the problem
                as a constrained nonconvex nonsmooth optimization problem. Despite the intricate nature
                of the problem, we provide a Taylor series characterization of its solution by solving the
                corresponding Karush–Kuhn–Tucker conditions. Using this characterization, we can show
                that the alternating minimization algorithm makes significant progress at each iteration
                and converges into the ground truth at a linear rate. Numerical experiments in video
                segmentation and anomaly detection highlight the superior feature extraction abilities of
                TCMF.
                Keywords:
                Matrix Factorization, Heterogeneity, Alternating minimization, Sparse noise,
                Outlier identification[0m

Box rectangle:  [32m(90.0, 636.5) -> (180.3, 648.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 658.6) -> (522.4, 683.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn the era of Big Data, an important task is to find low-rank features from high-dimensional
                observations. Methods including principal component analysis (Hotelling, 1933), low-rank[0m

Box rectangle:  [32m(93.7, 695.7) -> (193.4, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. Corresponding author[0m

Box rectangle:  [32m(89.1, 726.4) -> (283.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Naichen Shi, Salar Fattahi, Raed Al Kontar.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0400.html.[0m



=== Processing ../JMLR 2024/Two is Better Than One  Regularized Shrinkage of Large Minimum Variance Portfolios.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Two is Better Than One  Regularized Shrinkage of Large Minimum Variance Portfolios.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 11/22; Revised 6/24; Published 6/24[0m

Box rectangle:  [32m(102.9, 101.6) -> (509.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTwo is Better Than One: Regularized Shrinkage of Large
                Minimum Variance Portfolios[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTaras Bodnar
                taras.bodnar@liu.se
                Department of Management and Engineering
                Link ̈oping University
                SE-581 83 Link ̈oping Sweden[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNestor Parolya
                n.parolya@tudelft.nl
                Department of Applied Mathematics
                Delft University of Technology
                Mekelweg 4, 2628 CD Delft, The Netherlands[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mErik Thors ́en
                erik.thorsen@math.su.se
                Department of Mathematics
                Stockholm University
                Roslagsv ̈agen 101, SE-10691 Stockholm, Sweden[0m

Box rectangle:  [32m(90.0, 726.3) -> (299.8, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Taras Bodnar, Nestor Parolya and Erik Thors ́en.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-1337.html.[0m



=== Processing ../JMLR 2024/Uncertainty Quantification of MLE for Entity Ranking with Covariates.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Uncertainty Quantification of MLE for Entity Ranking with Covariates.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-83
                Submitted 4/23; Revised 3/24; Published 8/24[0m

Box rectangle:  [32m(93.8, 101.6) -> (518.3, 135.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUncertainty Quantification of MLE for Entity Ranking with
                Covariates[0m

Box rectangle:  [32m(90.0, 156.6) -> (522.0, 288.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJianqing Fan
                jqfan@princeton.edu
                Jikai Hou
                jikaih@princeton.edu
                Department of Operations Research and Financial Engineering
                Princeton University
                Princeton, NJ, United States
                Mengxin Yu
                mengxiny@wharton.upenn.edu
                Department of Statistics and Data Science, the Wharton School
                University of Pennsylvania
                Philadelphia, PA, United States[0m

Box rectangle:  [32m(90.0, 314.5) -> (157.1, 324.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Ji Zhu[0m

Box rectangle:  [32m(280.3, 349.5) -> (331.7, 361.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 368.0) -> (502.1, 484.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWe study statistical estimation and inference for the ranking problems based on pairwise
                comparisons with additional covariate information. In specific, in this paper, we study a
                Covariate-Assisted Ranking Estimation (CARE) model in a systematic way, that extends
                the well-known Bradley-Terry-Luce (BTL) model by incorporating the covariate informa-
                tion. We impose natural identifiability conditions, derive the statistical rates for the MLE
                under a sparse comparison graph, and obtain its asymptotic distribution. Moreover, we
                validate our theoretical results through large-scale numerical studies.
                Keywords:
                High-Dimensional Inference, Entity ranking, Ranking with covariates, Un-
                certainty quantification, Maximum likelihood estimator.[0m

Box rectangle:  [32m(90.0, 506.4) -> (176.6, 518.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1 Introduction[0m

Box rectangle:  [32m(90.0, 529.9) -> (522.1, 708.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRanking plays an essential role in many real-world applications. For example, it is crucial
                in individual choice (Luce, 2012), psychology (Thurstone, 1927, 2017), recommendation
                systems (Baltrunas et al., 2010; Li et al., 2019), and many others. The ranked items such as
                sports teams (Massey, 1997; Turner and Firth, 2012), scientific journals (Stigler, 1994), web
                pages (Dwork et al., 2001), election candidates (Plackett, 1975), or even movies (Harper
                and Konstan, 2015) will not only illustrate their qualities but also affect people’s future
                choices.
                Thus, the ranking problem has been extensively studied in statistics, machine
                learning, operations research, etc.; see, for example, (Hunter, 2004; Richardson et al., 2006;
                Jang et al., 2018; Chen et al., 2019, 2022b,a; Liu et al., 2022) for more details.
                Among various models for the ranking problem, the most well-known one is the Bradley-
                Terry-Luce (BTL) model (Bradley and Terry, 1952; Luce, 2012), which assumes the exis-
                tence of scores {θ∗
                i }n
                i=1 of n compared items such that the preference between item i and[0m

Box rectangle:  [32m(90.0, 726.4) -> (274.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Jianqing Fan, Jikai Hou and Mengxin Yu.[0m

Box rectangle:  [32m(90.0, 741.9) -> (517.0, 760.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0554.html.[0m



=== Processing ../JMLR 2024/Understanding Entropic Regularization in GANs.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Understanding Entropic Regularization in GANs.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-32
                Submitted 11/21; Revised 9/23; Published 7/24[0m

Box rectangle:  [32m(132.8, 101.6) -> (479.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnderstanding Entropic Regularization in GANs[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDaria Reshetova
                resh@stanford.edu
                Department of Electrical Engineering
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYikun Bai
                bai@udel.edu
                Department of Electrical and Computer Engineering
                University of Delaware
                Newark, DE 19716, USA[0m

Box rectangle:  [32m(90.0, 241.1) -> (522.0, 289.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiugang Wu
                xwu@udel.edu
                Department of Electrical and Computer Engineering
                University of Delaware
                Newark, DE 19716, USA[0m

Box rectangle:  [32m(90.0, 295.1) -> (522.0, 349.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAyfer  ̈Ozg ̈ur
                aozgur@stanford.edu
                Department of Electrical Engineering
                Stanford University
                Stanford, CA 94305, USA[0m

Box rectangle:  [32m(90.0, 724.5) -> (343.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Daria Reshetova, Yikun Bai, Xiugang Wu, and Ayfer  ̈Ozg ̈ur.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/21-1295.html.[0m



=== Processing ../JMLR 2024/Unified Binary and Multiclass Margin-Based Classification.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Unified Binary and Multiclass Margin-Based Classification.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-51
                Submitted 12/23; Revised 5/24; Published 5/24[0m

Box rectangle:  [32m(98.4, 101.6) -> (513.7, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnified Binary and Multiclass Margin-Based Classification[0m

Box rectangle:  [32m(90.0, 135.3) -> (522.0, 229.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYutong Wang1,2
                yutongw@umich.edu
                Clayton Scott1,3
                clayscot@umich.edu
                1Department of Electrical Engineering and Computer Science
                2Michigan Institute of Data Science
                3Department of Statistics
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (241.9, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yutong Wang and Clayton Scott.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1599.html.[0m



=== Processing ../JMLR 2024/Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via PAC-Bayesian Theory on Random Sets.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via PAC-Bayesian Theory on Random Sets.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 4/24; Revised 9/24; Published 12/24[0m

Box rectangle:  [32m(94.8, 101.6) -> (517.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUniform Generalization Bounds on Data-Dependent
                Hypothesis Sets via PAC-Bayesian Theory on Random Sets[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBenjamin Dupuis*
                benjamin.dupuis@inria.fr
                INRIA - D ́epartement d’Informatique de l’Ecole Normale Sup ́erieure
                PSL Research University
                Paris, France[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPaul Viallard
                paul.viallard@inria.fr
                Univ Rennes, Inria, CNRS IRISA - UMR 6074
                Rennes, France[0m

Box rectangle:  [32m(90.0, 247.1) -> (522.0, 283.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mGeorge Deligiannidis
                george.deligiannidis@stats.ox.ac.uk
                Department of Statistics
                University of Oxford, Oxford, UK[0m

Box rectangle:  [32m(90.0, 290.3) -> (522.0, 343.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUmut S ̧im ̧sekli
                umut.simsekli@inria.fr
                INRIA - D ́epartement d’Informatique de l’Ecole Normale Sup ́erieure
                PSL Research University - CNRS
                Paris, France[0m

Box rectangle:  [32m(90.0, 360.2) -> (196.4, 370.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m* Corresponding author.[0m

Box rectangle:  [32m(90.0, 726.3) -> (395.2, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Benjamin Dupuis, Paul Viallard, George Deligiannidis and Umut S ̧im ̧sekli.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0605.html.[0m



=== Processing ../JMLR 2024/Unlabeled Principal Component Analysis and Matrix Completion.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Unlabeled Principal Component Analysis and Matrix Completion.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-38
                Submitted 7/22; Revised 1/24; Published 2/24[0m

Box rectangle:  [32m(159.8, 101.6) -> (452.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnlabeled Principal Component Analysis
                and Matrix Completion[0m

Box rectangle:  [32m(88.8, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunzhen Yao
                yunzhen.yao@epfl.ch
                School of Computer and Communication Sciences
                EPFL
                CH-1015 Lausanne, Switzerland[0m

Box rectangle:  [32m(90.0, 205.4) -> (518.2, 217.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLiangzu Peng
                lpenn@seas.upenn.edu[0m

Box rectangle:  [32m(88.8, 224.8) -> (522.0, 291.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mManolis C. Tsakiris
                manolis@amss.ac.cn
                Key Laboratory for Mathematics Mechanization
                Academy of Mathematics and Systems Science
                Chinese Academy of Sciences
                Beijing, 100190, China[0m

Box rectangle:  [32m(90.0, 726.3) -> (302.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yunzhen Yao, Liangzu Peng, Manolis C. Tsakiris.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0816.html.[0m



=== Processing ../JMLR 2024/Unsupervised Anomaly Detection Algorithms on Real-world Data  How Many Do We Need.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Unsupervised Anomaly Detection Algorithms on Real-world Data  How Many Do We Need.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-34
                Submitted 5/23; Revised 1/24; Published 3/24[0m

Box rectangle:  [32m(92.5, 101.6) -> (519.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnsupervised Anomaly Detection Algorithms on Real-world
                Data: How Many Do We Need?[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRoel Bouman
                roel.bouman@ru.nl
                Institute for Computing and Information Sciences
                Radboud University
                Toernooiveld 212, 6525 EC Nijmegen, The Netherlands[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZaharah Bukhsh
                z.bukhsh@tue.nl
                Information Systems, Industrial Engineering and Innovation Sciences
                Eindhoven University of Technology
                Groene Loper 3, 5612 AE Eindhoven, The Netherlands[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTom Heskes
                tom.heskes@ru.nl
                Institute for Computing and Information Sciences
                Radboud University
                Toernooiveld 212, 6525 EC Nijmegen, The Netherlands[0m

Box rectangle:  [32m(90.0, 338.6) -> (204.0, 348.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Marc Schoenauer[0m

Box rectangle:  [32m(280.3, 372.2) -> (331.7, 384.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 390.4) -> (502.1, 582.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this study we evaluate 33 unsupervised anomaly detection algorithms on 52 real-world
                multivariate tabular data sets, performing the largest comparison of unsupervised anomaly
                detection algorithms to date. On this collection of data sets, the EIF (Extended Isolation
                Forest) algorithm significantly outperforms the most other algorithms.
                Visualizing and
                then clustering the relative performance of the considered algorithms on all data sets, we
                identify two clear clusters: one with “local” data sets, and another with “global” data sets.
                “Local” anomalies occupy a region with low density when compared to nearby samples,
                while “global” occupy an overall low density region in the feature space. On the local data
                sets the kNN (k-nearest neighbor) algorithm comes out on top. On the global data sets,
                the EIF (extended isolation forest) algorithm performs the best. Also taking into consid-
                eration the algorithms’ computational complexity, a toolbox with these two unsupervised
                anomaly detection algorithms suffices for finding anomalies in this representative collection
                of multivariate data sets. By providing access to code and data sets, our study can be
                easily reproduced and extended with more algorithms and/or data sets.
                Keywords:
                Unsupervised Anomaly Detection, Anomaly Analysis, Algorithm Compari-
                son, Outlier Detection, Outlier Analysis[0m

Box rectangle:  [32m(90.0, 603.6) -> (180.3, 615.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 626.6) -> (522.1, 705.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAnomaly detection is the study of finding data points that do not fit the expected struc-
                ture of the data. Anomalies can be caused by unexpected processes generating the data. In
                chemistry an anomaly might be caused by an incorrectly performed experiment, in medicine
                a certain disease might induce rare symptoms, and in predictive maintenance an anomaly
                can be indicative of early system failure. Depending on the application domain, anomalies
                have different properties, and may also be called by different names. Within the domain[0m

Box rectangle:  [32m(90.0, 726.4) -> (304.5, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Roel Bouman, Zaharah Bukhsh, and Tom Heskes.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0570.html.[0m



=== Processing ../JMLR 2024/Unsupervised Tree Boosting for Learning Probability Distributions.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Unsupervised Tree Boosting for Learning Probability Distributions.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-52
                Submitted 8/22; Revised 10/23; Published 7/24[0m

Box rectangle:  [32m(117.5, 101.6) -> (494.6, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mUnsupervised Tree Boosting for Learning Probability
                Distributions[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNaoki Awaya
                nawaya@waseda.jp
                School of Political Science and Economics
                Waseda University
                Shinjuku City, Tokyo 169-8050, Japan[0m

Box rectangle:  [32m(90.0, 207.0) -> (522.0, 259.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLi Ma
                li.ma@duke.edu
                Department of Statistical Science
                Duke University
                Durham, NC 27708, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (211.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Naoki Awaya and Li Ma.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/22-0980.html.[0m



=== Processing ../JMLR 2024/Value-Distributional Model-Based Reinforcement Learning.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Value-Distributional Model-Based Reinforcement Learning.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 7/23; Revised 5/24; Published 9/24[0m

Box rectangle:  [32m(102.3, 101.5) -> (509.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mValue-Distributional Model-Based Reinforcement Learning[0m

Box rectangle:  [32m(89.4, 133.9) -> (522.0, 169.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mCarlos E. Luis
                carlos@robot-learning.de
                Bosch Corporate Research, TU Darmstadt
                Robert-Bosch-Campus 1, 71272 Renningen (Germany)[0m

Box rectangle:  [32m(89.4, 175.5) -> (522.0, 199.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAlessandro G. Bottero
                alessandrogiacomo.bottero@bosch.com
                Bosch Corporate Research, TU Darmstadt[0m

Box rectangle:  [32m(89.4, 205.2) -> (522.0, 229.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJulia Vinogradska
                julia.vinogradska@bosch.com
                Bosch Corporate Research[0m

Box rectangle:  [32m(89.4, 234.8) -> (522.0, 258.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mFelix Berkenkamp
                felix.berkenkamp@bosch.com
                Bosch Corporate Research[0m

Box rectangle:  [32m(88.4, 266.1) -> (522.0, 291.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJan Peters
                jan.peters@tu-darmstadt.de
                TU Darmstadt, German Research Center for AI (DFKI), Hessian.AI[0m

Box rectangle:  [32m(90.0, 726.3) -> (465.6, 740.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, and Jan Peters.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0913.html.[0m



=== Processing ../JMLR 2024/Variance estimation in graphs with the fused lasso.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Variance estimation in graphs with the fused lasso.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-45
                Submitted 8/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(127.8, 101.6) -> (484.3, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVariance estimation in graphs with the fused lasso[0m

Box rectangle:  [32m(90.0, 135.5) -> (522.0, 188.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mOscar Hernan Madrid Padilla
                oscar.madrid@stat.ucla.edu
                Department of Statistics and Data Science
                Univeristy of California, Los Angeles
                Los Angeles, CA 90095.[0m

Box rectangle:  [32m(90.0, 726.3) -> (230.1, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Oscar Hernan Madrid Padilla.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1061.html.[0m



=== Processing ../JMLR 2024/Variational Estimators of the Degree-corrected Latent Block Model for Bipartite Networks.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Variational Estimators of the Degree-corrected Latent Block Model for Bipartite Networks.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-42
                Submitted 7/23; Revised 4/24; Published 5/24[0m

Box rectangle:  [32m(92.1, 101.6) -> (520.0, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVariational Estimators of the Degree-corrected Latent Block
                Model for Bipartite Networks[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYunpeng Zhao
                Yunpeng.Zhao@colostate.edu
                Department of Statistics
                Colorado State University
                Fort Collins, CO 80523, USA[0m

Box rectangle:  [32m(90.0, 205.4) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNing Hao
                nhao@math.arizona.edu
                Department of Mathematics
                University of Arizona
                Tucson, AZ 85721, USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 313.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJi Zhu
                jizhu@umich.edu
                Department of Statistics
                University of Michigan
                Ann Arbor, MI 48109, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (258.0, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yunpeng Zhao, Ning Hao and Ji Zhu.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0984.html.[0m



=== Processing ../JMLR 2024/Variation Spaces for Multi-Output Neural Networks  Insights on Multi-Task Learning and Network Compression.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Variation Spaces for Multi-Output Neural Networks  Insights on Multi-Task Learning and Network Compression.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-40
                Submitted 5/23; Revised 3/24; Published 6/24[0m

Box rectangle:  [32m(95.9, 101.6) -> (516.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVariation Spaces for Multi-Output Neural Networks:
                Insights on Multi-Task Learning and Network Compression[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJoseph Shenouda
                jshenouda@wisc.edu
                Department of Electrical and Computer Engineering
                University of Wisconsin–Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 253.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRahul Parhi∗
                rparhi@ucsd.edu
                Department of Electrical and Computer Engineering
                University of California, San Diego
                La Jolla, CA 92093, USA[0m

Box rectangle:  [32m(90.0, 260.6) -> (522.0, 327.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKangwook Lee
                kangwook.lee@wisc.edu
                Robert D. Nowak
                rdnowak@wisc.edu
                Department of Electrical and Computer Engineering
                University of Wisconsin–Madison
                Madison, WI 53706, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (380.7, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Joseph Shenouda, Rahul Parhi, Kangwook Lee, and Robert D. Nowak.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0677.html.[0m



=== Processing ../JMLR 2024/Virtual-Event-Based Posterior Sampling and Inference for Neyman-Scott Processes.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Virtual-Event-Based Posterior Sampling and Inference for Neyman-Scott Processes.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.6) -> (521.9, 49.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-67
                Submitted 2/24; Revised 11/24; Published 12/24[0m

Box rectangle:  [32m(104.9, 101.5) -> (507.2, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVirtual-Event-Based Posterior Sampling and Inference for
                Neyman-Scott Processes[0m

Box rectangle:  [32m(90.0, 151.8) -> (522.0, 211.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChengkuan Hong
                ckhong@mail.tsinghua.edu.cn
                Department of Computer Science and Technology, BNRist Center
                Tsinghua Institute for AI, Tsinghua-Bosch Joint Center for ML
                Tsinghua University
                Beijing 10084, China[0m

Box rectangle:  [32m(90.0, 217.3) -> (522.0, 265.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mChristian R. Shelton
                cshelton@cs.ucr.edu
                Department of Computer Science and Engineering
                University of California
                Riverside, CA 92521, USA[0m

Box rectangle:  [32m(90.0, 272.2) -> (522.0, 338.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJun Zhu ∗
                dcszj@tsinghua.edu.cn
                Department of Computer Science and Technology, BNRist Center
                Tsinghua Institute for AI, Tsinghua-Bosch Joint Center for ML
                Tsinghua University
                Beijing 10084, China[0m

Box rectangle:  [32m(90.0, 363.8) -> (196.7, 374.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Stephan Mandt[0m

Box rectangle:  [32m(280.3, 399.6) -> (331.7, 411.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 416.2) -> (502.1, 618.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mNeyman-Scott processes (NSPs) are a class of Cox processes constructed by stacking layers
                of Poisson processes into a deep structure. While a lot of research has been conducted
                regarding the posterior sampling and inference for NSPs, most of the existing methods only
                work for shallow NSPs (i.e., NSPs with one layer of latent Poisson processes). In this paper,
                we present virtual-event-based posterior sampling and inference algorithms for NSPs. The
                algorithms work for both deep NSPs and shallow NSPs. Moreover, we show that deep NSPs
                can be viewed as branching processes or a limiting case of probabilistic graphical models.
                We conduct a theoretical analysis of the convergence of our algorithms and provide the
                condition for the convergence to hold. In doing so, we also prove the convergence of virtual-
                event-based sampling inference algorithms for other point process models with missing
                information (Markov jump processes, piecewise-constant intensity models, and Hawkes
                processes). Like NSPs, the latent variables of these models with missing information are
                also point processes. Our experimental results demonstrate that the prediction based on
                our sampling and inference algorithms for NSPs can achieve good prediction performance
                compared with state-of-the-art methods.
                Keywords: Markov chain Monte Carlo, variational inference, point processes, hierarchical
                model, Neyman-Scott processes[0m

Box rectangle:  [32m(90.0, 638.6) -> (180.2, 650.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 660.0) -> (522.0, 684.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThere has been a long history of developing hierarchical models (e.g., deep neural networks
                (LeCun et al., 2015), graph neural networks (Scarselli et al., 2008), and probabilistic graphi-[0m

Box rectangle:  [32m(93.7, 695.9) -> (209.8, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m∗. The corresponding author[0m

Box rectangle:  [32m(90.0, 726.4) -> (319.5, 734.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Chengkuan Hong, Christian R. Shelton, and Jun Zhu.[0m

Box rectangle:  [32m(90.0, 740.8) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/24-0235.html.[0m



=== Processing ../JMLR 2024/Volterra Neural Networks (VNNs).pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Volterra Neural Networks (VNNs).pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-29
                Submitted 9/21; Revised 6/24; Published 6/24[0m

Box rectangle:  [32m(201.4, 101.7) -> (410.6, 116.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mVolterra Neural Networks (VNNs)[0m

Box rectangle:  [32m(90.0, 135.4) -> (521.7, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSiddharth Roheda
                SID.ROHEDA@GMAIL.COM
                Electrical and Computer Engineering Department
                North Carolina State University
                Raleigh, NC 27606, USA[0m

Box rectangle:  [32m(90.0, 189.0) -> (521.8, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHamid Krim
                AHK@NCSU.EDU
                Electrical and Computer Engineering Department
                North Carolina State University
                Raleigh, NC 27606, USA[0m

Box rectangle:  [32m(90.0, 244.2) -> (521.8, 295.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBo Jiang
                BJIANG8@NCSU.EDU
                Electrical and Computer Engineering Department
                North Carolina State University
                Raleigh, NC 27606, USA[0m

Box rectangle:  [32m(90.0, 320.7) -> (175.2, 330.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Honglak Lee[0m

Box rectangle:  [32m(283.8, 356.5) -> (328.2, 368.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 375.1) -> (502.1, 555.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mThe importance of inference in Machine Learning (ML) has led to an explosive number of different
                proposals, particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional
                Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture in-
                troduces controlled non-linearities in the form of interactions between the delayed input samples
                of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce
                the number of parameters required to carry out the same classification task as that of a conven-
                tional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural
                Network (VNN), along with its remarkable performance while retaining a relatively simpler and
                potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of
                this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal)
                information of a video sequence for action recognition. The proposed approach is evaluated on
                UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform state of the
                art CNN approaches. The code-base for our paper is available on github ( https://github.com/sid-
                roheda/Volterra-Neural-Networks).
                Keywords: Volterra Filter, Activity Recognition, Activation Free Learning[0m

Box rectangle:  [32m(90.0, 576.5) -> (168.0, 588.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m1. Introduction[0m

Box rectangle:  [32m(90.0, 599.7) -> (522.0, 705.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHuman action recognition is an important research topic in Computer Vision, and may be useful
                in surveillance, video retrieval, and man-machine interaction to name a few. The survey on Action
                Recognition approaches Kong and Fu (2018) provides a good progress overview. Video classifica-
                tion usually involves three stages (Wang et al., 2009; Liu et al., 2009; Niebles et al., 2010; Sivic
                and Zisserman, 2003; Karpathy et al., 2014), namely, visual feature extraction (local features like
                Histograms of Oriented Gradients (HoG) introduced in Dalal and Triggs (2005), or global features
                like Hue, Saturation, etc.), feature fusion/concatenation, and lastly classification. In Yi et al. (2011),
                an intrinsic stochastic modeling of human activity on a shape manifold is proposed and an accurate[0m

Box rectangle:  [32m(90.0, 726.7) -> (261.4, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Siddharth Roheda, Hamid Krim and Bo Jiang.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/21-1082.html.[0m



=== Processing ../JMLR 2024/Wasserstein Proximal Coordinate Gradient Algorithms.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Wasserstein Proximal Coordinate Gradient Algorithms.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-66
                Submitted 7/23; Revised 5/24; Published 8/24[0m

Box rectangle:  [32m(111.4, 101.6) -> (500.8, 115.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWasserstein Proximal Coordinate Gradient Algorithms[0m

Box rectangle:  [32m(90.0, 133.9) -> (522.0, 181.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mRentian Yao
                rentian2@illinois.edu
                Department of Statistics
                University of Illinois at Urbana–Champaign
                Champaign, IL 61820, USA[0m

Box rectangle:  [32m(90.0, 187.5) -> (522.0, 235.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiaohui Chen
                xiaohuic@usc.edu
                Department of Mathematics
                University of Southern California
                Los Angeles, CA 90089, USA[0m

Box rectangle:  [32m(90.0, 242.7) -> (522.0, 295.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYun Yang
                yy84@umd.edu
                Department of Mathematics
                University of Maryland
                College Park, MD 20742, USA[0m

Box rectangle:  [32m(90.0, 726.3) -> (278.4, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Rentian Yao, Xiaohui Chen and Yun Yang.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-0889.html.[0m



=== Processing ../JMLR 2024/White-Box Transformers via Sparse Rate Reduction  Compression Is All There Is.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/White-Box Transformers via Sparse Rate Reduction  Compression Is All There Is.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-128
                Submitted 11/23; Revised 8/24; Published 9/24[0m

Box rectangle:  [32m(119.0, 102.0) -> (493.1, 134.3)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWhite-Box Transformers via Sparse Rate Reduction:
                Compression Is All There Is?[0m

Box rectangle:  [32m(90.0, 152.9) -> (518.2, 165.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYaodong Yu†,⋆
                yyu@eecs.berkeley.edu[0m

Box rectangle:  [32m(90.0, 170.6) -> (518.2, 183.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mSam Buchanan‡,⋆
                sam@ttic.edu[0m

Box rectangle:  [32m(90.0, 188.3) -> (518.2, 201.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDruv Pai†,⋆
                druvpai@berkeley.edu[0m

Box rectangle:  [32m(90.0, 206.0) -> (518.2, 218.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mTianzhe Chu†,♢
                chutzh@berkeley.edu[0m

Box rectangle:  [32m(90.0, 223.7) -> (518.2, 236.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZiyang Wu†
                zywu@berkeley.edu[0m

Box rectangle:  [32m(90.0, 241.4) -> (518.2, 254.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShengbang Tong†
                tsb@berkeley.edu[0m

Box rectangle:  [32m(90.0, 259.3) -> (518.2, 271.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mHao Bai♯
                haob2@illinois.edu[0m

Box rectangle:  [32m(90.0, 276.9) -> (518.2, 289.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuexiang Zhai†
                simonzhai@berkeley.edu[0m

Box rectangle:  [32m(90.0, 295.8) -> (518.2, 308.4)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mBenjamin D. Haeffele♭
                bhaeffele@jhu.edu[0m

Box rectangle:  [32m(90.0, 315.0) -> (518.3, 327.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYi Ma†,♢
                mayi@hku.hk, yima@eecs.berkeley.edu[0m

Box rectangle:  [32m(88.0, 333.8) -> (278.0, 401.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m† University of California, Berkeley
                ‡ Toyota Technological Institute at Chicago
                ♯University of Illinois, Urbana-Champaign
                ♭Johns Hopkins University
                ♢University of Hong Kong[0m

Box rectangle:  [32m(90.0, 426.7) -> (222.8, 436.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mEditor: Mahdi Soltanolkotabi[0m

Box rectangle:  [32m(280.3, 441.9) -> (331.7, 453.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mAbstract[0m

Box rectangle:  [32m(109.9, 458.7) -> (502.1, 683.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mIn this paper, we contend that a natural objective of representation learning is to compress
                and transform the distribution of the data, say sets of tokens, towards a low-dimensional
                Gaussian mixture supported on incoherent subspaces. The goodness of such a representa-
                tion can be evaluated by a principled measure, called sparse rate reduction, that simultane-
                ously maximizes the intrinsic information gain and extrinsic sparsity of the learned represen-
                tation. From this perspective, popular deep network architectures, including transformers,
                can be viewed as realizing iterative schemes to optimize this measure. Particularly, we de-
                rive a transformer block from alternating optimization on parts of this objective: the multi-
                head self-attention operator compresses the representation by implementing an approximate
                gradient descent step on the coding rate of the features, and the subsequent multi-layer
                perceptron sparsifies the features. This leads to a family of white-box transformer-like deep
                network architectures, named crate, which are mathematically fully interpretable. We
                show, by way of a novel connection between denoising and compression, that the inverse
                to the aforementioned compressive encoding can be realized by the same class of crate
                architectures. Thus, the so-derived white-box architectures are universal to both encoders
                and decoders. Experiments show that these networks, despite their simplicity, indeed learn
                to compress and sparsify representations of large-scale real-world image and text datasets,
                and achieve strong performance across different settings: ViT, MAE, DINO, BERT, and
                GPT2. We believe the proposed computational framework demonstrates great potential in[0m

Box rectangle:  [32m(95.0, 694.4) -> (182.4, 704.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m.
                ⋆Equal contribution.[0m

Box rectangle:  [32m(90.0, 726.4) -> (504.1, 744.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33m©2024 Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang
                Zhai, Benjamin D. Haeffele, and Yi Ma.[0m

Box rectangle:  [32m(90.0, 750.4) -> (517.0, 768.2)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1547.html.[0m



=== Processing ../JMLR 2024/Win  Weight-Decay-Integrated Nesterov Acceleration for  Faster Network Training.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Win  Weight-Decay-Integrated Nesterov Acceleration for  Faster Network Training.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.9) -> (522.0, 49.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-74
                Submitted 8/23; Published 3/24[0m

Box rectangle:  [32m(109.9, 101.7) -> (502.4, 134.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mWin: Weight-Decay-Integrated Nesterov Acceleration for Faster
                Network Training[0m

Box rectangle:  [32m(89.8, 153.4) -> (521.8, 175.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mPan Zhou
                PANZHOU@SMU.EDU.SG
                School of Computing and Information Systems, Singapore Management University, Singapore[0m

Box rectangle:  [32m(89.6, 190.0) -> (521.8, 212.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXingyu Xie
                XYXIE@PKU.EDU.CN
                National Key Lab of General AI, School of Intelligence Science and Technology, Peking University, China[0m

Box rectangle:  [32m(89.7, 226.7) -> (521.8, 273.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhouchen Lin
                ZLIN@PKU.EDU.CN
                National Key Lab of General AI, School of Intelligence Science and Technology, Peking University, China
                Institute for Artificial Intelligence, Peking University, China
                Peng Cheng Laboratory, China[0m

Box rectangle:  [32m(89.6, 287.2) -> (522.0, 321.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mKim-Chuan Toh
                MATTOHKC@NUS.EDU.SG
                Department of Mathematics and Institute of Operations Research and Analytics, National University of
                Singapore, Singapore[0m

Box rectangle:  [32m(89.8, 337.4) -> (521.7, 361.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mShuicheng Yan
                SHUICHENG.YAN@GMAIL.COM
                Skywork AI[0m

Box rectangle:  [32m(90.0, 726.4) -> (347.3, 734.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Pan Zhou, Xingyu Xie, Zhouchen Lin, Kim-Chuan Toh, Shuicheng Yan.[0m

Box rectangle:  [32m(90.0, 740.7) -> (515.4, 758.6)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
                http://jmlr.org/papers/v25/23-1073.html.[0m



=== Processing ../JMLR 2024/Zeroth-order Stochastic Approximation Algorithms for DR-submodular Optimization.pdf ===

[31m--- Pieces of PosixPath('../JMLR 2024/Zeroth-order Stochastic Approximation Algorithms for DR-submodular Optimization.pdf') ---[0m

Box rectangle:  [32m(90.0, 41.7) -> (521.9, 49.7)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mJournal of Machine Learning Research 25 (2024) 1-55
                Submitted 11/23; Published 12/24[0m

Box rectangle:  [32m(111.9, 101.6) -> (500.3, 133.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZeroth-order Stochastic Approximation Algorithms for
                DR-submodular Optimization[0m

Box rectangle:  [32m(90.0, 151.9) -> (522.0, 199.9)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mYuefang Lian
                lianyf@emails.bjut.edu.cn
                Institute of Operations Research and Information Engineering
                Beijing University of Technology
                Beijing, 100124, China[0m

Box rectangle:  [32m(90.0, 205.1) -> (522.0, 241.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mXiao Wang∗
                wxucas@outlook.com
                Pengcheng Laboratory
                Shenzhen, 518066, China[0m

Box rectangle:  [32m(90.0, 247.1) -> (522.0, 295.1)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mDachuan Xu
                xudc@bjut.edu.cn
                Institute of Operations Research and Information Engineering
                Beijing University of Technology
                Beijing, 100124, China[0m

Box rectangle:  [32m(90.0, 302.2) -> (521.9, 355.0)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mZhongrui Zhao
                zhongrui.zhao@my.jcu.edu.au
                College of Science and Engineering
                James Cook University
                Queensland, 4814, Australia[0m

Box rectangle:  [32m(90.0, 726.3) -> (341.3, 734.5)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mc⃝2024 Yuefang Lian, Xiao Wang, Dachuan Xu and Zhongrui Zhao.[0m

Box rectangle:  [32m(90.0, 741.0) -> (517.0, 758.8)[0m  # (x0, y0) -> (x1, y1)
Text content:   [33mLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
                at http://jmlr.org/papers/v25/23-1523.html.[0m

